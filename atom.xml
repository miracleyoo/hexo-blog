<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Miracleyoo</title>
  
  
  <link href="https://www.miracleyoo.com/atom.xml" rel="self"/>
  
  <link href="https://www.miracleyoo.com/"/>
  <updated>2019-10-04T03:01:16.920Z</updated>
  <id>https://www.miracleyoo.com/</id>
  
  <author>
    <name>Miracle Yoo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Curriculum Vitae -- Zhongyang Zhang</title>
    <link href="https://www.miracleyoo.com/2118/10/30/resume/"/>
    <id>https://www.miracleyoo.com/2118/10/30/resume/</id>
    <published>2118-10-30T19:44:20.000Z</published>
    <updated>2019-10-04T03:01:16.920Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2118/10/30/resume/cv.png" alt="Resume_ZhongyangZhang_HUST"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2118/10/30/resume/cv.png&quot; alt=&quot;Resume_ZhongyangZhang_HUST&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="Resume" scheme="https://www.miracleyoo.com/tags/Resume/"/>
    
  </entry>
  
  <entry>
    <title>pytorch-lightning</title>
    <link href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/"/>
    <id>https://www.miracleyoo.com/2021/03/11/pytorch-lightning/</id>
    <published>2021-03-12T01:09:57.000Z</published>
    <updated>2021-03-12T01:09:57.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch-lighting"><a class="markdownIt-Anchor" href="#pytorch-lighting"></a> Pytorch-Lighting</h1><h2 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h2><p>Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？</p><p>于是我第二次发现了Pytorch-Lightning。</p><p>真香。</p><p>但是问题还是来了。这个框架并没有因为香而变得更加易学。官网的教程很丰富，可以看出来开发者们在努力做了。但是很多相连的知识点都被分布在了不同的版块里，还有一些核心的理解要点并没有被强调出来，而是小字带过，这让我想做一个普惠的教程，包含所有我在学习过程中认为重要的概念，好用的参数，一些注意点、坑点，大量的示例代码段和一些核心问题的集中讲解。</p><p>最后，第三部分提供了一个我总结出来的易用于大型项目、容易迁移、易于复用的模板，有兴趣的可以去<a href="https://github.com/miracleyoo/pytorch-lightning-template">GitHub</a>试用。</p><h2 id="crucial"><a class="markdownIt-Anchor" href="#crucial"></a> Crucial</h2><ul><li><p>Pytorch-Lighting 的一大特点是把模型和系统分开来看。模型是像Resnet18， RNN之类的纯模型， 而系统定义了一组模型如何相互交互，如GAN（生成器网络与判别器网络）、Seq2Seq（Encoder与Decoder网络）和Bert。同时，有时候问题只涉及一个模型，那么这个系统则可以是一个通用的系统，用于描述模型如何使用，并可以被复用到很多其他项目。</p></li><li><p>Pytorch-Lighting 的核心设计思想是“自给自足”。每个网络也同时包含了如何训练、如何测试、优化器定义等内容。</p></li></ul><p><img src="/2021/03/11/pytorch-lightning/plres.png" alt="img"></p><h2 id="推荐使用方法"><a class="markdownIt-Anchor" href="#推荐使用方法"></a> 推荐使用方法</h2><p>这一部分放在最前面，因为全文内容太长，如果放后面容易忽略掉这部分精华。</p><p>Pytorch-Lightning 是一个很好的库，或者说是pytorch的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。缺点也很明显，这个包需要学习和理解的内容还是挺多的，或者换句话说，很重。如果直接按照官方的模板写代码，小型project还好，如果是大型项目，有复数个需要调试验证的模型和数据集，那就不太好办，甚至更加麻烦了。经过几天的摸索和调试，我总结出了下面这样一套好用的模板，也可以说是对Pytorch-Lightning的进一步抽象。</p><p>欢迎大家尝试这一套代码风格，如果用习惯的话还是相当方便复用的，也不容易半道退坑。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">|-data</span><br><span class="line">|-__init__.py</span><br><span class="line">|-data_interface.py</span><br><span class="line">|-xxxdataset1.py</span><br><span class="line">|-xxxdataset2.py</span><br><span class="line">|-...</span><br><span class="line">|-model</span><br><span class="line">|-__init__.py</span><br><span class="line">|-model_interface.py</span><br><span class="line">|-xxxmodel1.py</span><br><span class="line">|-xxxmodel2.py</span><br><span class="line">|-...</span><br><span class="line">|-main.py</span><br></pre></td></tr></table></figure><p>如果对每个模型直接上plmodule，对于已有项目、别人的代码等的转换将相当耗时。另外，这样的话，你需要给每个模型都加上一些相似的代码，如<code>training_step</code>，<code>validation_step</code>。显然，这并不是我们想要的，如果真的这样做，不但不易于维护，反而可能会更加杂乱。同理，如果把每个数据集类都直接转换成pl的DataModule，也会面临相似的问题。基于这样的考量，我建议使用上述架构：</p><ul><li><p>主目录下只放一个<code>main.py</code>文件。</p></li><li><p><code>data</code>和<code>modle</code>两个文件夹中放入<code>__init__.py</code>文件，做成包。这样方便导入。两个<code>init</code>文件分别是：</p><ul><li><code>from .data_interface import DInterface</code></li><li><code>from .model_interface import MInterface</code></li></ul></li><li><p>在<code>data_interface</code>中建立一个<code>class DInterface(pl.LightningDataModule):</code>用作所有数据集文件的接口。<code>__init__()</code>函数中import相应Dataset类，<code>setup()</code>进行实例化，并老老实实加入所需要的的<code>train_dataloader</code>, <code>val_dataloader</code>, <code>test_dataloader</code>函数。这些函数往往都是相似的，可以用几个输入args控制不同的部分。</p></li><li><p>同理，在<code>model_interface</code>中建立<code>class MInterface(pl.LightningModule):</code>类，作为模型的中间接口。<code>__init__()</code>函数中import相应模型类，然后老老实实加入<code>configure_optimizers</code>, <code>training_step</code>, <code>validation_step</code>等函数，用一个接口类控制所有模型。不同部分使用输入参数控制。</p></li><li><p><code>main.py</code>函数只负责：</p><ul><li>定义parser，添加parse项。</li><li>选好需要的<code>callback</code>函数们。</li><li>实例化<code>MInterface</code>, <code>DInterface</code>, <code>Trainer</code>。</li></ul><p>完事。</p></li></ul><h2 id="lightning-module"><a class="markdownIt-Anchor" href="#lightning-module"></a> Lightning Module</h2><h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">主页面</a></p><ul><li><p>三个核心组件：</p><ul><li>模型</li><li>优化器</li><li>Train/Val/Test步骤</li></ul></li><li><p>数据流伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">    out = training_step(batch)</span><br><span class="line">    outs.append(out)</span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><p>等价Lightning代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    prediction = ...</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        <span class="comment"># do something with these</span></span><br></pre></td></tr></table></figure><p>我们需要做的，就是像填空一样，填这些函数。</p></li></ul><h3 id="组件与函数"><a class="markdownIt-Anchor" href="#组件与函数"></a> 组件与函数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api">API页面</a></p><ul><li><p>一个Pytorch-Lighting 模型必须含有的部件是：</p><ul><li><p><code>init</code>: 初始化，包括模型和系统的定义。</p></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.training_step"><code>training_step(self, batch, batch_idx)</code></a>: 即每个batch的处理函数。</p><blockquote><p>参数：</p><ul><li><strong>batch</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> | (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …) | [<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …]) – The output of your <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. A tensor, tuple or list.</li><li><strong>batch_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Integer displaying index of this batch</li><li><strong>optimizer_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – When using multiple optimizers, this argument will also be present.</li><li><strong>hiddens</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>) – Passed in if <a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.params.truncated_bptt_steps"><code>truncated_bptt_steps</code></a> &gt; 0.</li></ul><p>返回值：Any of.</p><ul><li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> - The loss tensor</li><li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li><li><code>None</code> - Training will skip to the next batch</li></ul></blockquote><p>返回值无论如何也需要有一个loss量。如果是字典，要有这个key。没loss这个batch就被跳过了。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y, z = batch</span><br><span class="line">    out = self.encoder(x)</span><br><span class="line">    loss = self.loss(out, x)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple optimizers (e.g.: GANs)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx, optimizer_idx</span>):</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># do training_step with encoder</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># do training_step with decoder</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Truncated back-propagation through time</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx, hiddens</span>):</span></span><br><span class="line">    <span class="comment"># hiddens are the hidden states from the previous truncated backprop step</span></span><br><span class="line">    ...</span><br><span class="line">    out, hiddens = self.lstm(data, hiddens)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;hiddens&#x27;</span>: hiddens&#125;</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization"><code>configure_optimizers</code></a>: 优化器定义，返回一个优化器，或数个优化器，或两个List（优化器，Scheduler）。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most cases</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    opt = Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">return</span> opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> generator_opt, disriminator_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [generator_opt, disriminator_opt], [discriminator_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sched = &#123;<span class="string">&#x27;scheduler&#x27;</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">                 <span class="string">&#x27;interval&#x27;</span>: <span class="string">&#x27;step&#x27;</span>&#125;  <span class="comment"># called after each training step</span></span><br><span class="line">    dis_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sched, dis_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: dis_opt, <span class="string">&#x27;frequency&#x27;</span>: n_critic&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: gen_opt, <span class="string">&#x27;frequency&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li></ul></li><li><p>可以指定的部件有：</p><ul><li><code>forward</code>: 和正常的<code>nn.Module</code>一样，用于inference。内部调用时：<code>y=self(batch)</code></li><li><code>training_step_end</code>: 只在使用多个node进行训练且结果涉及如softmax之类需要全部输出联合运算的步骤时使用该函数。同理，<code>validation_step_end</code>/<code>test_step_end</code>。</li><li><code>training_epoch_end</code>:<ul><li>在一个训练epoch结尾处被调用。</li><li>输入参数：一个List，List的内容是前面<code>training_step()</code>所返回的每次的内容。</li><li>返回：None</li></ul></li><li><code>validation_step(self, batch, batch_idx)</code>/<code>test_step(self, batch, batch_idx)</code>:<ul><li>没有返回值限制，不一定非要输出一个<code>val_loss</code>。</li></ul></li><li><code>validation_epoch_end</code>/<code>test_epoch_end</code>:</li></ul></li><li><p>工具函数有：</p><ul><li><p><code>freeze</code>：冻结所有权重以供预测时候使用。仅当已经训练完成且后面只测试时使用。</p></li><li><p><code>print</code>：尽管自带的<code>print</code>函数也可以使用，但如果程序运行在分布式系统时，会打印多次。而使用<code>self.print()</code>则只会打印一次。</p></li><li><p><code>log</code>：像是TensorBoard等log记录器，对于每个log的标量，都会有一个相对应的横坐标，它可能是batch number或epoch number。而<code>on_step</code>就表示把这个log出去的量的横坐标表示为当前batch，而<code>on_epoch</code>则表示将log的量在整个epoch上进行累积后log，横坐标为当前epoch。</p><table><thead><tr><th>LightningMoule Hook</th><th>on_step</th><th>on_epoch</th><th>prog_bar</th><th>logger</th></tr></thead><tbody><tr><td>training_step</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_step_end</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_epoch_end</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_epoch_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr></tbody></table><p><code>*</code> also applies to the test loop</p><blockquote><p>参数</p><ul><li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – key name</li><li><strong>value</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>) – value name</li><li><strong>prog_bar</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the progress bar</li><li><strong>logger</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the logger</li><li><strong>on_step</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs at this step. None auto-logs at the training_step but not validation/test_step</li><li><strong>on_epoch</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</li><li><strong>reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – reduction function over step values for end of epoch. Torch.mean by default</li><li><strong>tbptt_reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – function to reduce on truncated back prop</li><li><strong>tbptt_pad_token</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – token to use for padding</li><li><strong>enable_graph</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, will not auto detach the graph</li><li><strong>sync_dist</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, reduces the metric across GPUs/TPUs</li><li><strong>sync_dist_op</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>, <a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>]) – the op to sync across GPUs/TPUs</li><li><strong>sync_dist_group</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>]) – the ddp group</li></ul></blockquote></li><li><p><code>log_dict</code>：和<code>log</code>函数唯一的区别就是，<code>name</code>和<code>value</code>变量由一个字典替换。表示同时log多个值。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;acc&#x27;</span>: acc, ..., <span class="string">&#x27;metric_n&#x27;</span>: metric_n&#125;</span><br><span class="line">self.log_dict(values)</span><br></pre></td></tr></table></figure></li><li><p><code>save_hyperparameters</code>：储存<code>init</code>中输入的所有超参。后续访问可以由<code>self.hparams.argX</code>方式进行。同时，超参表也会被存到文件中。</p></li></ul></li><li><p>函数内建变量：</p><ul><li><code>device</code>：可以使用<code>self.device</code>来构建设备无关型tensor。如：<code>z = torch.rand(2, 3, device=self.device)</code>。</li><li><code>hparams</code>：含有所有前面存下来的输入超参。</li><li><code>precision</code>：精确度。常见32和16。</li></ul></li></ul><h3 id="要点"><a class="markdownIt-Anchor" href="#要点"></a> 要点</h3><ul><li>如果准备使用DataParallel，在写<code>training_step</code>的时候需要调用forward函数，<code>z=self(x)</code></li></ul><h3 id="模板"><a class="markdownIt-Anchor" href="#模板"></a> 模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitModel</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">training_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">training_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">configure_optimizers</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">any_extra_hook</span>(<span class="params">...</span>)</span></span><br></pre></td></tr></table></figure><h2 id="trainer"><a class="markdownIt-Anchor" href="#trainer"></a> Trainer</h2><h3 id="基础使用"><a class="markdownIt-Anchor" href="#基础使用"></a> 基础使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightningModule()</span><br><span class="line"></span><br><span class="line">trainer = Trainer()</span><br><span class="line">trainer.fit(model, train_dataloader, val_dataloader)</span><br></pre></td></tr></table></figure><p>如果连<code>validation_step</code>都没有，那<code>val_dataloader</code>也就算了。</p><h3 id="伪代码与hooks"><a class="markdownIt-Anchor" href="#伪代码与hooks"></a> 伪代码与hooks</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks">Hooks页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">...</span>):</span></span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> global_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># prepare data is called on GLOBAL_ZERO only</span></span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> gpu/tpu <span class="keyword">in</span> gpu/tpus:</span><br><span class="line">        train_on_device(model.copy())</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_on_device</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="comment"># setup is called PER DEVICE</span></span><br><span class="line">    setup()</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_pretrain_routine_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_loop()</span><br><span class="line"></span><br><span class="line">    teardown()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_loop</span>():</span></span><br><span class="line">    on_train_epoch_start()</span><br><span class="line">    train_outs = []</span><br><span class="line">    <span class="keyword">for</span> train_batch <span class="keyword">in</span> train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----- train_step methods -------</span></span><br><span class="line">        out = training_step(batch)</span><br><span class="line">        train_outs.append(out)</span><br><span class="line"></span><br><span class="line">        loss = out.loss</span><br><span class="line"></span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line">        optimizer_step()</span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end training epoch</span></span><br><span class="line">    logs = training_epoch_end(outs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_loop</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line">    val_outs = []</span><br><span class="line">    <span class="keyword">for</span> val_batch <span class="keyword">in</span> val_dataloader():</span><br><span class="line">        on_validation_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------- val step methods -------</span></span><br><span class="line">        out = validation_step(val_batch)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up for train</span></span><br><span class="line">    model.train()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="推荐参数"><a class="markdownIt-Anchor" href="#推荐参数"></a> 推荐参数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags">参数介绍（附视频）</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api">类定义与默认参数</a></p><ul><li><p><code>default_root_dir</code>：默认存储地址。所有的实验变量和权重全部会被存到这个文件夹里面。推荐是，每个模型有一个独立的文件夹。每次重新训练会产生一个新的<code>version_x</code>子文件夹。</p></li><li><p><code>max_epochs</code>：最大训练周期数。<code>trainer = Trainer(max_epochs=1000)</code></p></li><li><p><code>min_epochs</code>：至少训练周期数。当有Early Stop时使用。</p></li><li><p><code>auto_scale_batch_size</code>：在进行任何训练前自动选择合适的batch size。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (no scaling of batch size)</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run batch size scaling, result overrides hparams.batch_size</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="string">&#x27;binsearch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the batch size</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>auto_select_gpus</code>：自动选择合适的GPU。尤其是在有GPU处于独占模式时候，非常有用。</p></li><li><p><code>auto_lr_find</code>：自动找到合适的初始学习率。使用了该<a href="https://arxiv.org/abs/1506.01186">论文</a>的技术。当且仅当执行<code>trainer.tune(model)</code>代码时工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run learning rate finder, results override hparams.learning_rate</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run learning rate finder, results override hparams.my_lr_arg</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="string">&#x27;my_lr_arg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the lr</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>precision</code>：精确度。正常是32，使用16可以减小内存消耗，增大batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 16-bit precision</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">16</span>, gpus=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>val_check_interval</code>：进行Validation测试的周期。正常为1，训练1个epoch测试4次是0.25，每1000 batch测试一次是1000。</p><blockquote><ul><li>use (float) to check within a training epoch：此时这个值为一个epoch的百分比。每百分之多少测试一次。</li><li>use (int) to check every n steps (batches)：每多少个batch测试一次。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set 4 times during a training epoch</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set every 1000 training batches</span></span><br><span class="line"><span class="comment"># use this when using iterableDataset and your dataset has no length</span></span><br><span class="line"><span class="comment"># (ie: production cases with streaming data)</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#gpus"><code>gpus</code></a>：控制使用的GPU数。当设定为None时，使用cpu。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (ie: train on CPU)</span></span><br><span class="line">trainer = Trainer(gpus=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># int: train on 2 gpus</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list: train on GPUs 1, 4 (by bus ordering)</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;1, 4&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1: train on all gpus</span></span><br><span class="line">trainer = Trainer(gpus=-<span class="number">1</span>)</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;-1&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine with num_nodes to train on multiple GPUs across nodes</span></span><br><span class="line"><span class="comment"># uses 8 gpus in total</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>, num_nodes=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train only on GPUs 1 and 4 across nodes</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>], num_nodes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#limit-train-batches"><code>limit_train_batches</code></a>：使用训练数据的百分比。如果数据过多，或正在调试，可以使用这个。值的范围为0~1。同样，有<code>limit_test_batches</code>，<code>limit_val_batches</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 25% of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 10 batches of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run"><code>fast_dev_run</code></a>：bool量。如果设定为true，会只执行一个batch的train, val 和 test，然后结束。仅用于debug。</p><blockquote><p>Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like <code>LearningRateLogger</code> and runs for only 1 epoch</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 1 train, val, test batch and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 7 train, val, test batches and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="number">7</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="fit函数"><a class="markdownIt-Anchor" href="#fit函数"></a> .fit()函数</h3><p><code>Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None)</code>：输入第一个量一定是model，然后可以跟一个LigntningDataModule或一个普通的Train DataLoader。如果定义了Val step，也要有Val DataLoader。</p><blockquote><p>参数</p><ul><li><strong>datamodule</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule"><code>LightningDataModule</code></a>]) – A instance of <code>LightningDataModule</code>.</li><li><strong>model</strong> (<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule"><code>LightningModule</code></a>) – Model to fit.</li><li><strong>train_dataloader</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>]) – A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.</li><li><strong>val_dataloaders</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>, <a href="https://docs.python.org/3/library/typing.html#typing.List"><code>List</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>], <a href="https://docs.python.org/3/library/constants.html#None"><code>None</code></a>]) – Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped</li></ul></blockquote><h3 id="其他要点"><a class="markdownIt-Anchor" href="#其他要点"></a> 其他要点</h3><ul><li><code>.test()</code>若非直接调用，不会运行。<code>trainer.test()</code></li><li><code>.test()</code>会自动load最优模型。</li><li><code>model.eval()</code> and <code>torch.no_grad()</code> 在进行测试时会被自动调用。</li><li>默认情况下，<code>Trainer()</code>运行于CPU上。</li></ul><h3 id="使用样例"><a class="markdownIt-Anchor" href="#使用样例"></a> 使用样例</h3><ol><li>手动添加命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">hparams</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer(gpus=hparams.gpus)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gpus&#x27;</span>, default=<span class="literal">None</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="2"><li>自动添加所有<code>Trainer</code>会用到的命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="3"><li>混合式，既使用<code>Trainer</code>相关参数，又使用一些自定义参数，如各种模型超参：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> LightningModule, Trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">32</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hidden_dim&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">128</span>)</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="所有参数"><a class="markdownIt-Anchor" href="#所有参数"></a> 所有参数</h3><blockquote><p><code>Trainer.``__init__</code>(<em>logger=True</em>, <em>checkpoint_callback=True</em>, <em>callbacks=None</em>, <em>default_root_dir=None</em>, <em>gradient_clip_val=0</em>, <em>process_position=0</em>, <em>num_nodes=1</em>, <em>num_processes=1</em>, <em>gpus=None</em>, <em>auto_select_gpus=False</em>, <em>tpu_cores=None</em>, <em>log_gpu_memory=None</em>, <em>progress_bar_refresh_rate=None</em>, <em>overfit_batches=0.0</em>, <em>track_grad_norm=- 1</em>, <em>check_val_every_n_epoch=1</em>, <em>fast_dev_run=False</em>, <em>accumulate_grad_batches=1</em>, <em>max_epochs=None</em>, <em>min_epochs=None</em>, <em>max_steps=None</em>, <em>min_steps=None</em>, <em>limit_train_batches=1.0</em>, <em>limit_val_batches=1.0</em>, <em>limit_test_batches=1.0</em>, <em>limit_predict_batches=1.0</em>, <em>val_check_interval=1.0</em>, <em>flush_logs_every_n_steps=100</em>, <em>log_every_n_steps=50</em>, <em>accelerator=None</em>, <em>sync_batchnorm=False</em>, <em>precision=32</em>, <em>weights_summary=‘top’</em>, <em>weights_save_path=None</em>, <em>num_sanity_val_steps=2</em>, <em>truncated_bptt_steps=None</em>, <em>resume_from_checkpoint=None</em>, <em>profiler=None</em>, <em>benchmark=False</em>, <em>deterministic=False</em>, <em>reload_dataloaders_every_epoch=False</em>, <em>auto_lr_find=False</em>, <em>replace_sampler_ddp=True</em>, <em>terminate_on_nan=False</em>, <em>auto_scale_batch_size=False</em>, <em>prepare_data_per_node=True</em>, <em>plugins=None</em>, <em>amp_backend=‘native’</em>, <em>amp_level=‘O2’</em>, <em>distributed_backend=None</em>, <em>move_metrics_to_cpu=False</em>, <em>multiple_trainloader_mode=‘max_size_cycle’</em>, <em>stochastic_weight_avg=False</em>)</p></blockquote><h3 id="log和return-loss到底在做什么"><a class="markdownIt-Anchor" href="#log和return-loss到底在做什么"></a> Log和return loss到底在做什么</h3><p>To add a training loop use the training_step method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitClassifier</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = model</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">         x, y = batch</span><br><span class="line">         y_hat = self.model(x)</span><br><span class="line">         loss = F.cross_entropy(y_hat, y)</span><br><span class="line">         <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>无论是<code>training_step</code>，还是<code>validation_step</code>，<code>test_step</code>返回值都是<code>loss</code>。返回的loss会被用一个list收集起来。</li></ul><p>Under the hood, Lightning does the following (pseudocode):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put model in train mode</span></span><br><span class="line">model.train()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    loss = training_step(batch)</span><br><span class="line">    losses.append(loss.detach())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><h4 id="training-epoch-level-metrics"><a class="markdownIt-Anchor" href="#training-epoch-level-metrics"></a> Training epoch-level metrics</h4><p>If you want to calculate epoch-level metrics and log them, use the <code>.log</code> method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logs metrics for each training_step,</span></span><br><span class="line">    <span class="comment"># and the average across the epoch, to the progress bar and logger</span></span><br><span class="line">    self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>如果在<code>x_step</code>函数中使用了<code>.log()</code>函数，那么这个量将会被逐步记录下来。每一个<code>log</code>出去的变量都会被记录下来，每一个<code>step</code>会集中生成一个字典dict，而每个epoch都会把这些字典收集起来，形成一个字典的list。</li></ul><p>The .log object automatically reduces the requested metrics across the full epoch. Here’s the pseudocode of what it does under the hood:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">epoch_metric = torch.mean(torch.stack([x[<span class="string">&#x27;train_loss&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> outs]))</span><br></pre></td></tr></table></figure><h4 id="train-epoch-level-operations"><a class="markdownIt-Anchor" href="#train-epoch-level-operations"></a> Train epoch-level operations</h4><p>If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    preds = ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;other_stuff&#x27;</span>: preds&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span></span><br><span class="line">   <span class="keyword">for</span> pred <span class="keyword">in</span> training_step_outputs:</span><br><span class="line">       <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><p>The matching pseudocode is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><h2 id="datamodule"><a class="markdownIt-Anchor" href="#datamodule"></a> DataModule</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html">主页面</a></p><h3 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h3><ul><li><p>首先，这个<code>DataModule</code>和之前写的Dataset完全不冲突。前者是后者的一个包装，并且这个包装可以被用于多个torch Dataset 中。在我看来，其最大的作用就是把各种train/val/test划分、DataLoader初始化之类的重复代码通过包装类的方式得以被简单的复用。</p></li><li><p>具体作用项目：</p><ul><li>Download instructions：下载</li><li>Processing instructions：处理</li><li>Split instructions：分割</li><li>Train dataloader：训练集Dataloader</li><li>Val dataloader(s)：验证集Dataloader</li><li>Test dataloader(s)：测试集Dataloader</li></ul></li><li><p>其次，<code>pl.LightningDataModule</code>相当于一个功能加强版的torch Dataset，加强的功能包括：</p><ul><li><code>prepare_data(self)</code>：<ul><li>最最开始的时候，进行一些无论GPU有多少只要执行一次的操作，如写入磁盘的下载操作、分词操作(tokenize)等。</li><li>这里是一劳永逸式准备数据的函数。</li><li>由于只在单线程中调用，不要在这个函数中进行<code>self.x=y</code>似的赋值操作。</li><li>但如果是自己用而不是给大众分发的话，这个函数可能并不需要调用，因为数据提前处理好就好了。</li></ul></li><li><code>setup(self, stage=None)</code>：<ul><li>实例化数据集（Dataset），并进行相关操作，如：清点类数，划分train/val/test集合等。</li><li>参数<code>stage</code>用于指示是处于训练周期(<code>fit</code>)还是测试周期(<code>test</code>)，其中，<code>fit</code>周期需要构建train和val两者的数据集。</li><li>setup函数不需要返回值。初始化好的train/val/test set直接赋值给self即可。</li></ul></li><li><code>train_dataloader/val_dataloader/test_dataloader</code>：<ul><li>初始化<code>DataLoader</code>。</li><li>返回一个DataLoader量。</li></ul></li></ul></li></ul><h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNISTDataModule</span>(<span class="params">pl.LightningDataModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_dir: <span class="built_in">str</span> = <span class="string">&#x27;./&#x27;</span>, batch_size: <span class="built_in">int</span> = <span class="number">64</span>, num_workers: <span class="built_in">int</span> = <span class="number">8</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line"></span><br><span class="line">        self.transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.dims is returned when you call dm.size()</span></span><br><span class="line">        <span class="comment"># Setting default dims here because we know them.</span></span><br><span class="line">        <span class="comment"># Could optionally be assigned dynamically in dm.setup()</span></span><br><span class="line">        self.dims = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        self.num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># download</span></span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;fit&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">            self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assign test dataset for use in dataloader(s)</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;test&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mnist_test = MNIST(self.data_dir, train=<span class="literal">False</span>, transform=self.transform)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">val_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)</span><br></pre></td></tr></table></figure><h3 id="要点-2"><a class="markdownIt-Anchor" href="#要点-2"></a> 要点</h3><ul><li>若在DataModule中定义了一个<code>self.dims</code> 变量，后面可以调用<code>dm.size()</code>获取该变量。</li></ul><h2 id="saving-and-loading"><a class="markdownIt-Anchor" href="#saving-and-loading"></a> Saving and Loading</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html">主页面</a></p><h3 id="saving"><a class="markdownIt-Anchor" href="#saving"></a> Saving</h3><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint">ModelCheckpoint</a>: 自动储存的callback module。默认情况下training过程中只会自动储存最新的模型与相关参数，而用户可以通过这个module自定义。如观测一个<code>val_loss</code>的量，并储存top 3好的模型，且同时储存最后一个epoch的模型，等等。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span></span><br><span class="line">checkpoint_callback = ModelCheckpoint(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    filename=<span class="string">&#x27;sample-mnist-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">    save_top_k=<span class="number">3</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    save_last=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer(gpus=<span class="number">1</span>, max_epochs=<span class="number">3</span>, progress_bar_refresh_rate=<span class="number">20</span>, callbacks=[checkpoint_callback])</span><br></pre></td></tr></table></figure></li><li><p>另外，也可以手动存储checkpoint: <code>trainer.save_checkpoint(&quot;example.ckpt&quot;)</code></p></li><li><p><code>ModelCheckpoint</code> Callback中，如果<code>save_weights_only =True</code>，那么将会只储存模型的权重（相当于<code>model.save_weights(filepath)</code>），反之会储存整个模型（相当于<code>model.save(filepath)</code>）。</p></li></ul><h3 id="loading"><a class="markdownIt-Anchor" href="#loading"></a> Loading</h3><ul><li><p>load一个模型，包括它的weights、biases和超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightingModule.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line">print(model.learning_rate)</span><br><span class="line"><span class="comment"># prints the learning_rate you used in this checkpoint</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">y_hat = model(x)</span><br></pre></td></tr></table></figure></li><li><p>load模型时替换一些超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitModel</span>(<span class="params">LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim, out_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.l1 = nn.Linear(self.hparams.in_dim, self.hparams.out_dim)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># if you train and save the model like this it will use these values when loading</span></span><br><span class="line"><span class="comment"># the weights. But you can overwrite this</span></span><br><span class="line">LitModel(in_dim=<span class="number">32</span>, out_dim=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=32, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=128, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH, in_dim=<span class="number">128</span>, out_dim=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p>完全load训练状态：load包括模型的一切，以及和训练相关的一切参数，如<code>model, epoch, step, LR schedulers, apex</code>等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = LitModel()</span><br><span class="line">trainer = Trainer(resume_from_checkpoint=<span class="string">&#x27;some/path/to/my_checkpoint.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically restores model, epoch, step, LR schedulers, apex, etc...</span></span><br><span class="line">trainer.fit(model)</span><br></pre></td></tr></table></figure></li></ul><h2 id="callbacks"><a class="markdownIt-Anchor" href="#callbacks"></a> Callbacks</h2><ul><li>Callback 是一个自包含的程序，可以与训练流程交织在一起，而不会污染主要的研究逻辑。</li><li>Callback 并非只会在epoch结尾调用。pytorch-lightning 提供了数十个hook（接口，调用位置）可供选择，也可以自定义callback，实现任何想实现的模块。</li><li>推荐使用方式是，随问题和项目变化的操作，这些函数写到lightning module里面，而相对独立，相对辅助性的，需要复用的内容则可以定义单独的模块，供后续方便地插拔使用。</li></ul><h3 id="callbacks推荐"><a class="markdownIt-Anchor" href="#callbacks推荐"></a> Callbacks推荐</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks">内建Callbacks</a></p><ul><li><p><code>EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)</code>：根据某个值，在数个epoch没有提升的情况下提前停止训练。</p><blockquote><p>参数：</p><ul><li><strong>monitor</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – quantity to be monitored. Default: <code>'early_stop_on'</code>.</li><li><strong>min_delta</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><code>float</code></a>) – minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: <code>0.0</code>.</li><li><strong>patience</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – number of validation epochs with no improvement after which training will be stopped. Default: <code>3</code>.</li><li><strong>verbose</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – verbosity mode. Default: <code>False</code>.</li><li><strong>mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – one of <code>'min'</code>, <code>'max'</code>. In <code>'min'</code> mode, training will stop when the quantity monitored has stopped decreasing and in <code>'max'</code> mode it will stop when the quantity monitored has stopped increasing.</li><li><strong>strict</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – whether to crash the training if monitor is not found in the validation metrics. Default: <code>True</code>.</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">trainer = Trainer(callbacks=[early_stopping])</span><br></pre></td></tr></table></figure></li><li><p><code>ModelCheckpoint</code>：见上文<strong>Saving and Loading</strong>.</p></li><li><p><code>PrintTableMetricsCallback</code>：在每个epoch结束后打印一份结果整理表格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pl_bolts.callbacks <span class="keyword">import</span> PrintTableMetricsCallback</span><br><span class="line"></span><br><span class="line">callback = PrintTableMetricsCallback()</span><br><span class="line">trainer = pl.Trainer(callbacks=[callback])</span><br><span class="line">trainer.fit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># at the end of every epoch it will print</span></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss│train_loss│val_loss│epoch</span></span><br><span class="line"><span class="comment"># ──────────────────────────────</span></span><br><span class="line"><span class="comment"># 2.2541470527648926│2.2541470527648926│2.2158432006835938│0</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="logging"><a class="markdownIt-Anchor" href="#logging"></a> Logging</h2><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html">Logging</a>：Logger默认是TensorBoard，但可以指定各种主流Logger<a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#supported-loggers">框架</a>，<a href="http://xn--Comet-gv5i.ml">如Comet.ml</a>，MLflow，Netpune，或直接CSV文件。可以同时使用复数个logger。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> loggers <span class="keyword">as</span> pl_loggers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(</span><br><span class="line">    save_dir=os.getcwd(),</span><br><span class="line">    version=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;lightning_logs&#x27;</span></span><br><span class="line">)</span><br><span class="line">trainer = Trainer(logger=tb_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or use the same format as others</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One Logger</span></span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=comet_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save code snapshot</span></span><br><span class="line">logger = pl_loggers.TestTubeLogger(<span class="string">&#x27;logs/&#x27;</span>, create_git_tag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Logger</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=[tb_logger, comet_logger])</span><br></pre></td></tr></table></figure><p>默认情况下，每50个batch log一次，可以通过调整参数</p></li><li><p>如果想要log输出非scalar（标量）的内容，如图片，文本，直方图等等，可以直接调用<code>self.logger.experiment.add_xxx()</code>来实现所需操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">...</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># the logger you used (in this case tensorboard)</span></span><br><span class="line">    tensorboard = self.logger.experiment</span><br><span class="line">    tensorboard.add_image()</span><br><span class="line">    tensorboard.add_histogram(...)</span><br><span class="line">    tensorboard.add_figure(...)</span><br></pre></td></tr></table></figure></li><li><p>使用log：如果是TensorBoard，那么：<code>tensorboard --logdir ./lightning_logs</code>。在Jupyter Notebook中，可以使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start tensorboard.</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir lightning_logs/</span><br></pre></td></tr></table></figure><p>在行内打开TensorBoard。</p></li><li><p>小技巧：如果在局域网内开启了TensorBoard，加上flag <code>--bind_all</code>即可使用主机名访问：</p><p><code>tensorboard --logdir lightning_logs --bind_all</code> -&gt; <code>http://SERVER-NAME:6006/</code></p></li></ul><h3 id="同时使用tensorboard和csv-logger"><a class="markdownIt-Anchor" href="#同时使用tensorboard和csv-logger"></a> 同时使用TensorBoard和CSV Logger</h3><p>如果同时使用两个Logger，PL会有睿智操作：如果保存根目录相同，他们会依次建立两个version文件夹，令人窒息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> TensorBoardLogger, CSVLogger</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_loggers</span>():</span></span><br><span class="line">    loggers = []</span><br><span class="line">    loggers.append(TensorBoardLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tb&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CSVLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;csv&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CometLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tt&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loggers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_callbacks</span>(<span class="params">logger</span>):</span></span><br><span class="line">    callbacks = []</span><br><span class="line">    dirpath = <span class="string">f&#x27;lightning_logs/<span class="subst">&#123;logger.name&#125;</span>/version_<span class="subst">&#123;logger.version&#125;</span>/checkpoints&#x27;</span></span><br><span class="line">    callbacks.append(ModelCheckpoint(</span><br><span class="line">        dirpath=dirpath,</span><br><span class="line">        monitor=<span class="string">&#x27;loss_epoch&#x27;</span>,</span><br><span class="line">        filename=<span class="string">&#x27;&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">        save_top_k=<span class="number">3</span>,</span><br><span class="line">        mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">        save_last=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> callbacks</span><br><span class="line"></span><br><span class="line">loggers = load_loggers()</span><br><span class="line">callbacks = load_callbacks(loggers[<span class="number">0</span>])</span><br><span class="line">trainer = pl.Trainer(logger=loggers, callbacks=callbacks)</span><br></pre></td></tr></table></figure><h2 id="transfer-learning"><a class="markdownIt-Anchor" href="#transfer-learning"></a> Transfer Learning</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html#transfer-learning">主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagenetTransferLearning</span>(<span class="params">LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a pretrained resnet</span></span><br><span class="line">        backbone = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">        num_filters = backbone.fc.in_features</span><br><span class="line">        layers = <span class="built_in">list</span>(backbone.children())[:-<span class="number">1</span>]</span><br><span class="line">        self.feature_extractor = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use the pretrained model to classify cifar-10 (10 image classes)</span></span><br><span class="line">        num_target_classes = <span class="number">10</span></span><br><span class="line">        self.classifier = nn.Linear(num_filters, num_target_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            representations = self.feature_extractor(x).flatten(<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(representations)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h2 id="关于device操作"><a class="markdownIt-Anchor" href="#关于device操作"></a> 关于device操作</h2><p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># good (self is LightningModule)</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br></pre></td></tr></table></figure><p>For tensors that need to be model attributes, it is best practice to register them as buffers in the modules’s <code>__init__</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">self.t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;t&quot;</span>, torch.rand(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>前面两段是教程中的文本。然而实际上有一个暗坑：</p><p>如果你使用了一个中继的<code>pl.LightningModule</code>，而这个module里面实例化了某个普通的<code>nn.Module</code>，而这个模型中又需要内部生成一些tensor，比如图片每个通道的mean，std之类，那么如果你从<code>pl.LightningModule</code>中pass一个<code>self.device</code>，实际上在一开始这个<code>self.device</code>永远是<code>cpu</code>。所以如果你在调用的<code>nn.Module</code>的<code>__init__()</code>中初始化，使用<code>to(device)</code>或干脆什么都不用，结果就是它永远都在<code>cpu</code>上。</p><p>但是，经过实验，虽然<code>pl.LightningModule</code>在<code>__init__()</code>阶段<code>self.device</code>还是<code>cpu</code>，当进入了<code>training_step()</code>之后，就迅速变为了<code>cuda</code>。所以，对于子模块，最佳方案是，使用一个<code>forward</code>中传入的量，如<code>x</code>，作为一个reference变量，用<code>type_as</code>函数将在模型中生成的tensor都放到和这个参考变量相同的device上即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RDNFuse</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_norm_func</span>(<span class="params">self, ref</span>):</span></span><br><span class="line">        self.mean = torch.tensor(np.array(self.mean_sen), dtype=torch.float32).type_as(ref)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mean&#x27;</span>):</span><br><span class="line">            self.init_norm_func(x)</span><br></pre></td></tr></table></figure><h2 id="关于limit_train_batches选项"><a class="markdownIt-Anchor" href="#关于limit_train_batches选项"></a> 关于<code>limit_train_batches</code>选项</h2><p>这里涉及到一个问题，就是每个epoch使用部分数据而非全部时，程序将会怎么工作。</p><blockquote><p>The shuffling happens when the iterator is created. In the case of the for loop, that happens just before the for loop starts. You can create the iterator manually with:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator gets created, the data has been shuffled at this point.</span></span><br><span class="line">data_iterator = <span class="built_in">iter</span>(namesTrainLoader)</span><br></pre></td></tr></table></figure><blockquote><p>By default the data loader uses <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler"><code>torch.utils.data.RandomSampler</code></a> if you set <code>shuffle=True</code> (without providing your own sampler). Its implementation is very straight forward and you can see where the data is shuffled when the iterator is created by looking at the <a href="https://github.com/pytorch/pytorch/blob/f3e620ee83f080283445aa1a7242d40e30eb6a7f/torch/utils/data/sampler.py#L103-L107"><code>RandomSampler.__iter__</code></a> method:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">    <span class="keyword">if</span> self.replacement:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br></pre></td></tr></table></figure><blockquote><p>The return statement is the important part, where the shuffling takes place. It simply creates a random permutation of the indices.</p><p>That means you will see your entire dataset every time you fully consume the iterator, just in a different order every time. Therefore there is no data lost (not including cases with <code>drop_last=True</code>) and your model will see all data at every epoch.</p></blockquote><p>总结下来，如果使用了<code>shuffle=True</code>选项，那么即使每次都不跑完整个epoch，你还是有机会见到所有的数据的。数据集的shuffle发生在<code>iter</code>被创建的时候，在我们一般的代码中，也就是内层for循环开始时。但如果你没有选择<code>shuffle=True</code>，那你将永远只能看到你设定的前面N个数据。</p><h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ul><li><p><code>pl.seed_everything(1234)</code>：对所有相关的随机量固定种子。</p></li><li><p>使用LR Scheduler时候，不用自己<code>.step()</code>。它也被Trainer自动处理了。<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html?highlight=scheduler#">Optimization 主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single optimizer</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">        loss = model.training_step(batch, batch_idx, ...)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Multiple optimizers</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">     <span class="keyword">for</span> opt <span class="keyword">in</span> optimizers:</span><br><span class="line">        disable_grads_for_other_optimizers()</span><br><span class="line">        train_step(opt)</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">     scheduler.step()</span><br></pre></td></tr></table></figure></li><li><p>关于划分train和val集合的方法。与PL无关，但很常用，两个例子：</p><ol><li><code>random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</code></li><li>如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br></pre></td></tr></table></figure><p>Parameters：</p><ul><li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</li><li><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</li><li><strong>generator</strong> (<a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>Generator</em></a>) – Generator used for the random permutation.</li></ul></li><li><p>如果使用了<code>PrintTableMetricsCallback</code>，那么<code>validation_step</code>不要return内容，否则会炸。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch-lighting&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#pytorch-lighting&quot;&gt;&lt;/a&gt; Pytorch-Lighting&lt;/h1&gt;
&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownI</summary>
      
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
    <category term="pytorch-lightning" scheme="https://www.miracleyoo.com/tags/pytorch-lightning/"/>
    
  </entry>
  
  <entry>
    <title>Mac、Windows、iPad三端完美论文管理、阅读与编辑系统配置</title>
    <link href="https://www.miracleyoo.com/2020/04/26/paper-zotero-dropbox/"/>
    <id>https://www.miracleyoo.com/2020/04/26/paper-zotero-dropbox/</id>
    <published>2020-04-26T22:09:00.000Z</published>
    <updated>2020-04-26T22:09:00.050Z</updated>
    
    <content type="html"><![CDATA[<p>管理好自己手中的论文，不让他们被吞噬与“Download”文件夹的茫茫文件海洋中已非易事，而能在需要时迅速定位，能在Windows的Desktop与便携的MacBook上无缝对接论文库则需要相对精细化的管理。然而，很多时候并不适合展开自己的笔记本电脑来“郑重”地阅读一篇论文，在一些相对零碎的时间里随手抽出包中的iPad，读读前面顺手存下的论文，则可大大增加自己的学术幸福感。</p><h2 id="需求分析"><a class="markdownIt-Anchor" href="#需求分析"></a> 需求分析</h2><p>上面这几点也正是我对论文管理系统的要求。注意这里提到的是“系统”，而并是一个单一的软件。这里提炼一下日常对论文管理及阅读的需求，您可以看看和自己的需求是否吻合：</p><ol><li>以分层目录的形式将论文进行归档，并且有时需要让同一篇论文同时存放于多个目录中。</li><li>支持多及目录（多&gt;2）。</li><li>支持从网页上便捷地储存论文以及文档。</li><li>支持拖入PDF自动寻找论文信息。</li><li>需要对论文的PDF原件进行存储，最好能够自动下载缺失的PDF。</li><li>需要在Mac和Windows端都能访问论文目录，且对PDF文件的修改能够同步。</li><li>PDF的存储最好和数据库系统分离，以便搜索、单独更改或访问。</li><li>需要对论文进行可自定义格式的自动重命名。</li><li>界面不能太丑。</li><li>支持高度自定义或有充足的功能，最好可以使用第三方插件。</li><li>支持各种引文格式。</li><li>支持Latex和Word的便捷引用，Word最好有插件。</li><li>拥有较大的云存储空间，至少足以存储所有的PDF文档。</li><li>可以从iPad上访问并且可以同步、上传对论文的更改。</li><li>三个平台PDF阅读器配置最好统一，以免高亮、插入文本等格式不一。</li></ol><p>归纳出的这几点便是我对论文管理系统的所有需求了。之后便是对各种论文管理软件的试用和组合。这里直奔主题，给出我现在的全套系统配置及选择的原因。</p><h2 id="系统配置"><a class="markdownIt-Anchor" href="#系统配置"></a> 系统配置</h2><ul><li>Mac与Windows端主论文管理软件：<a href="https://www.zotero.org/">Zotero</a></li><li>主要插件：<a href="http://zotfile.com/">zotfile</a>，<a href="https://github.com/retorquere/zotero-better-bibtex">zotero-better-bibtex</a></li><li>Mac、Windows与iPad统一PDF同步软件：Dropbox</li><li>Mac、Windows与iPad统一PDF阅读器：Adobe Acrobat</li></ul><h2 id="选用原因"><a class="markdownIt-Anchor" href="#选用原因"></a> 选用原因</h2><h3 id="zotero"><a class="markdownIt-Anchor" href="#zotero"></a> Zotero</h3><p>各大平台上以及Zotero官网对其讲解都很多也很充分了，如果你能够并且愿意花上一些时间来进行自定义配置，它将是一个可以满足几乎所有对论文管理需求的终极软件。我这里只指出一些我认为非常不错的特性。</p><ol><li>跨平台。Mac，Windows和Linux都有支持。</li><li>浏览器论文抓取插件很好用。既可直接抓取PDF文章之后解析出论文，也可在如Google Scholar等页面直接批量抓取添加论文索引，并再自动下载相应的PDF。甚至在一些作业性质的小论文中有时需要直接引用某个网页或某个文档，它也可以直接生成条目。</li><li>支持多级目录。这个多理论上似乎可以无限多下去，非常自由。</li><li>有很好用的Word插件，使用体验丝滑流畅。</li><li>拥有在线的庞大引用格式库，基本可以找到所有需要的引文格式。</li><li>可以加载很多第三方插件，如支持自定义格式重命名的zotfile，针对输出引用进行优化的better-bibtex等。</li><li>可以将条目的PDF文件储存目录单列出来，存到一个自定义的地方，比如Dropbox文件夹内部。</li></ol><h3 id="dropbox"><a class="markdownIt-Anchor" href="#dropbox"></a> Dropbox</h3><ol><li>因为现在在美国读书，Dropbox的服务相对来说是快速且稳定的。</li><li>Dropbox的同步功能做的非常好，这也是其在此领域深耕多年的结果。</li><li>最重要一点，Dropbox在iPad上和Adobe Acrobat有着原生的集成。在Dropbox中使用Acrobat打开并编辑文件，其修改是可以直接同步到Dropbox云端的。这一点对于移动端浏览和编辑论文起到了至关重要的作用。</li><li>免费版虽然只有2G初始空间，但是可以通过邀请好友注册达到最大的18G。虽然不是特别大，但是对于存储论文已经绰绰有余了。（另这个邀请注册送空间可以直接去淘宝搜索，可以画很少的钱得到很多注册服务，直达18G。）</li></ol><h3 id="adobe-acrobat"><a class="markdownIt-Anchor" href="#adobe-acrobat"></a> Adobe Acrobat</h3><ol><li>多平台支持。Mac，Windows，Linux，iOS，Android都有着很好的支持。</li><li>专业，支持各种编辑方式。常见的高亮、下划线、添加文本、画方框、做批注等自然都是支持的。</li><li>很多高校都和Adobe公司有着协议，使用学生邮箱可以直接免费使用全套功能。</li><li>同前面所述，其和Dropbox的关联性支持是整套系统成功的关键。</li></ol><h3 id="对比原因"><a class="markdownIt-Anchor" href="#对比原因"></a> 对比原因</h3><ol><li>之前有在用Mendeley，但是对比Zotero，尤其是对比Zotero和其第三方插件的丰富功能后，前者明显力不从心。另外Windows版本其界面没有针对高分辨率进行适配，并且使用了默认的宋体，显示英文丑陋不堪，且无法更改。在论坛上看到有许多坛友几年前就提出了这些问题，然而显然开发者并没有做出相应。对比Zotero，其论坛环境、活跃程度以及问题解决速度都会更好。</li><li>Endnote是收费软件，我并没有深入使用，这里不多置评，但各位可以容易查到Endnote和Zotero的区别。</li><li>iPad上也考虑过使用PDF Reader – Document Expert作为浏览和编辑软件，但是从Dropbox打开的文件编辑后是以副本的形式存在了本地，并没有被同步，这是无法接受的。</li></ol><h2 id="配置细节"><a class="markdownIt-Anchor" href="#配置细节"></a> 配置细节</h2><h3 id="zotero设置部分"><a class="markdownIt-Anchor" href="#zotero设置部分"></a> Zotero设置部分</h3><ol><li>在所有需要同步的电脑上登录Zotero的账号，如没有，请注册。</li><li>打开设置，更改<code>Files and Folders</code> 中的<code>Base Directory</code>选项为你的同步盘地址，如Dropbox，OneDrive，坚果云等。请不要动下面的<code>Data Directory Location</code>，这个是Zotero的总体数据库的地址，不建议放到云文件夹下，因为只要有两个端同时使用Zotero这个同步就崩掉了。</li></ol><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110225913571.png" alt="image-20191110225913571"></p><ol start="3"><li>安装<a href="http://zotfile.com/">zotfile</a>。更改有关文件的设置。</li></ol><p>从Tools栏进入ZotFile Preference</p><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230601630.png" alt="image-20191110230601630"></p><p>更改PDF文件存储位置。这里是把文件储存到云盘的关键步骤。上面那个更改文件存储地址的作用是指定未来加入的PDF文件的存储地址，而这里是把已经在库的文件移动到这个地址。两个地址相同。</p><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230944192.png" alt="image-20191110230944192"></p><p>更改重命名相关的设置。这里的%y就是论文发表年份，%j是期刊名，%t是论文标题。而中间的下划线则只是单纯的会在重命名后的文件名中的两个元素之间加一个下滑线罢了，这里可以替换做任意。</p><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230654373.png" alt="image-20191110230654373"></p><p>在你的所有需要同步的电脑上做完上述步骤后，如果你之前没有Zotero或它是全新的没有条目，那你的设定已经结束。如果库中已有很多论文，想要直接移动到Dropbox相应目录下，那么请执行下一步：</p><ol start="4"><li>移动与重命名已有文件</li></ol><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110231928468.png" alt="image-20191110231928468"></p><p>点击 My Library，全选所有条目，右键选择<code>Manage Attachments</code>-&gt;<code>Rename Attachments</code>开始移动和重命名。</p><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110232045736.png" alt="image-20191110232045736"></p><p><img src="/2020/04/26/paper-zotero-dropbox/image-20191110232150562.png" alt="image-20191110232150562"></p><h3 id="ipad设置部分"><a class="markdownIt-Anchor" href="#ipad设置部分"></a> iPad设置部分</h3><ol><li>下载Dropbox和Acrobat。</li><li>打开Dropbox，进入你的论文同步文件夹，任选一篇点击打开</li><li>右下角找到一个光标键，点击，会提示用Adobe Acrobat Reader打开。</li><li>All set.</li></ol><p><img src="/2020/04/26/paper-zotero-dropbox/IMG_2601.jpg" alt="IMG_2601"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;管理好自己手中的论文，不让他们被吞噬与“Download”文件夹的茫茫文件海洋中已非易事，而能在需要时迅速定位，能在Windows的Desktop与便携的MacBook上无缝对接论文库则需要相对精细化的管理。然而，很多时候并不适合展开自己的笔记本电脑来“郑重”地阅读一篇论文</summary>
      
    
    
    
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>避免脏活，完美使用Markdown在知乎编辑内容</title>
    <link href="https://www.miracleyoo.com/2020/04/26/markdown-4-zhihu/"/>
    <id>https://www.miracleyoo.com/2020/04/26/markdown-4-zhihu/</id>
    <published>2020-04-26T22:01:57.000Z</published>
    <updated>2020-04-26T22:04:53.940Z</updated>
    
    <content type="html"><![CDATA[<p>知乎上的本文链接：<a href="https://zhuanlan.zhihu.com/p/97455277">Link</a></p><p>首先吐槽一下知乎的编辑器。虽然个人博客上的不少内容都曾有想过搬到知乎一份，但是知乎的编辑器真的是令人绝望式的难用。尽管现在可以使用文件导入功能导入md文件和Word文档，且能支持一些简单的Markdown语法，但每种途径都有着无法避免的缺点，从结果上来说则是只能被迫接受或是不完美的格式亦或是大量手动且重复的图片上传。</p><p>口说无凭，这里放一下几种方法的对比图来详述一下问题所在：</p><h3 id="typora中原文件"><a class="markdownIt-Anchor" href="#typora中原文件"></a> Typora中原文件</h3><img src="/2020/04/26/markdown-4-zhihu/image-20191214174243537.png" alt="image-20191214174243537" style="zoom:50%;"><p>这份测试文件虽然短，但是基本包含了常见几种要素：标题、正文、图片、表格、代码、公式。下面让我们看看知乎支持的几种上传方式的效果：</p><h3 id="1-直接复制typora中的内容到知乎编辑器"><a class="markdownIt-Anchor" href="#1-直接复制typora中的内容到知乎编辑器"></a> 1. 直接复制Typora中的内容到知乎编辑器</h3><img src="/2020/04/26/markdown-4-zhihu/image-20191214174118623.png" alt="image-20191214174118623" style="zoom:50%;"><p>可以看到，标题和正文区分开了，不过所有的标题都变成了一级标题。另外本地的图片无法导入，只剩下一个展占位符。表格全乱，公式直接消失了。但代码的高亮仍是C++，正确。</p><h3 id="2-直接导入markdown文件"><a class="markdownIt-Anchor" href="#2-直接导入markdown文件"></a> 2. 直接导入Markdown文件</h3><p>你可以在编辑器的这个位置导入文件：</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174529632.png" alt="image-20191214174529632" style="zoom:33%;"><img src="/2020/04/26/markdown-4-zhihu/image-20191214174506716.png" alt="image-20191214174506716" style="zoom:33%;"><p>导入刚才我们看到的测试文件原档的效果是这样的：</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174704023.png" alt="image-20191214174704023" style="zoom:50%;"><p>同前，标题和正文区分开了，不过所有的标题都变成了一级标题。另外本地的图片无法导入，只剩下一个占位符。表格全乱，公式没有消失，但也并没有被渲染。代码的高亮仍是C++，正确。</p><h3 id="3-先使用typora导出为word再用知乎编辑器导入word"><a class="markdownIt-Anchor" href="#3-先使用typora导出为word再用知乎编辑器导入word"></a> 3. 先使用Typora导出为Word，再用知乎编辑器导入Word</h3><p>上面的两种最直观的方法的一大问题就是图片导入不进去。而对于一些长篇的科技文章，图片既多又重要，手动一个个添加容易错而且浪费科研人员的时间和热情。当然我知道导入Markdown时并没有顺带把图片本身导入进去，但我仍觉得这是知乎团队应该做的工作，而且是相当基本的工作。好吧，既然现在不可行，那么导出成Word再导入该不会有这个问题了吧，我们来看看：</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214175626414.png" alt="image-20191214175626414" style="zoom:50%;"><p>好家伙，图片导入进去了，表格直接炸飞天了，而且更可气的是代码的高亮没了，格式也出现了问题。其他的嘛，不看不得了，一看发现公式似乎直接没了，中间还莫名其妙多了一堆空行。当然，标题等级的问题还是没解决。</p><p>那是Typora导出Word导出的不好吗？我打开了导出的Word文件：</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214180048117.png" alt="image-20191214180048117" style="zoom:50%;"><p>公式存在，高亮正确，标题等级正确，表格正确，没有奇怪的空行。虽然和Markdown渲染的结果相比也并不好看说实话，但至少它是对的，而知乎编辑器错的五花八门。</p><h2 id="那么如何拯救自己的双手和灵魂呢"><a class="markdownIt-Anchor" href="#那么如何拯救自己的双手和灵魂呢"></a> 那么，如何拯救自己的双手和灵魂呢？</h2><h3 id="首先调整好你的markdown编辑器"><a class="markdownIt-Anchor" href="#首先调整好你的markdown编辑器"></a> 首先调整好你的Markdown编辑器</h3><p>为什么要首先调整好编辑器呢？这里我说的调整主要指的是对图片管理方式的调整。如果您使用Typora，建议在偏好设置页面将相关参数调整至和下图完全一致，以防后面出现问题。</p><p>这里做的工作主要是将所有来源的图片都自动保存至同名文件夹下，以相对路径储存。使用其他Markdown编辑器的小伙伴也可以对照调整。这么做的目的主要是为了方便后面对图片的批量上传与转换。</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214221536561.png" alt="image-20191214221536561" style="zoom:50%;"><p>相信很多同学看到这里就会发出疑问，为什么不适用iPic之类的图床软件直接上传至图床呢？既方便又舒适。我的答案是，因为我吃过亏。我的内容之前一直独发于我的个人博客，然而今年中旬，突然之间整个网站所有的图片都挂掉了，只显示一个占位符和无法访问的提示，之后我发现之前使用的新浪图床加入了防盗链，所以就GG了。当然后面我也用Python再一次解决了这个问题，对解决方法感兴趣的图形可以移步这里，然而这一次的教训让我理解了这些图床<strong>并不可控</strong>。它们随时可以剥夺掉你博客中的全部图片，而你是无力至极的。</p><p>在那之后，我就选择了本地储存+Github备份的模式，这样既可以永久有安心的本地档，也有方便使用的Github链接，可以说是既方便又安全。</p><h3 id="之后解决图片上传问题"><a class="markdownIt-Anchor" href="#之后解决图片上传问题"></a> 之后解决图片上传问题</h3><p>最方便的解决办法即为利用好GitHub的资源了。建立一个Public的GitHub仓库，这里我命名作**<a href="https://github.com/miracleyoo/Markdown4Zhihu">Markdown4Zhihu</a>**，注意一定要为Public，否则知乎无法访问这些图片。</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214221339326.png" alt="image-20191214221339326" style="zoom:50%;"><p>当然，如果你觉得麻烦，也可以直接folk我建好的仓库，一会儿我们要提到的“一键Markdown知乎适配脚本”也会在这个仓库里。你只需要将你的文件和相应的图片文件夹放到这个<code>Data</code>子目录下，即可调用脚本一键转换，并将涉及到的图片自动推到你相应的GitHub仓库中。</p><img src="/2020/04/26/markdown-4-zhihu/image-20191214213407292.png" alt="image-20191214213407292" style="zoom:50%;"><p>这是我们使用脚本一键转换后的结果。它很好的解决的图片上传的问题，同时也保证了代码段的高亮，同时，所有的行内公式和多行公式都得到了转换。转换后的公式在知乎上传文件之后，是可交互的，即你可以在上传之后在知乎编辑器中修改你的公式，而不必重新再来一遍。</p><p>至于表格，这个真木得办法，因为知乎压根不支持表格你说这咋整嘛。但是也不是没有可替代方案。如果表格不是很多，你可以直接对其进行截图，删去原代码后粘贴截图。之后它就会按照图片模式被兼容上去。如果你不想截图也可，做了相应操作后会得到上图的结果，对于少量表格来看也是OK的。你可以在<a href="https://zhuanlan.zhihu.com/p/97432671">这里</a>看到上传到知乎后的效果。</p><h3 id="最后是具体使用流程"><a class="markdownIt-Anchor" href="#最后是具体使用流程"></a> 最后是具体使用流程</h3><p>这里我们假设您的文件名为<code>一个测试文档.md</code>，并将其和同名图片文件夹放到<code>Data</code>目录下（如果新建文件时就直接在Data里面建会更加方便），接着打开terminal(Linux/MacOS)或Git Bash(Windows)(或其他任何支持Git命令的终端)，<code>cd</code>进入该项目的根目录，即<code>Markdown4Zhihu</code>目录，输入以下命令：</p><p><code>python zhihu-publisher.py --input=&quot;./Data/一个测试文档.md&quot;</code></p><p>OK，all set. 在<code>Data</code>目录下，你可以看到一个<code>一个测试文档_for_zhihu.md</code>的文件，将它上传至知乎编辑器即可。</p><p>PS: 脚本使用Python3，Python2可能会有潜在问题。</p><h2 id="最后的话"><a class="markdownIt-Anchor" href="#最后的话"></a> 最后的话</h2><p>知乎的开发者的逻辑其实我真的比较迷，我们大学学生团队的自建论坛都可以原生完美支持Markdown和公式，然而知乎却一直说这个功能必要性不足强调开发难度。同样令人难受的是知乎的搜索，多少年过去了非热门话题还是一如既往的难用，搜索还是借助Google 的 “问题+知乎”。不知道这是什么原因，不过还是希望知乎团队先把这些非常基础的东西做好再大谈用户体验。</p><p>这次的解决方案需要对GitHub和命令行有基础的了解，不过考虑到会来读这篇文章的人应该程序员居多，问题应该不是很大。脚本还比较新，如果有bug欢迎提出。最后再放一下GitHub链接，如果它有帮到你，希望能随手留下一个star，谢谢！<strong><a href="https://github.com/miracleyoo/Markdown4Zhihu">Markdown4Zhihu</a></strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;知乎上的本文链接：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/97455277&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先吐槽一下知乎的编辑器。虽然个人博客上的不少内容都曾有想过搬到知乎一份，但是知乎的编辑器真的是令人绝望式的难用。尽管现在可以使</summary>
      
    
    
    
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
    <category term="markdown" scheme="https://www.miracleyoo.com/tags/markdown/"/>
    
    <category term="math" scheme="https://www.miracleyoo.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Docker与Nivida-Docker的用法与注意事项</title>
    <link href="https://www.miracleyoo.com/2020/04/26/docker-et-nvidia/"/>
    <id>https://www.miracleyoo.com/2020/04/26/docker-et-nvidia/</id>
    <published>2020-04-26T21:56:52.000Z</published>
    <updated>2020-04-26T21:56:52.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker-images-and-containers"><a class="markdownIt-Anchor" href="#docker-images-and-containers"></a> Docker Images and Containers</h2><ul><li><p>清除所有已经停止的container：<code>docker container prune -f</code> 。其中 <code>-f</code> 表示不弹出确认提示。也可使用<code>docker rm $(docker ps -a -q)</code>来清理。其中，<code>docker rm</code>代表删除container，而<code>docker rmi</code>则是删除image。</p></li><li><p>如果你需要实例化一个只用一次的container，那么使用<code>docker run --rm</code>参数，结束后会自动删除。</p></li><li><p><code>docker ps &lt;-a&gt;</code> 可以列出正在运行的/全部的container。其效果和<code>docker container ls &lt;-a&gt;</code>相同。而若想列出全部images，则要使用<code>docker images</code>。</p></li><li><p>docker images中的环境变量有四个来源：</p><ol><li>Dockerfile中通过<code>ENV</code>指令添加的环境变量，如<code>ENV PATH /opt/conda/bin:$PATH</code></li><li>Dockerfile中通过修改<code>/root/.bashrc</code>文件使用<code>export</code>命令添加到bash中的环境变量，如<code>export PATH=/OPT/conda/bin:$PATH</code>命令。</li><li>在通过image实例化container时添加<code>-e</code>或<code>--env</code>参数来添加到环境中的变量。这个方法有局限性，它不能完成对已有变量的“添加”操作，只能新建一个新的环境变量，如<code>--env NEW_VAR=/opt/conda/bin</code></li><li>在<code>docker run</code>末端的container内命令的前面添加一句<code>export</code>引导的命令，如：<code>docker run -it -v $(PWD):/app debian:jessie bash -c 'export PATH=$PATH:/opt/conda/bin; bash'</code>。它的缺点是较为复杂。</li></ol><p>其中，如果能找到源Dockerfile，最好的方法是通过修改Dockerfile然后重新build得到一个自己的版本。其次是方法三，实在不行使用方法四。如果先进入bash再运行命令可以正常运行，而直接使用<code>docker run</code>出现了环境变量相关的失败提示，很可能是由于Dockerfile写的时候使用的是在<code>/root/.bashrc</code>中添加环境变量的方法所致。</p></li><li><p>如果需要一个container长期在后台待机候命，那可以使用<code>-d</code>或<code>--detach</code>选项建立一个一直待机的docker进程。使用方法：</p><ol><li><code>docker run -itd --name NAME xxx/xxx:xx /bin/bash</code></li><li><code>docker exec -it NAME your-command</code></li></ol></li><li><p>启动时如果需要对本地文件夹和Docker内部文件夹做映射，则使用<code>docker run -v &lt;LOCAL_FOLDER&gt;:&lt;DOCKER_FOLDER&gt;</code> 。 该参数可以复数次出现，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run \</span><br><span class="line">    -v <span class="variable">$AUDIO_IN</span>:/input \</span><br><span class="line">    -v <span class="variable">$AUDIO_OUT</span>:/output \</span><br><span class="line">    -v <span class="variable">$MODEL_DIRECTORY</span>:/model \</span><br><span class="line">    -e MODEL_PATH=/model \</span><br><span class="line">    researchdeezer/spleeter \</span><br><span class="line">    separate -i /input/audio_1.mp3 /input/audio_2.mp3 -o /output</span><br></pre></td></tr></table></figure></li><li><p>docker run所有的参数都应该写在镜像名字<code>xxx/xxx:xx</code>前面，写在其后面的统统会被视作在docker container中运行的命令或命令参数。</p></li><li><p>如果你有了一个在后台持续运行的container，且你想弄一个交互性bash，此时你仍需要加上<code>-it</code>参数，同样是在<code>docker exec</code>后，container名字前加需要的参数，restart和start同理。</p></li><li><p>如果你对作者的Docker Image不满意，需要修改，此时有两种方法：</p><ol><li>找到Dockerfile并修改，<code>docker build</code>，<code>docker push</code></li><li>使用bash进入一个实例化的Image，在里面做一通操作，出来后使用<code>docker commit -m &lt;YOUR_MESSAGE&gt; -a &lt;AUTHOR_NAME&gt; &lt;CONTAINER_ID&gt; &lt;DOCKERHUB_USERNAME/NEW_IMAGE_NAME:TAG&gt;</code>提交更改使其保存为一个新的镜像，最后使用<code>docker push</code>推送新的镜像到docker hub。如果命名有误或忘记添加docker hub username作为前缀，那么可以使用<code>docker tag &lt;existing-image&gt; &lt;hub-user&gt;/&lt;repo-name&gt;[:&lt;tag&gt;]</code>改名。</li></ol><p>注意，使用<code>docker push</code>需要有docker hub账号，并在push前使用<code>docker login</code>操作登录。</p></li><li><p>一个辨析：<code>docker commit</code>针对的是一个正在运行的container，使其固化为一个image；而<code>docker push</code>推送的则是一个image到docker hub。前者是本地操作，后者是上传操作。</p></li><li><p>一个区别：<code>docker start</code>是启动一个已经停止的container，而<code>docker restart</code>则是先stop一个container再start。如果一个container已经停止了，那么二者等效。</p></li></ul><h2 id="dockerfile"><a class="markdownIt-Anchor" href="#dockerfile"></a> Dockerfile</h2><ul><li>Docker Hub中并不直接提供Dockerfile，但可以通过查看image的“标签”页面看每个image的docker建立操作。但由于Docker build的时候使用git会很方便，所以很多作者会在其Github上发布这些Dockerfile，往往可以查看介绍页面找到链接。</li><li>Dockerfile中设置进入点命令：<code>ENTRYPOINT [&quot;spleeter&quot;]</code>。</li></ul><ol><li>这里”spleeter“是一个bin可执行文件。它的效果是：本来需要用户在<code>docker run</code>时输入<code>docker run xxx/xxx:xx spleeter separate</code>， 现在就只用输入<code>docker run xxx/xxx:xx separate</code>了，即run的时候帮你先打了一个命令标记但没给你按回车。</li><li>如果你发现自己在运行一个docker image时候提示了某个你没有输入的命令的相关问题，如<code>xxx don't have a parameter yyy, please input aaa, bbb, or ccc</code>，很有可能是Dockerfile中设定了进入点。</li><li>如果作者在Dockerfile中设定了进入点，但你需要进入docker进行调试或检查时，可以使用<code>docker run -it --entrypoint bash</code>来切换入点，进入一个bash命令行中调试。</li></ol><ul><li><code>docker build</code>针对的是一个url或是一个本地的文件夹。如果是本地的文件夹，文件夹内需要含有一个以<code>Dockerfile</code>为名的文件，如果需要导入某些文件到Docker Image中，则这些文件需要在正确的位置。<ol><li>如果dockerfile的名字不是<code>Dockerfile</code>，则使用<code>-f/--file &lt;DOCKERFILE_NAME&gt;</code>来指定名称。</li><li>如果需要指定输出image的名字和tag，则使用<code>-t/--tag</code>标签，以<code>name:tag</code>命名。</li><li>如果build的时候忘记了命名image，则输出的image没有名字和tag，只有一个随机序号。此时如果要重命名，可以用<code>docker tag &lt;SERIAL_NUMBER&gt; &lt;NAME:TAG&gt;</code> 命令。默认tag为latest。</li><li>Docker Build示例：<ul><li>本地文件夹：<code>docker build -f &lt;NAME:TAG&gt; &lt;TARGET DICTIONARY&gt;</code></li><li>本地文件：<code>docker build - &lt; &lt;Dockerfile_Path&gt;</code></li><li>URL：<code>docker build https://github.com/&lt;USERNAME&gt;/&lt;REPONAME&gt;.git#&lt;BRUNCH&gt;:&lt;SUBFOLDERNAME&gt;</code></li></ul></li></ol></li></ul><h2 id="nivdia-docker"><a class="markdownIt-Anchor" href="#nivdia-docker"></a> Nivdia Docker</h2><ul><li><p>先放<a href="https://github.com/NVIDIA/nvidia-docker">链接</a>。这里是Nvidia Docker的Github仓库。</p></li><li><p>再说作用。若想在Docker中运行GPU程序，则普通的Docker是做不到的，程序无法默认在Docker中使用GPU计算资源；另一方面，如果本地已经安装了某个版本的CUDA，但目标程序需要依赖另一个版本，这也是非常麻烦的。而Nvidia Docker的出现则很好解决了这个问题。它相当于在Docker的下面塞进了一层CUDA层，介于Container和OS之间。</p><p><img src="/2020/04/26/docker-et-nvidia/5b208976-b632-11e5-8406-38d379ec46aa.png" alt="Nvidia Docker 原理图"></p></li><li><p>然后是安装。</p><ol><li><p>作为前置条件，需要本机上安装有Nvidia Driver，不强制要求CUDA。（不过既然都安到Driver了，不如把本机CUDA也装了）官方教程<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation">链接</a>。当然，请安装Docker。</p></li><li><p>执行以下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add the package repositories</span></span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> $ID<span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>代码可能会随着Nvidia Docker的的升级而发生变化，最好参阅本章第一条的链接。</p></li></ol></li><li><p>试运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Test nvidia-smi with the latest official CUDA image</span></span><br><span class="line">docker run --gpus all nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start a GPU enabled container on two GPUs</span></span><br><span class="line">docker run --gpus 2 nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting a GPU enabled container on specific GPUs</span></span><br><span class="line">docker run --gpus <span class="string">&#x27;&quot;device=1,2&quot;&#x27;</span> nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line">docker run --gpus <span class="string">&#x27;&quot;device=UUID-ABCDEF,1&quot;&#x27;</span> nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specifying a capability (graphics, compute, ...) for my container</span></span><br><span class="line"><span class="comment"># Note this is rarely if ever used this way</span></span><br><span class="line">docker run --gpus all,capabilities=utility nvidia/cuda:10.0-base nvidia-smi</span><br></pre></td></tr></table></figure><p>其标志性特点就是一个参数<code>--gpus &lt;PARAMETERS&gt;</code> 一般使用<code>--gpus all</code>即可，其他部分和普通docker一模一样。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;docker-images-and-containers&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#docker-images-and-containers&quot;&gt;&lt;/a&gt; Docker Images and Containers&lt;/h</summary>
      
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="docker" scheme="https://www.miracleyoo.com/tags/docker/"/>
    
    <category term="nvidia" scheme="https://www.miracleyoo.com/tags/nvidia/"/>
    
    <category term="gpu" scheme="https://www.miracleyoo.com/tags/gpu/"/>
    
  </entry>
  
  <entry>
    <title>Linux(Ubuntu)装机与配置笔记</title>
    <link href="https://www.miracleyoo.com/2020/04/11/linux-setup/"/>
    <id>https://www.miracleyoo.com/2020/04/11/linux-setup/</id>
    <published>2020-04-11T22:11:47.000Z</published>
    <updated>2020-04-11T22:11:47.080Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘相关"><a class="markdownIt-Anchor" href="#硬盘相关"></a> 硬盘相关</h2><ol><li><p><strong>df命令</strong><br><code>df</code>：检查linux服务器的文件系统的磁盘空间占用情况。<strong>它只会显示已经挂载的磁盘信息！</strong></p><p><code>df -h</code>, 即<code>--human-readble</code>：以1024的倍数的方式显示大小。(e.g., 1023M)</p><p><code>df -T</code>：查看所有磁盘的文件系统类型(type)</p></li><li><p><strong><code>fdisk</code>命令</strong></p><p><code>fdisk</code>：强大的磁盘监视和操作工具。</p><p><code>fdisk -l</code>会显示<strong>所有的</strong>磁盘和分区！不论有没有挂载，都会被列出来。</p></li><li><p><strong>mount命令</strong></p><p><code>mount</code>：挂载一个文件系统</p><p><code>mount -t ntfs &lt;source&gt; &lt;target&gt;</code>：以ntfs文件系统的形式从源目录挂载到目标目录。t表示types类型</p><p><code>mount -a</code>：挂载 fstab 中的所有文件系统。a表示all</p></li><li><p><strong>blkid命令</strong></p><p><code>sudo blkid</code>：获取各个分区的UUID和分区类型TYPE</p></li><li><p>物理磁盘与磁盘分区：一个物理磁盘在<code>fdisk -l</code>中的显示往往类似于<code>/dev/sda</code>，<code>/dev/sdb</code>，<code>/dev/nvme0n1</code>。一般情况下是不带数字的，sda sdb是最常见的命名。而分区命名则是如：<code>/dev/sda1</code>，<code>/dev/sdb2</code>之类在物理磁盘的后面带上数字表示分区编号。</p><p>但有些如双系统中，可能会出现最后一个例子中展示的命名，这种磁盘的分区则是以<code>p[x]</code>结尾，如<code>/dev/nvme0n1p1</code>，<code>/dev/nvme0n1p9</code>。</p></li><li><p>Linux开机后不会自动挂载Windows文件格式NTFS的磁盘。</p></li><li><p><code>sudo chmod -R 777 &lt;Folder_Name&gt;</code> 可以取消一个文件夹的全部访问权限。</p></li><li><p><code>chmod</code>命令对ext3/4文件系统，即Linux格式的文件系统才有效，对其他文件系统，如vfat(Fat32)，NTFS都是无效的。</p></li><li><p>/etc/fstab` 文件是掌管硬盘自动挂载配置的文件，包含自动挂载分区过程的必要信息。每一条记录格式如下：</p><p><code>[Device] [Mount Point] [File System Type] [Options] [Dump] [Pass]</code></p><p>如：</p><p><code>UUID=B45A01D55A019570 /data ntfs defaults 0 2</code></p><p>其中：</p><p><code>[Options]</code> ：<code>defaults</code>表示用默认的<code>rw, suid, dev, exec, auto, nouser, async</code>等选项（不同内核和文件系统不同）进行挂载，这些选项的含义：<code>rw</code> 可读写；<code>suid</code> 执行程序时遵守<code>uuid</code>；<code>dev</code> 解释字符或禁止特殊设备；<code>exec</code> 允许执行二进制文件；<code>auto</code> 可以<code>-a</code>方式加载；<code>nouser</code> 禁止普通用户挂载此文件系统；<code>async</code> 所有I/O异步完成。</p><p><code>[Dump]</code> ：是否开启分区备份，0表示关闭</p><p><code>[Pass]</code>：系统启动时检查分区错误的顺序，root为1，其他为2，0为不检查。</p></li><li><p>在<code>fstab</code>文件中添加记录前一定要先尝试用mount命令手动挂载。</p></li></ol><h3 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h3><ol><li><a href="https://blog.csdn.net/qxqxqzzz/article/details/89790688">Ubuntu18.04 开机自动挂载其他硬盘</a></li><li><a href="https://blog.csdn.net/ybdesire/article/details/79145180">Linux查看与挂载新磁盘</a></li></ol><h2 id="cuda的安装"><a class="markdownIt-Anchor" href="#cuda的安装"></a> CUDA的安装</h2><ol><li><p>检查自己的GPU是否是CUDA-capable，在终端中输入<code>lspci | grep -I NVIDIA</code> ，会显示自己的NVIDIA GPU版本信息，去CUDA的官网查看自己的GPU版本是否在CUDA的支持列表中。</p></li><li><p>检查自己的Linux版本是否支持 CUDA（Ubuntu 稳定支持版没问题）。</p></li><li><p>检查其他问题。这里就不详述了，正常情况下一般OK，这里主要要检查是否安装了<code>gcc</code>，是否安装了<code>kernel header</code>和 <code>package development</code>。如果害怕出现问题可以参考<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">官网</a>执行这几步检测。</p></li><li><p>于<a href="https://developer.nvidia.com/cuda-downloads">CUDA官网</a>下载与系统对应的CUDA版本。最后一个选项选择<code>runfile</code>，因为其所需步骤最少，也因此最不容易出问题。所有选项完成后，你会看到如下两行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run</span><br><span class="line">sudo sh cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure><p>先不要执行第二条<code>sudo</code>开头的指令，只使用<code>wget</code>下载。</p></li><li><p>如果之前有安装过其他版本的CUDA并希望将其卸载，使用<code>sudo nvidia-uninstall</code>卸载。如果该命令不在系统路径中，则使用<code>sudo /usr/bin/nvidia-uninstall</code>（位置可能变化）卸载。如果还是没有，或是之前的驱动已经损坏，则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br><span class="line">sudo chmod +x NVIDIA-Linux-x86_64-410.93.run</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-410.93.run --uninstall</span><br></pre></td></tr></table></figure></li><li><p>屏蔽<code>nouveau</code>驱动。</p></li></ol><h3 id="nouveau是什么"><a class="markdownIt-Anchor" href="#nouveau是什么"></a> Nouveau是什么</h3><blockquote><h4 id="nouveau-accelerated-open-source-driver-for-nvidia-cards"><a class="markdownIt-Anchor" href="#nouveau-accelerated-open-source-driver-for-nvidia-cards"></a> Nouveau: Accelerated Open Source driver for nVidia cards</h4><p>The <strong>nouveau</strong> project aims to build high-quality, free/libre software drivers for <a href="https://nouveau.freedesktop.org/wiki/CodeNames/">nVidia cards</a>. “Nouveau” [<em>nuvo</em>] is the French word for “new”. Nouveau is composed of a Linux kernel KMS driver (nouveau), Gallium3D drivers in Mesa, and the Xorg DDX (xf86-video-nouveau). The kernel components have also been ported to <a href="https://nouveau.freedesktop.org/wiki/NetBSD/">NetBSD</a>.</p></blockquote><p>简单说，nouveau是Linux系统默认的给NVIDIA卡预装的一个图形加速驱动，而这个驱动会与CUDA产生部分冲突，所以在安装CUDA之前需要将其禁用，否则会出现卡在开机登录界面无法进入图形界面（仍然可以ssh访问），黑屏，鼠标键盘输入被禁用等问题中的一个或多个（亲身经历）。</p><p>继续安装教程：</p><ol start="6"><li><p>刚才说到要屏蔽<code>nouveau</code>，那么怎么知道你有没有装它呢？<br>使用<code>lsmod | grep nouveau</code>命令，如果没有输出，就可以判定你没有运行<code>nouveau</code>，可以直接进入下一步，否则：</p><ol><li><p>Create a file at <code>/etc/modprobe.d/blacklist-nouveau.conf</code> with the following contents:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure></li><li><p>Regenerate the kernel initramfs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure></li><li><p>Restart.</p></li><li><p>Run <code>lsmod | grep nouveau</code> again. If there is no output, then you succeed.</p></li></ol></li><li><p>此后建议进入一个非图形界面安装，这里可以在重启后使用<code>ssh</code>接入，也可以在重启后按<code>alt+ctrl+f1</code>，进入<strong>text mode</strong>，登录账户。</p></li><li><p>输入 <code>sudo service lightdm stop</code> 关闭图形化界面。</p></li><li><p>执行刚才官网中给出的第二条命令：<code>sudo sh cuda_10.2.89_440.33.01_linux.run</code>。注意这里的版本会不断有变化。注意这里有一个点，即你是否要同时安装OpenGL，如果你是双显，且主显是非NVIDIA的GPU需要选择no，否则yes。同理，如果准备选no，也可以一开始就加上参数<code>--no-opengl-files</code>。 另外，如果不能直接执行，使用<code>sudo chmod a+x cuda_xx.xx.xx_linux.run</code>为其赋权。</p></li><li><p>安装成功后，会提示你将cuda的几个路径添加到系统路径中，这里重复一下，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-10.2/bin:/usr/<span class="built_in">local</span>/cuda-10.2/NsightCompute-2019.1<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-10.2/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>使用<code>nvcc -V</code>检测是否安装成功。当然也可以同时测试<code>nvidia-smi</code>。这里可能会报错并提示需要apt安装一个包，按提示来。</p></li><li><p>完成。</p></li></ol><h3 id="参考-2"><a class="markdownIt-Anchor" href="#参考-2"></a> 参考</h3><ol><li><a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA下载官网</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">NVIDIA 官方安装指南（英文）</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">NVIDIA 官方安装指南中前置检查部分</a></li><li><a href="https://www.pugetsystems.com/labs/hpc/How-To-Install-CUDA-10-together-with-9-2-on-Ubuntu-18-04-with-support-for-NVIDIA-20XX-Turing-GPUs-1236/">How To Install CUDA 10 (together with 9.2) on Ubuntu 18.04 with support for NVIDIA 20XX Turing GPUs</a></li><li><a href="https://blog.csdn.net/lipi37/article/details/90407099">Ubuntu 安装 cuda 时卡在登录界面（login loop)的解决方案之一</a></li><li><a href="https://blog.csdn.net/wkk15903468980/article/details/56489704">ubuntu安装cuda循环登录</a></li><li><a href="https://blog.csdn.net/qq_33200967/article/details/80689543">Ubuntu安装和卸载CUDA和CUDNN</a></li><li><a href="https://blog.csdn.net/wf19930209/article/details/81879514">Linux安装CUDA的正确姿势</a></li></ol><h2 id="cuda-与-cudnn-的联系"><a class="markdownIt-Anchor" href="#cuda-与-cudnn-的联系"></a> CUDA 与 CUDNN 的联系</h2><ol><li>要先装CUDA再装CUDNN。</li><li>前者是平台，后者是基于平台的深度学习加速器。加速可以应用于几乎全部深度学习平台。还是要安的。</li><li>一般深度学习使用安装runtime版本即可。</li><li><a href="https://developer.nvidia.com/rdp/cudnn-download">CUDNN官方下载</a>，<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html">CUDNN官方安装步骤</a></li></ol><h2 id="修复ubuntu中检测到系统程序错误的问题"><a class="markdownIt-Anchor" href="#修复ubuntu中检测到系统程序错误的问题"></a> 修复Ubuntu中“检测到系统程序错误”的问题</h2><h3 id="问题描述"><a class="markdownIt-Anchor" href="#问题描述"></a> 问题描述</h3><p>每次开机时都会有“<strong>Ubuntu xx.xx 在启动时检测到系统程序错误</strong> ”弹窗出现。即使点击报告下次还会继续出现。</p><h3 id="问题来源"><a class="markdownIt-Anchor" href="#问题来源"></a> 问题来源</h3><p>之前的某个时刻某个程序崩溃了，而Ubuntu想让你决定要不要把这个问题报告给开发者，这样他们就能够修复这个问题。</p><h3 id="解决办法"><a class="markdownIt-Anchor" href="#解决办法"></a> 解决办法</h3><ol><li><code>sudo rm /var/crash/*</code> ：删除这些错误报告。但是如果又有一个程序崩溃了，你就会再次看到“检测到系统程序错误”的错误。你可以再次删除这些报告文件，或者选择禁用Apport来彻底地摆脱这个错误弹窗。如果你这样做，系统中任何程序崩溃时，系统都不会再通知你。但这未必一件坏事，除非你愿意填写错误报告。如果你不想填写错误报告，那么这些错误通知存不存在都不会有什么区别。</li><li><code>sudo vim /etc/default/apport</code> 永久屏蔽这些报错。</li></ol><h3 id="参考-3"><a class="markdownIt-Anchor" href="#参考-3"></a> 参考</h3><ol><li><a href="https://blog.csdn.net/hywerr/article/details/72582082">如何修复ubuntu中检测到系统程序错误的问题</a></li><li><a href="https://itsfoss.com/how-to-fix-system-program-problem-detected-ubuntu/">How To Fix System Program Problem Detected In Ubuntu</a></li></ol><h2 id="安装python36版本的anaconda"><a class="markdownIt-Anchor" href="#安装python36版本的anaconda"></a> 安装Python3.6版本的Anaconda</h2><p>由于之前使用的一些开源库和软件对3.7的支持性尚还有问题，而Anaconda默认Python版本为3.6， 所以有必要把Anaconda降级为3.6版本。</p><p>安装方法：</p><ol><li><p>到Anaconda官网下载并安装最新3.7版本。</p></li><li><p>世界线开始分歧，你可以选择保留3.7版本的Anaconda，并创建一个虚拟环境，或是直接替换Python版本。</p><ol><li><p>对前者，<br>若只要一个python环境不要packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name ana36 python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure><p>反之，如果要安装一个新的Anaconda，包含默认的所有packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n ana36 anaconda python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure></li><li><p>对后者，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python=3.6</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="添加vim拷贝至系统剪贴板快捷键支持"><a class="markdownIt-Anchor" href="#添加vim拷贝至系统剪贴板快捷键支持"></a> 添加Vim拷贝至系统剪贴板快捷键支持</h2><p>(from: <a href="http://vim.wikia.com/wiki/Mac_OS_X_clipboard_sharing">link</a>)</p><p>Having trouble copying selected text from Vim (not MacVim)? Since using <code>&quot;+y</code> or ‘&quot;*y’ in Vim on a Mac doesn’t actually copy the selected text to the system clipboard, you might find it beneficial to do the following:</p><ol><li>Open your <code>~/.vimrc</code> file</li><li>add <code>vmap '' :w !pbcopy</code></li><li>Save it and <code>source</code> the file</li></ol><p>现在，你就可以在 visual mode， 即在Esc命令模式后按下v键后的选择模式中，选好需要拷贝区域后，连击两次<code>'</code> ，即使用 <code>''</code>来拷贝所选区域。</p><h2 id="在maclinux上使用ssh挂载远程网络硬盘"><a class="markdownIt-Anchor" href="#在maclinux上使用ssh挂载远程网络硬盘"></a> 在Mac/Linux上使用ssh挂载远程网络硬盘</h2><p>TL;DR：</p><ol><li>安装sshfs: <code>sudo apt-get install sshfs</code></li><li>直接在<code>~/.zshrc</code>中添加以下行：（当然，需要更改文件夹名称，以及挂载后的命名）</li></ol><h3 id="连接本地linux-server"><a class="markdownIt-Anchor" href="#连接本地linux-server"></a> 连接本地Linux Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_misaka</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-home&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/misaka-home</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/home/miracle /Volumes/misaka-home/ -ovolname=mk-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-storage&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/misaka-storage</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/data /Volumes/misaka-storage/ -ovolname=mk-2T</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="连接gypsum"><a class="markdownIt-Anchor" href="#连接gypsum"></a> 连接Gypsum</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_gypsum</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/home/zhongyangzha /Volumes/gypsum/ -ovolname=gp-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-scratch/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum-scratch/</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/scratch1/zhongyangzha/ /Volumes/gypsum-scratch/ -ovolname=gp-scratch</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-work/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum-work</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/work1/trahman/zhongyangzha /Volumes/gypsum-work/ -ovolname=gp-work</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数解释"><a class="markdownIt-Anchor" href="#参数解释"></a> 参数解释</h3><ol><li><code>ovolname</code>：挂载上网络硬盘之后硬盘的命名</li><li><code>IdentityFile</code>：如果已经设置了免密登录，用这个参数指明ssh私钥位置即可，不需要输入密码。</li><li><code>&lt;source&gt; &lt;target&gt;</code>：网络硬盘源位置&lt;username@ip.address:/the/source/path&gt; 与本机目标挂载位置</li><li><code>reconnect,ServerAliveInterval=15,ServerAliveCountMax=3</code>：多次断线重连，可以再断开网络连接、服务器重启等问题发生后再次自动连接。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;硬盘相关&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#硬盘相关&quot;&gt;&lt;/a&gt; 硬盘相关&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;df命令&lt;/strong&gt;&lt;br&gt;
&lt;code&gt;df&lt;/code&gt;：检查linux服务器的文件系</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="ssh" scheme="https://www.miracleyoo.com/tags/ssh/"/>
    
    <category term="net-disk" scheme="https://www.miracleyoo.com/tags/net-disk/"/>
    
    <category term="cuda" scheme="https://www.miracleyoo.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>Pandas Resample</title>
    <link href="https://www.miracleyoo.com/2020/03/24/pandas-resample/"/>
    <id>https://www.miracleyoo.com/2020/03/24/pandas-resample/</id>
    <published>2020-03-25T01:26:56.000Z</published>
    <updated>2020-04-26T22:27:22.090Z</updated>
    
    <content type="html"><![CDATA[<p>Pandas原生支持<code>resample</code>功能，前提是目标DataFrame需要有一个index的column。假设我们现在在对一个取样率为30Hz的DataFrame做操作，并想将它变resample为16Hz。</p><p>首先我们要建立一个<code>timestamp</code>的列，这个名字随意，然后它是以秒为单位的该帧的时间，如3.25，14.33。然后我们将其转换为datatime格式，单位为s。</p><p>之后便是直接resample，resample中的<code>rule</code>，即第一个参数，指明了resample后两帧之间的时间间隔，即周期。如果我们是16Hz，那这个周期为62.5ms。</p><p><code>resample</code>方法的格式是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.resample(rule, how=<span class="literal">None</span>, axis=<span class="number">0</span>, fill_method=<span class="literal">None</span>, closed=<span class="literal">None</span>, label=<span class="literal">None</span>, convention=<span class="string">&#x27;start&#x27;</span>,kind=<span class="literal">None</span>, loffset=<span class="literal">None</span>, limit=<span class="literal">None</span>, base=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="示例代码"><a class="markdownIt-Anchor" href="#示例代码"></a> 示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.index=pd.to_datetime(df[<span class="string">&#x27;timestamp&#x27;</span>],unit=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">df=df.resample(<span class="string">&#x27;62.5L&#x27;</span>).mean()</span><br><span class="line">df=df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">&#x27;timestamp&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="pandas时间缩写"><a class="markdownIt-Anchor" href="#pandas时间缩写"></a> Pandas时间缩写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">B         business day frequency</span><br><span class="line">C         custom business day frequency (experimental)</span><br><span class="line">D         calendar day frequency</span><br><span class="line">W         weekly frequency</span><br><span class="line">M         month end frequency</span><br><span class="line">SM        semi-month end frequency (15th and end of month)</span><br><span class="line">BM        business month end frequency</span><br><span class="line">CBM       custom business month end frequency</span><br><span class="line">MS        month start frequency</span><br><span class="line">SMS       semi-month start frequency (1st and 15th)</span><br><span class="line">BMS       business month start frequency</span><br><span class="line">CBMS      custom business month start frequency</span><br><span class="line">Q         quarter end frequency</span><br><span class="line">BQ        business quarter endfrequency</span><br><span class="line">QS        quarter start frequency</span><br><span class="line">BQS       business quarter start frequency</span><br><span class="line">A         year end frequency</span><br><span class="line">BA, BY    business year end frequency</span><br><span class="line">AS, YS    year start frequency</span><br><span class="line">BAS, BYS  business year start frequency</span><br><span class="line">BH        business hour frequency</span><br><span class="line">H         hourly frequency</span><br><span class="line">T, min    minutely frequency</span><br><span class="line">S         secondly frequency</span><br><span class="line">L, ms     milliseconds</span><br><span class="line">U, us     microseconds</span><br><span class="line">N         nanoseconds</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Pandas原生支持&lt;code&gt;resample&lt;/code&gt;功能，前提是目标DataFrame需要有一个index的column。假设我们现在在对一个取样率为30Hz的DataFrame做操作，并想将它变resample为16Hz。&lt;/p&gt;
&lt;p&gt;首先我们要建立一个&lt;co</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>EMD, EEMD与CEEMD</title>
    <link href="https://www.miracleyoo.com/2020/03/12/eemd/"/>
    <id>https://www.miracleyoo.com/2020/03/12/eemd/</id>
    <published>2020-03-12T18:24:50.000Z</published>
    <updated>2020-04-26T22:25:10.220Z</updated>
    
    <content type="html"><![CDATA[<h2 id="emd"><a class="markdownIt-Anchor" href="#emd"></a> EMD</h2><p>EMD: Empirical Mode Decomposition</p><h3 id="特征"><a class="markdownIt-Anchor" href="#特征"></a> 特征</h3><ol><li>自适应。与小波分析相比，克服了基函数无自适应性的问题，解决了全局最优小波基在局部并非最优的问题，有基函数自适应特性。</li><li>可以直接进行分解，不需要预分析和研究。</li></ol><h3 id="内涵模态分量"><a class="markdownIt-Anchor" href="#内涵模态分量"></a> 内涵模态分量</h3><p>内涵模态分量（Intrinsic Mode Functions, IMF）就是原始信号被EMD分解之后得到的各层信号分量。EMD的提出人黄锷认为，任何信号都可以拆分成若干个内涵模态分量之和。而内涵模态分量有两个约束条件：</p><p>1）在整个数据段内，极值点的个数和过零点的个数必须相等或相差最多不能超过一个。</p><p>2）在任意时刻，由局部极大值点形成的上包络线和由局部极小值点形成的下包络线的平均值为零，即上、下包络线相对于时间轴局部对称。</p><p>啥意思？</p><p>用不严谨的语言和灵魂画师来解释一下：</p><p>1）图线要反复跨越x轴，像这样：</p><p><img src="/2020/03/12/eemd/v2-0e5b832aee81e8a9068c9665e6eb2a3a_1440w.jpg" alt="img"></p><p>在整个数据段内，极值点的个数和过零点的个数必须相等或相差最多不能超过一个</p><p>而不能像这样某次穿过零点后出现多个极点：</p><p><img src="/2020/03/12/eemd/v2-921bc09334db7a4e443578091117788f_1440w.jpg" alt="极点数目偏多"></p><p>2）包络线要对称，像这样：</p><p><img src="/2020/03/12/eemd/v2-8826ddaefd1cebee1841bf5ff083c494_1440w.jpg" alt="包络线对称"></p><p>而不能像这样：</p><p><img src="/2020/03/12/eemd/v2-deb9cd0d0dcb8a154f8621276cce9972_1440w.jpg" alt="包络线不对称"></p><p>洗洗眼睛，看个正常点的例子吧：</p><p><img src="/2020/03/12/eemd/v2-a609c2680a2f4c525648a414d9b0a358_1440w.jpg" alt="EMD分解"></p><p>上图由7张图片组成，其中第1张为原始信号，后边依次为EMD分解之后得到的6个分量，分别叫做IMF1~IMF5，最后一张图为残差，每一个IMF分量代表了原始信号中存在的一种内涵模态分量。可以看出，每个IMF分量都是满足这两个约束条件的。</p><h3 id="分解步骤"><a class="markdownIt-Anchor" href="#分解步骤"></a> 分解步骤</h3><p>1）根据原始信号上下极值点，分别画出上、下包络线。</p><p><img src="/2020/03/12/eemd/v2-c18c7b4e6d60711351a4d55cb8271320_1440w.jpg" alt="img">上、下包络线</p><p>2）求上、下包络线的均值，画出均值包络线。</p><p><img src="/2020/03/12/eemd/v2-d56c460e9dd9e245521140497afddb39_1440w.jpg" alt="img">均值包络线</p><p>3）原始信号减均值包络线，得到中间信号。</p><p><img src="/2020/03/12/eemd/v2-e74e49a23dda87df74a562809257ddda_1440w.jpg" alt="img">原始信号减均值包络线</p><p>4）判断该中间信号是否满足IMF的两个条件，如果满足，该信号就是一个IMF分量；如果不是，以该信号为基础，重新做1）~4）的分析。IMF分量的获取通常需要若干次的迭代。</p><p><img src="/2020/03/12/eemd/v2-8b6643d803c3bdfb47639e65a75d4c8d_1440w.jpg" alt="img">不满足约束2，需要继续迭代</p><p>使用上述方法得到第一个IMF后，用原始信号减IMF1，作为新的原始信号，再通过1）~4）的分析，可以得到IMF2，以此类推，完成EMD分解。</p><p><img src="/2020/03/12/eemd/v2-f735266df804d187b1d173fe6f1bb168_1440w.jpg" alt="img">迭代分解结果</p><p>上述例子中的图来自<a href="http://perso.ens-lyon.fr/patrick.flandrin/emd.ppt">http://perso.ens-lyon.fr/patri</a></p><h2 id="eemd"><a class="markdownIt-Anchor" href="#eemd"></a> EEMD</h2><p>EEMD: Ensemble Empirical Mode Decomposition</p><p>简单的说，EEMD是在EMD的基础上，对原始信号进行了N次添加各异等幅白噪声并分别进行EMD分解后，对每个IMF中间分量进行平均。</p><p>其原理是通过加入白噪声来改变信号极值点的分布，得到符合信号特征的上下包络线，消除模态混叠效应。加入的白噪声通过多次平均消除。</p><h2 id="ceemd"><a class="markdownIt-Anchor" href="#ceemd"></a> CEEMD</h2><p>CEEMD是在EEMD的基础上，把随机添加的N组白噪声改为了N/2组正噪声和N/2组负噪声，依旧是最后进行平均。</p><p>根据 Yeh 等人的研究，在加入相同数量以及相同幅值的白噪声时，EEMD 剩余噪声会随着集成平均的次数而逐渐减小。CEEMD 的剩余噪声一直维持在一个较小的程度，不论集成平均次数多少。在一定程度上使用 CEEMD方法进行信号分解，可以使用相对较少的集成平均次数，从某种意义上来说，CEEMD在保证小剩余噪声干扰的情况下，能够节省计算时间。</p><p><img src="/2020/03/12/eemd/931855-20190117162939850-50932674.png" alt="img"><img src="/2020/03/12/eemd/931855-20190117163017276-1187230461.png" alt="img"></p><h2 id="python库"><a class="markdownIt-Anchor" href="#python库"></a> Python库</h2><p>EMD, EEMD, CEEMDAN and some visualization support are contained in this repository.</p><p>We can use <code>pip install EMD-signal</code> to install this library.</p><h2 id="引用"><a class="markdownIt-Anchor" href="#引用"></a> 引用</h2><ol><li><a href="https://www.cnblogs.com/Dinging006/p/10282993.html">EMD——EEMD——CEEMD语音增强算法基础</a></li><li><a href="https://zhuanlan.zhihu.com/p/40005057">这篇文章能让你明白经验模态分解（EMD）——基础理论篇</a></li><li><a href="https://zhuanlan.zhihu.com/p/44833026">这篇文章能让你明白经验模态分解（EMD）——IMF的物理含义</a></li><li><a href="https://www.researchgate.net/publication/220531146_Ensemble_Empirical_Mode_Decomposition_a_Noise-Assisted_Data_Analysis_Method">Ensemble Empirical Mode Decomposition: a Noise-Assisted Data Analysis Method</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;emd&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#emd&quot;&gt;&lt;/a&gt; EMD&lt;/h2&gt;
&lt;p&gt;EMD: Empirical Mode Decomposition&lt;/p&gt;
&lt;h3 id=&quot;特征&quot;&gt;&lt;a class=&quot;markdownI</summary>
      
    
    
    
    
    <category term="signal-processing" scheme="https://www.miracleyoo.com/tags/signal-processing/"/>
    
    <category term="audio" scheme="https://www.miracleyoo.com/tags/audio/"/>
    
  </entry>
  
  <entry>
    <title>Face Recognition, Landmark and Relevant Other Feature Extraction</title>
    <link href="https://www.miracleyoo.com/2020/03/06/face-features/"/>
    <id>https://www.miracleyoo.com/2020/03/06/face-features/</id>
    <published>2020-03-07T01:42:43.000Z</published>
    <updated>2020-03-07T01:42:43.040Z</updated>
    
    <content type="html"><![CDATA[<h2 id="some-facts"><a class="markdownIt-Anchor" href="#some-facts"></a> Some Facts</h2><ol><li>The output of face detector is not always the same, it can be a square, a rectangle, or an oval bounding box.</li><li>Most of the landmark detectors need to take in an square bounding box for the detection.</li><li>Although the bounding box shape is different, they roughly have the same shape center. For the square and rectangle, they have the same bounding box center, and the edge length of the square box is roughly the same as the mean value of the two edge lengths of the rectangle. Here is a sample.</li></ol><p><img src="/2020/03/06/face-features/A48432AC-5A9B-48D6-A502-0777810A592A.png" alt="img"></p><blockquote><p>White: Dlib Result</p><p>Green: RetinaFace Result</p><p>Red: RetinaFace Transferred Result (Same center, length=(width+height)/2)</p></blockquote><ol start="4"><li>Many face detection and alignment models has an absolute detectable face size range in pixel. If the input image contains a face too big, some network cannot generate correct face bounding box or landmark, like RetinaFace, SFD, Hrnet. On the other hand, if the input image contains faces too small(also in absolute pixel), other network like MTCNN and other traditional method will fail.</li></ol><h2 id="libraries-and-papers"><a class="markdownIt-Anchor" href="#libraries-and-papers"></a> Libraries and Papers</h2><h3 id="face-detection"><a class="markdownIt-Anchor" href="#face-detection"></a> Face Detection</h3><h4 id="datasets"><a class="markdownIt-Anchor" href="#datasets"></a> Datasets</h4><table><thead><tr><th>Name</th><th>Site</th><th>Description</th></tr></thead><tbody><tr><td>Wider Face</td><td><a href="http://shuoyang1213.me/WIDERFACE/">Link</a></td><td><strong>32,203</strong> images, <strong>393,703</strong> faces labeled with a high degree of variability in scale, pose and occlusion</td></tr><tr><td>FFDB</td><td><a href="http://vis-www.cs.umass.edu/fddb/">Link</a></td><td><strong>5171</strong> faces, in which <strong>2845</strong> images from the <a href="http://tamaraberg.com/faceDataset/index.html">Faces in the Wild</a> data set</td></tr><tr><td>AFLW</td><td><a href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/">Link</a></td><td><strong>25k faces</strong> are annotated with up to <strong>21 landmarks</strong> per image</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h4 id="researches"><a class="markdownIt-Anchor" href="#researches"></a> Researches</h4><table><thead><tr><th>Name</th><th>Paper</th><th>Code</th><th>Year</th><th>Accuracy</th><th>Pre-trained Model</th></tr></thead><tbody><tr><td><a href="https://www.paperswithcode.com/paper/190500641">RetinaFace</a></td><td><a href="https://arxiv.org/pdf/1905.00641v2.pdf">Link</a></td><td><a href="https://github.com/deepinsight/insightface"> Original</a><br><a href="https://github.com/biubug6/Pytorch_Retinaface"> Pytorch</a></td><td>2019</td><td>Wider Face (Hard): 0.914</td><td>104M(Resnet)<br><a href="https://drive.google.com/drive/folders/1oZRSG0ZegbVkVwUd8wUIQx8W7yfZ_ki1">2M(MobileNet0.25)</a></td></tr><tr><td><a href="https://www.paperswithcode.com/paper/dsfd-dual-shot-face-detector">DFSD</a></td><td><a href="https://arxiv.org/pdf/1810.10220v3.pdf">Link</a></td><td><a href="https://github.com/TencentYoutuResearch/FaceDetection-DSFD">Link</a></td><td>2018</td><td>FDDB: 0.991<br>Wider Face: 0.960, 0.953, 0.900</td><td><a href="https://drive.google.com/file/d/1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-/view">459MB</a></td></tr><tr><td><a href="SFD/S3FD">SFD/S3FD</a></td><td><a href="https://arxiv.org/pdf/1708.05237.pdf">Link</a></td><td><a href="https://github.com/sfzhang15/SFD">Original</a><br><a href="https://github.com/yxlijun/S3FD.pytorch">Pytorch</a></td><td>2017</td><td>FDDB: 0.983<br>Wider Face: 0.928, 0.913, 0.840</td><td><a href="https://pan.baidu.com/s/1epyTAUc6qSt3oZ7veK4oEw">85.7M</a></td></tr><tr><td><a href="https://www.paperswithcode.com/paper/joint-face-detection-and-alignment-using">MTCNN</a></td><td><a href="https://arxiv.org/abs/1604.02878">Link</a></td><td><a href="https://github.com/kuaikuaikim/DFace">Original</a><br><a href="https://github.com/ipazc/mtcnn">Pip Version</a></td><td>2016</td><td>Wider Face: 0.851,  0.820, 0.607</td><td><a href="https://github.com/ipazc/mtcnn/blob/master/mtcnn/data/mtcnn_weights.npy">2.85M</a></td></tr></tbody></table><h5 id="retinaface-pros-cons"><a class="markdownIt-Anchor" href="#retinaface-pros-cons"></a> RetinaFace Pros &amp; Cons:</h5><p>RetinaFace can generate an accurate rectangle face bounding box together with a 5-points facial landmark. It supports two backbone kernels: Resnet and MobileNet. The first one is more accurate but relatively slow, the MobileNet version is fast and really small.</p><p>RetinaFace focus more on the detection of the relatively small faces, and it can do a good(best) job on wider face dataset hard level. However, when the input is an image contain a really large face(About ${short_face_edge}&gt;700\space pixel $ ), RetinaFace tend to fail. Since there is also other people asking the same question on the issue of its GitHub page, I tend to think this is a design defects of RetinaFace.</p><p>Since the game streamers’ face videos can be relatively large, I think this may become a fatal drawback. There are three possible solutions:</p><ol><li>Scale the input image to make sure the largest face short edge is smaller than 700, recommend around 500.</li><li>Wait for the author to change or change the pyramid weights parameters. This is delicate and success is not guaranteed.</li></ol><img src="/2020/03/06/face-features/image-20200306140736438.png" alt="image-20200306140736438" style="zoom:50%;"><h5 id="other-models"><a class="markdownIt-Anchor" href="#other-models"></a> Other Models</h5><p>DFSD can behave well on both easy, medium and hard level of Wider Face dataset. The only drawback is that it is much too large and slow. Not to mention real-time, it is even too heavy for GPU prediction when the input is a video and we also need to predict other features.</p><p>SFD has the similar problem as RetinaFace. It will also fail at big face cases.</p><p>MTCNN is just really small and easy to use. It is wrapped finely into a pip package and we can use one line to do face detection here. It behaves much worse in small faces, but better when the input face is big compared to other method. In fact, MTCNN might be a good choice for our project, since it is friendly to big face and fast enough.</p><h4 id="wrapped-libraries"><a class="markdownIt-Anchor" href="#wrapped-libraries"></a> Wrapped Libraries</h4><table><thead><tr><th>Name</th><th>Site</th><th>Year</th><th>Language</th><th>Pip (Name)</th></tr></thead><tbody><tr><td>Dlib</td><td><a href="http://dlib.net/">Link</a></td><td>2006-&gt;now</td><td>C++ &amp; Python</td><td>√ dlib</td></tr><tr><td>OpenFace V1</td><td><a href="http://cmusatyalab.github.io/openface/">Home Page</a><br><a href="https://github.com/cmusatyalab/openface">GitHub</a></td><td>2016</td><td>Python &amp; Lua</td><td>× conda</td></tr><tr><td>OpenFace V2</td><td><a href="https://github.com/TadasBaltrusaitis/OpenFace">Link</a></td><td>2018</td><td>C++</td><td>× compile locally</td></tr><tr><td>facenet-pytorch</td><td><a href="https://pypi.org/project/facenet-pytorch/">Link</a></td><td>2017</td><td>Python(PT)</td><td>√ facenet-pytorch</td></tr><tr><td>MTCNN</td><td><a href="https://pypi.org/project/mtcnn/">Link</a></td><td>2016</td><td>Python(TF)</td><td>√ mtcnn</td></tr></tbody></table><h5 id="comment"><a class="markdownIt-Anchor" href="#comment"></a> Comment</h5><ol><li>OpenFace V1 uses face detection model from dlib and OpenCV.</li><li>OpenFace V2 also used MTCNN as core face detector.</li></ol><h3 id="face-landmark"><a class="markdownIt-Anchor" href="#face-landmark"></a> Face Landmark</h3><h4 id="datasets-2"><a class="markdownIt-Anchor" href="#datasets-2"></a> Datasets</h4><table><thead><tr><th>Name</th><th>Site</th><th>Description</th></tr></thead><tbody><tr><td>300-W</td><td><a href="https://ibug.doc.ic.ac.uk/resources/300-W/">Link</a></td><td>68 points. Bounding boxes calculated by the boundary of all <strong>68</strong> points.</td></tr><tr><td>WFLW</td><td><a href="https://wywu.github.io/projects/LAB/WFLW.html">Link</a></td><td>Wider Facial Landmarks in-the-wild (WFLW) contains <strong>10000</strong> faces (7500 for training and 2500 for testing) with <strong>98</strong> fully manual annotated landmarks. Rich attribute annotations included, i.e., occlusion, pose, make-up, illumination, blur and expression.</td></tr><tr><td>COFW</td><td><a href="http://www.vision.caltech.edu/xpburgos/ICCV13/">Link</a></td><td>All images were hand annotated using the same <strong>29</strong> landmarks as in LFPW. Both the landmark positions and their occluded/unoccluded state are annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over <strong>23%</strong>.</td></tr></tbody></table><p>To increase the number of training images, and since<br>COFW has the exact same landmarks as LFPW, for training<br>we use the original non-augmented 845 LFPW faces + 500 COFW faces (<strong>1345</strong> total), and for testing the remaining <strong>507</strong> COFW faces. |<br>| AFLW  | <a href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/">Link</a> | Annotated Facial Landmarks in the Wild (AFLW) provides a large-scale collection of annotated face images gathered from the web, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about <strong>25k faces</strong> are annotated with up to <strong>21 landmarks</strong> per image. |</p><h5 id="attention"><a class="markdownIt-Anchor" href="#attention"></a> Attention</h5><ol><li>The bounding box of 300-W dataset is not human labeled. It is the smallest rectangle which can accurately include every 68 points.</li></ol><img src="/2020/03/06/face-features/figure_4_n_2.png" alt="img" style="zoom:15%;"><h4 id="researches-2"><a class="markdownIt-Anchor" href="#researches-2"></a> Researches</h4><table><thead><tr><th>Name</th><th>Paper</th><th>Code</th><th>Year</th><th>Accuracy</th></tr></thead><tbody><tr><td><a href="https://www.paperswithcode.com/paper/adaptive-wing-loss-for-robust-face-alignment">AWing</a></td><td><a href="https://arxiv.org/pdf/1904.07399v2.pdf">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</a></td><td><a href="https://github.com/protossw512/AdaptiveWingLoss">Link</a></td><td>2019</td><td>300-W: 3.07</td></tr><tr><td><a href="https://www.paperswithcode.com/paper/look-at-boundary-a-boundary-aware-face">LAB</a></td><td><a href="https://wywu.github.io/projects/LAB/LAB.html">Look at Boundary</a></td><td><a href="https://github.com/wywu/LAB">Link</a></td><td>2018</td><td>300-W: 3.49</td></tr><tr><td><a href="https://www.paperswithcode.com/paper/style-aggregated-network-for-facial-landmark">SAN</a></td><td><a href="https://arxiv.org/pdf/1803.04108v4.pdf">Style Aggregated Network</a></td><td><a href="https://github.com/D-X-Y/landmark-detection">Link</a></td><td>2018</td><td>300W NME: 3.98</td></tr><tr><td>HRNet</td><td><a href="https://arxiv.org/abs/1908.07919">Deep High-Resolution Representation Learning</a></td><td><a href="https://github.com/HRNet/HRNet-Facial-Landmark-Detection">Link</a></td><td>2019</td><td>300-W: 3.32</td></tr><tr><td>FAN</td><td><a href="https://arxiv.org/abs/1703.07332">Face Alignment Network</a></td><td><a href="https://github.com/1adrianb/face-alignment">Link</a></td><td>2017</td><td>300-W: Acc(7% threshold)66.9%</td></tr><tr><td><a href="https://www.paperswithcode.com/paper/deep-alignment-network-a-convolutional-neural">DAN-Menpo</a></td><td><a href="https://arxiv.org/pdf/1706.01789v2.pdf">Deep Alignment Network</a></td><td><a href="https://github.com/MarekKowalski/DeepAlignmentNetwork">Link</a></td><td>2017</td><td>300-W: 3.44</td></tr></tbody></table><p>The accuracy on 300-W is based on <strong>FULLSET (PUBLIC)</strong>.</p><h4 id="overview-on-researches"><a class="markdownIt-Anchor" href="#overview-on-researches"></a> Overview on researches</h4><ol><li>FAN is the final method I choose now. It cannot only generate 2D but also accurate 3D landmark, which is quite important for us considering the head pose and eye gaze can also be deduced from here.</li><li>Although HRNet seems to be good, it will crash completely when the input face is large(About <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>f</mi><mi>a</mi><mi>c</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>e</mi><mi>d</mi><mi>g</mi><mi>e</mi></mrow><mo>&gt;</mo><mn>250</mn></mrow><annotation encoding="application/x-tex">{face\_min\_edge} &gt; 250</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">a</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">e</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">0</span></span></span></span>). Not really recommended if no extra operation added. But this can also be fixed by resize before sending to our pipeline, since our target is a single big-face player. When the input face size is limited, the result is quite decent.</li></ol><h4 id="wrapped-libraries-2"><a class="markdownIt-Anchor" href="#wrapped-libraries-2"></a> Wrapped Libraries</h4><table><thead><tr><th>Name</th><th>Site</th><th>Year</th><th>Language</th><th>Pip (Name)</th></tr></thead><tbody><tr><td>Dlib</td><td><a href="http://dlib.net/">Link</a></td><td>2006-&gt;now</td><td>C++ &amp; Python</td><td>√ dlib</td></tr><tr><td>OpenFace V1</td><td><a href="http://cmusatyalab.github.io/openface/">Home Page</a><br><a href="https://github.com/cmusatyalab/openface">GitHub</a></td><td>2016</td><td>Python &amp; Lua</td><td>× conda</td></tr><tr><td>OpenFace V2</td><td><a href="https://github.com/TadasBaltrusaitis/OpenFace">Link</a></td><td>2018</td><td>C++</td><td>× compile locally</td></tr><tr><td>face-alignment</td><td><a href="https://github.com/1adrianb/face-alignment">Link</a></td><td>2017</td><td>Python &amp; Lua</td><td>√ face-alignment</td></tr></tbody></table><h3 id="eye-blinking"><a class="markdownIt-Anchor" href="#eye-blinking"></a> Eye Blinking</h3><h4 id="datasets-3"><a class="markdownIt-Anchor" href="#datasets-3"></a> Datasets</h4><table><thead><tr><th>Name</th><th>Site</th><th>Description</th></tr></thead><tbody><tr><td>Closed Eyes In The Wild (CEW)</td><td><a href="http://parnec.nuaa.edu.cn/xtan/data/ClosedEyeDatabases.html">Link</a></td><td>2423 subjects, among which 1192 subjects with both eyes closed are collected directly from Internet, and 1231 subjects with eyes open are selected from the Labeled Face in the Wild (LFW [2]) database. Cropped coarse faces resized to the 100×100 and extract eye patches of 24×24 centered at the localized eye position.</td></tr><tr><td>EBV</td><td><a href="https://drive.google.com/file/d/1jJTImI-QkmGYFS-0UmE1qvqbwoYOCPnR/view?usp=sharing">Link</a></td><td>It contains <strong>11376</strong> video clips, each clip has around <strong>15</strong> image series, whether contains a blink or not. The video fps they use is <strong>30</strong>.</td></tr><tr><td>Eyeblink8</td><td><a href="https://www.blinkingmatters.com/research">Link</a></td><td>8 videos with 4 individuals (1 wearing glasses). Videos are recorded in a home environment. 408 eye blinks on 70 992 annotated frames with resolution 640x480.</td></tr><tr><td>MRL</td><td><a href="http://mrl.cs.vsb.cz/eyedataset">Link</a></td><td>Infrared images in low and high resolution, all captured in various lightning conditions and by different devices. Approximately 15 000 annotation for pupil points (images).</td></tr><tr><td>RT-BENE</td><td><a href="https://zenodo.org/record/3685316#.XmL4pJP0lQI">Link</a></td><td>Annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed.</td></tr></tbody></table><h4 id="researches-3"><a class="markdownIt-Anchor" href="#researches-3"></a> Researches</h4><table><thead><tr><th>Name</th><th>Paper</th><th>Code</th><th>Description</th><th>Pre-trained Model</th></tr></thead><tbody><tr><td>In Ictu Oculi</td><td><a href="https://arxiv.org/abs/1806.02877">Link</a></td><td><a href="https://github.com/danmohaha/WIFS2018_In_Ictu_Oculi">Link</a></td><td>Using Blink to detect Fake Video.</td><td><a href="https://drive.google.com/file/d/1OJZ4mvZefwMJ7Knpsf_RFhCsA4xbAOMc/view">429M</a></td></tr><tr><td>RT-GENE &amp; RT-BENE</td><td><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html">Link</a></td><td><a href="https://github.com/Tobias-Fischer/rt_gene">Link</a></td><td>Robust gaze estimation in natural environments.</td><td></td></tr></tbody></table><h4 id="my-update"><a class="markdownIt-Anchor" href="#my-update"></a> My Update</h4><ol><li>Build a new Resnet18-LSTM based eye blink detection network, which reaches 99.8% accuracy on both training and testing set the same as “In Ictu Oculi” paper proposed.</li><li>Build a new mixed dataset which has three classes: open, closed, and not-eye. Not-eye class includes hand, cup, microphone, hat, fast food, and bedroom background. They are frequently appearing objects in the gamer’s streaming video. In many cases, part of the face are covered by this kind of things, but the face landmark detector can still give a predicted landmark. For example, one’s left eye may be shield by a microphone constantly due to the camera angle, but we can still get the useful information from his right eye. So in these cases, the new dataset will be more representative.</li><li>All of these new 6-classes images are collected from google image. Total number is <strong>2738</strong>, image number of each class is roughly balanced. I did a manual selecting to make sure the image is usable and representative of that classes. Every collected image will be resized to <strong>48, 64, and 128</strong>(the short edge), and then randomly crop three <strong>(32,32)</strong> images from each resized image. At last, <strong>32855</strong> images are collected.</li><li>EBV dataset from the author “In Ictu Oculi” is used as base dataset. It contains <strong>11376</strong> video clips, each clip has around <strong>15</strong> image series, whether contains a blink or not. The video fps they use is <strong>30</strong>. I build the new dataset by inserting 1, 2, or 3 continuous background images into the video clip image folder. The insert position is random, and the proportion of inserting 0,1,2,3 background images is: 40%, 10%, 30%, 20%. The inserted images is also randomly selected continuous images from all generated <strong>32855</strong> images. Training and validation set separation is the same as original dataset.</li><li>A new “Robust-Eye-Blink” network is trained based on this new dataset, and after sufficient training, it can reach 99.7% at both train and test dataset.</li></ol><h3 id="head-pose"><a class="markdownIt-Anchor" href="#head-pose"></a> Head Pose</h3><h4 id="calculation"><a class="markdownIt-Anchor" href="#calculation"></a> Calculation</h4><p>3D Facial Landmark -&gt; <a href="https://en.wikipedia.org/wiki/Rotation_matrix">Rotation matrix</a> -&gt; <a href="https://en.wikipedia.org/wiki/Euler_angles">Euler angles</a> -&gt; <a href="https://carsexplained.wordpress.com/2017/02/21/fundamentals-of-car-science-pitch-and-roll/">(yaw, pitch, roll)</a></p><img src="/2020/03/06/face-features/1*U4ZQ8UjzouVMRo2Fgsz7UA.png" alt="yaw pitch roll of head" style="zoom:50%;"><p>Visualization of yaw, pitch and roll: <a href="http://www.ctralie.com/Teaching/COMPSCI290/Materials/EulerAnglesViz/">Link</a></p><p><strong>Euler angles:</strong></p><p><img src="/2020/03/06/face-features/300px-Eulerangles.svg.png" alt="img"></p><h5 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h5><img src="/2020/03/06/face-features/image-20200306134751864.png" alt="image-20200306134751864" style="zoom:40%;"><p>Use the 3D landmark computed, and set the vector from 1 to 17 as the x axis, and 9 to 28 as the y axis, then the z axis is computed by set it perpendicular to both x and y, pointing out of front face.</p><p>Now, the head pose detection part has been wrapped into a module, we can get the pose within one line.</p><h4 id="code-reference"><a class="markdownIt-Anchor" href="#code-reference"></a> Code Reference</h4><ol><li><a href="https://github.com/jerryhouuu/Face-Yaw-Roll-Pitch-from-Pose-Estimation-using-OpenCV">Face-Yaw-Roll-Pitch-from-Pose-Estimation-using-OpenCV</a></li><li><a href="https://gist.github.com/crmccreary/1593090">euler_angles_from_rotation_matrix</a></li></ol><h3 id="emotion"><a class="markdownIt-Anchor" href="#emotion"></a> Emotion</h3><h4 id="can-facial-expression-really-tell-emotion"><a class="markdownIt-Anchor" href="#can-facial-expression-really-tell-emotion"></a> Can facial expression really tell emotion</h4><h5 id="cons"><a class="markdownIt-Anchor" href="#cons"></a> Cons</h5><ol><li>Facial expression is not universal, it also depend on culture and education. Like in Asia, people tend to convey more emotion in their eyes, but in western culture, people use their mouth to deliver more information.</li><li>Human can easily trick the face expression to emotion system, since what they show is not necessarily what they feel.</li><li>One certain facial expression can have multiple possible meanings, and this tend to depend on things near the face, namely the context. Like a soccer player win the game and shouting, without the context, you will judge him as angry or so.</li><li>The current facial expression classification method usually classify all of the emotion into several classes, like 6 or 7. But the fact is that each big emotion contain multiple sub-emotions which is more delicate. They can overlap or differ.</li><li>After reading 1000 papers, they find there was little to no evidence that people can reliably infer someone else’s emotional state from a set of facial movements.</li></ol><h5 id="pros"><a class="markdownIt-Anchor" href="#pros"></a> Pros</h5><ol><li>It is actually accurate. Affectiva has reached an accuracy of more than 90%.</li><li>Most of the culture share the similar facial expression.</li></ol><h4 id="datasets-4"><a class="markdownIt-Anchor" href="#datasets-4"></a> Datasets</h4><table><thead><tr><th>Name</th><th>Site</th><th>Description</th></tr></thead><tbody><tr><td>FER2013</td><td><a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">Kaggle Link</a></td><td>48x48 pixel grayscale images of faces. (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples. The public test set consists of 3,589 examples. The final test set consists of another 3,589 examples.</td></tr><tr><td><a href="https://ieeexplore.ieee.org/document/5543262">CK+</a></td><td><a href="http://www.consortium.ri.cmu.edu/ckagree/">Link</a></td><td>Posed Facial Expressions: 593 sequences from 123 subjects.</td></tr></tbody></table><h4 id="wrapped-libraries-3"><a class="markdownIt-Anchor" href="#wrapped-libraries-3"></a> Wrapped Libraries</h4><table><thead><tr><th>Name</th><th>Site</th><th>Year</th><th>Language</th><th>Pip/Pt model</th></tr></thead><tbody><tr><td>Facial-Expression-Recognition</td><td><a href="https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch">Link</a></td><td>2018</td><td>Python(PT)</td><td><a href="https://drive.google.com/file/d/1Oy_9YmpkSKX1Q8jkOhJbz3Mc7qjyISzU/view">76M</a></td></tr></tbody></table><h4 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h4><ol><li><a href="https://www.nature.com/articles/d41586-020-00507-5">Why faces don’t always tell the truth about feelings</a></li></ol><p>By Zhongyang Zhang</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;some-facts&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#some-facts&quot;&gt;&lt;/a&gt; Some Facts&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The output of face detector is not always </summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="machine-earning" scheme="https://www.miracleyoo.com/tags/machine-earning/"/>
    
    <category term="face-detection" scheme="https://www.miracleyoo.com/tags/face-detection/"/>
    
    <category term="face-alignment" scheme="https://www.miracleyoo.com/tags/face-alignment/"/>
    
    <category term="face-features" scheme="https://www.miracleyoo.com/tags/face-features/"/>
    
    <category term="face-expression" scheme="https://www.miracleyoo.com/tags/face-expression/"/>
    
    <category term="eye-blink" scheme="https://www.miracleyoo.com/tags/eye-blink/"/>
    
    <category term="head-pose" scheme="https://www.miracleyoo.com/tags/head-pose/"/>
    
  </entry>
  
  <entry>
    <title>Python 在类的静态函数中调用另一个静态函数</title>
    <link href="https://www.miracleyoo.com/2020/03/04/python-call-static-in-static/"/>
    <id>https://www.miracleyoo.com/2020/03/04/python-call-static-in-static/</id>
    <published>2020-03-04T23:28:42.000Z</published>
    <updated>2020-04-26T22:28:54.930Z</updated>
    
    <content type="html"><![CDATA[<p>有两种常见解决办法：</p><ol><li>调用目标函数的<code>__func__()</code>方法。</li><li>使用<code>CLASS_NAME.target_func()</code>方法。这种方法更加干净、Pythonic。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Klass</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod  </span><span class="comment"># use as decorator</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stat_func</span>():</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">42</span></span><br><span class="line"></span><br><span class="line">    _ANS = stat_func.__func__()  <span class="comment"># call the staticmethod</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span>(<span class="params">self</span>):</span></span><br><span class="line">        ret = Klass.stat_func()</span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;有两种常见解决办法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调用目标函数的&lt;code&gt;__func__()&lt;/code&gt;方法。&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;CLASS_NAME.target_func()&lt;/code&gt;方法。这种方法更加干净、Pythonic。&lt;/li&gt;
&lt;/ol</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python的赋值与深浅拷贝实例解析</title>
    <link href="https://www.miracleyoo.com/2020/02/11/python-copy/"/>
    <id>https://www.miracleyoo.com/2020/02/11/python-copy/</id>
    <published>2020-02-11T23:54:55.000Z</published>
    <updated>2020-04-26T22:55:09.170Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对象是一个定值"><a class="markdownIt-Anchor" href="#对象是一个定值"></a> 对象是一个定值</h2><p>此时三种方法的作用实际上是相同的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">a = <span class="string">&quot;亚丝娜&quot;</span></span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br></pre></td></tr></table></figure><p>输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">源：id(a)-&gt;&gt;&gt; 4394180400</span><br><span class="line">赋值：id(b)-&gt;&gt;&gt; 4394180400</span><br><span class="line">浅拷贝：id(c)-&gt;&gt;&gt; 4394180400</span><br><span class="line">深拷贝：id(d)-&gt;&gt;&gt; 4394180400</span><br></pre></td></tr></table></figure><p>如果a的值被更改，只有a本身的id会改变，bcd都不变。</p><h2 id="对象是一个引用"><a class="markdownIt-Anchor" href="#对象是一个引用"></a> 对象是一个引用</h2><ul><li>首先，元组，数组，字典，类等的本质都是引用，或称之为“指针”，每个引用指向的实体都是有其相应地址的，比如这里的<code>“亚丝娜”</code>在内存中有一个具体的地址，而<code>[“亚丝娜”]</code>则是一个对于内存中“亚丝娜”实体的一个引用集，这个引用集本身也有一个独特的地址。</li><li>对于不可变对象，Python 用引用计数的方式管理它们，所以 Python 不会对值相同的不可变对象，申请单独的内存空间。只会记录它的引用次数。</li><li>使用“引用集”赋值是把引用集本身的地址赋给了左边的变量，即一个引用的引用。</li><li>使用赋值的方法得到的对象，当原“引用集”中的任何引用发生任何改变时，其都会随着改变，就像一个快捷方式。但如果原“引用集”直接被覆盖了，则不会随之改变。</li></ul><h3 id="例"><a class="markdownIt-Anchor" href="#例"></a> 例：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一位数组更改内层元素</span></span><br><span class="line">a=[<span class="string">&quot;亚丝娜&quot;</span>]</span><br><span class="line">b=a</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>] = <span class="string">&quot;桐人&quot;</span></span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br><span class="line">print(a,b,c,d)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">源：id(a)-&gt;&gt;&gt; 4585988416</span></span><br><span class="line"><span class="string">赋值：id(b)-&gt;&gt;&gt; 4585988416</span></span><br><span class="line"><span class="string">浅拷贝：id(c)-&gt;&gt;&gt; 4585374256</span></span><br><span class="line"><span class="string">深拷贝：id(d)-&gt;&gt;&gt; 4585117648</span></span><br><span class="line"><span class="string">[&#x27;桐人&#x27;] [&#x27;桐人&#x27;] [&#x27;亚丝娜&#x27;] [&#x27;亚丝娜&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一位数组完全变更</span></span><br><span class="line">a=[<span class="string">&quot;亚丝娜&quot;</span>]</span><br><span class="line">b=a</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">a=[<span class="string">&quot;利兹&quot;</span>]</span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br><span class="line">print(a,b,c,d)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">源：id(a)-&gt;&gt;&gt; 4586047600</span></span><br><span class="line"><span class="string">赋值：id(b)-&gt;&gt;&gt; 4583430096</span></span><br><span class="line"><span class="string">浅拷贝：id(c)-&gt;&gt;&gt; 4585223344</span></span><br><span class="line"><span class="string">深拷贝：id(d)-&gt;&gt;&gt; 4585988416</span></span><br><span class="line"><span class="string">[&#x27;利兹&#x27;] [&#x27;亚丝娜&#x27;] [&#x27;亚丝娜&#x27;] [&#x27;亚丝娜&#x27;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组更改最内层元素</span></span><br><span class="line">a=[[<span class="string">&quot;亚丝娜&quot;</span>]]</span><br><span class="line">b=a</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">print(<span class="string">&quot;源（重赋值前）：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>]=<span class="string">&quot;利兹&quot;</span></span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br><span class="line">print(a,b,c,d)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">源（重赋值前）：id(a)-&gt;&gt;&gt; 4586327008</span></span><br><span class="line"><span class="string">源：id(a)-&gt;&gt;&gt; 4586327008</span></span><br><span class="line"><span class="string">赋值：id(b)-&gt;&gt;&gt; 4586327008</span></span><br><span class="line"><span class="string">浅拷贝：id(c)-&gt;&gt;&gt; 4586041088</span></span><br><span class="line"><span class="string">深拷贝：id(d)-&gt;&gt;&gt; 4585375216</span></span><br><span class="line"><span class="string">[[&#x27;利兹&#x27;]] [[&#x27;利兹&#x27;]] [[&#x27;利兹&#x27;]] [[&#x27;亚丝娜&#x27;]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组更改第一维</span></span><br><span class="line">a=[[<span class="string">&quot;亚丝娜&quot;</span>]]</span><br><span class="line">b=a</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">print(<span class="string">&quot;源（重赋值前）：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>]=[<span class="string">&quot;利兹&quot;</span>]</span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br><span class="line">print(a,b,c,d)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">源（重赋值前）：id(a)-&gt;&gt;&gt; 4585239328</span></span><br><span class="line"><span class="string">源：id(a)-&gt;&gt;&gt; 4585239328</span></span><br><span class="line"><span class="string">赋值：id(b)-&gt;&gt;&gt; 4585239328</span></span><br><span class="line"><span class="string">浅拷贝：id(c)-&gt;&gt;&gt; 4586082944</span></span><br><span class="line"><span class="string">深拷贝：id(d)-&gt;&gt;&gt; 4586327008</span></span><br><span class="line"><span class="string">[[&#x27;利兹&#x27;]] [[&#x27;利兹&#x27;]] [[&#x27;亚丝娜&#x27;]] [[&#x27;亚丝娜&#x27;]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组完全变更</span></span><br><span class="line">a=[[<span class="string">&quot;亚丝娜&quot;</span>]]</span><br><span class="line">b=a</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">print(<span class="string">&quot;源（重赋值前）：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line"></span><br><span class="line">a=[[<span class="string">&quot;利兹&quot;</span>]]</span><br><span class="line">print(<span class="string">&quot;源：id(a)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(a))</span><br><span class="line">print(<span class="string">&quot;赋值：id(b)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(b))</span><br><span class="line">print(<span class="string">&quot;浅拷贝：id(c)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(c))</span><br><span class="line">print(<span class="string">&quot;深拷贝：id(d)-&gt;&gt;&gt;&quot;</span>, <span class="built_in">id</span>(d))</span><br><span class="line">print(a,b,c,d)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">源（重赋值前）：id(a)-&gt;&gt;&gt; 4586082944</span></span><br><span class="line"><span class="string">源：id(a)-&gt;&gt;&gt; 4585468704</span></span><br><span class="line"><span class="string">赋值：id(b)-&gt;&gt;&gt; 4586082944</span></span><br><span class="line"><span class="string">浅拷贝：id(c)-&gt;&gt;&gt; 4586327008</span></span><br><span class="line"><span class="string">深拷贝：id(d)-&gt;&gt;&gt; 4586041088</span></span><br><span class="line"><span class="string">[[&#x27;利兹&#x27;]] [[&#x27;亚丝娜&#x27;]] [[&#x27;亚丝娜&#x27;]] [[&#x27;亚丝娜&#x27;]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>赋值是完全的快捷方式。</li><li>浅拷贝的实质是对一个“引用集”的所有引用的拷贝，即拷贝了一份“引用集”中记录的所有的这些的不可变对象的地址，但只拷贝了一层，或称并没有把这些对象本身拷贝一遍。</li><li>如果引用集里还有引用集x，那么浅拷贝对x的作用和赋值相同，即如果x里的元素变了，浅拷贝的结果还是会跟着变。</li><li>深拷贝把拷贝对象里面的所有层的引用集全部做了拷贝动作，直到引用到不可变变量，所以可以说深拷贝出来的结果和其拷贝对象没有任何耦合关系了。</li></ul><h2 id="简要版本"><a class="markdownIt-Anchor" href="#简要版本"></a> 简要版本</h2><ul><li>由于 Python 内部引用计数的特性，对于不可变对象，浅拷贝和深拷贝的作用是一致的，就相当于复制了一份副本，原对象内部的不可变对象的改变，不会影响到复制对象。</li><li>浅拷贝的拷贝。其实是拷贝了原始元素的引用（内存地址），所以当拷贝可变对象时，原对象内可变对象的对应元素的改变，会在复制对象的对应元素上，有所体现。</li><li>深拷贝在遇到可变对象时，又在内部做了新建了一个副本。所以，不管它内部的元素如何变化，都不会影响到原来副本的可变对象。</li></ul><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><p><a href="https://juejin.im/post/5c6943266fb9a049ed316931">5张图彻底理解Python中的浅拷贝与深拷贝</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;对象是一个定值&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#对象是一个定值&quot;&gt;&lt;/a&gt; 对象是一个定值&lt;/h2&gt;
&lt;p&gt;此时三种方法的作用实际上是相同的。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;t</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>MATLAB 中与函数、方程相关内容</title>
    <link href="https://www.miracleyoo.com/2020/01/10/matlab-func/"/>
    <id>https://www.miracleyoo.com/2020/01/10/matlab-func/</id>
    <published>2020-01-11T00:01:11.000Z</published>
    <updated>2020-04-26T23:03:26.220Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号变量-syms"><a class="markdownIt-Anchor" href="#符号变量-syms"></a> 符号变量 syms</h2><p>在MATLAB中创建或定义一个函数需要用到符号变量。一般情况下，想要绘制函数图像时，往往使用<code>x=[a:0.01:b]</code>的方式先创建一个x的离散定义域，然后再用<code>y=func(x)</code>的方式定义函数本身，最后使用plot(x,y)的方式绘制图像。</p><p>然而，当涉及到函数极值、求导、方程求解、连续图像绘制等问题时，这种方法就不够用了。</p><p>想要创建一个<strong>符号函数</strong>，我们首先要创建一个或多个<strong>符号变量</strong>，用以表示符号函数本身。其定义方式即为<code>syms x x1 x2</code>。其中<code>x,x1,x2</code> 都是符号变量，一个符号函数可以由多个符号变量组成。</p><p>符号变量可以有定义域，这里或称<strong>限制条件</strong>。</p><p>限制条件可以在定义时就加上，但往往是较为简单的条件，如<em>positive</em>, <em>real</em>, <em>integer</em> 等。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% Create symbolic variables x and y, and assume that they are integers.</span></span><br><span class="line">syms x y integer</span><br><span class="line"></span><br><span class="line"><span class="comment">% Create another variable z, and assume that it has a positive rational value.</span></span><br><span class="line">syms z positive rational</span><br><span class="line"></span><br><span class="line"><span class="comment">% Check assumptions on each variable. For example, check assumptions set on the variable x.</span></span><br><span class="line">assumptions(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">% Clear assumptions on x, y, and z.</span></span><br><span class="line">assume([x y z],<span class="string">&#x27;clear&#x27;</span>)</span><br><span class="line">assumptions</span><br><span class="line"></span><br><span class="line"><span class="comment">% Create a 1-by-3 symbolic array a and assume that the array elements have real values.</span></span><br><span class="line">syms a [<span class="number">1</span> <span class="number">3</span>] <span class="built_in">real</span></span><br><span class="line">assumptions</span><br></pre></td></tr></table></figure><p>其次，我们也可以使用更加精确的方法进行限定，即使用<code>assume</code>命令。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">syms x</span><br><span class="line">assume(<span class="number">0</span>&lt;x&lt;<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h2 id="定义函数"><a class="markdownIt-Anchor" href="#定义函数"></a> 定义函数</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 方法1 使用这种方法不用特意定义自变量</span></span><br><span class="line">y = @(t) <span class="built_in">cos</span>(<span class="number">3</span>*t);</span><br><span class="line"></span><br><span class="line"><span class="comment">% 方法2 先定义自变量为符号变量再定义函数本身</span></span><br><span class="line">syms x</span><br><span class="line">fun = <span class="number">0.5</span>*x*(<span class="built_in">exp</span>(<span class="number">-2</span>*x)+<span class="built_in">exp</span>(<span class="number">-1.5</span>*x)+<span class="built_in">exp</span>(-x))</span><br></pre></td></tr></table></figure><h2 id="快速绘制函数图像"><a class="markdownIt-Anchor" href="#快速绘制函数图像"></a> 快速绘制函数图像</h2><p>MATLAB中的函数<code>fplot</code>可以迅速绘制一个符号函数的函数图像，并可以对其显示的x范围进行设定。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fplot(fun)</span><br><span class="line">fplot(fun,x,[<span class="number">1</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>默认情况下，其绘制区间为<code>[-5, 5]</code>，但如果符号变量本身有定义域限制，则会优先其定义域，优先级最高的是在绘制函数中指定的绘制区间。</p><p>当然，<code>fplot</code>函数还可以绘制多条曲线、分段函数以及参数函数等，详见<a href="https://ww2.mathworks.cn/help/matlab/ref/fplot.html">帮助文档</a>，这里给出几个简单常用例子。</p><p>指定绘图区间并绘制分段函数<br><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup><mtext> </mtext><mi mathvariant="normal">−</mi><mn>3</mn><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">e^x\space −3&lt;x&lt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.747722em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mspace"> </span><span class="mord">−</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mtext> </mtext><mn>0</mn><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">cos(x)\space 0&lt;x&lt;3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"> </span><span class="mord">0</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用 <span class="built_in">hold</span> on 绘制多个线条。使用 fplot 的第二个输入参数指定绘图区间。使用 <span class="string">&#x27;b&#x27;</span> 将绘制的线条颜色指定为蓝色。在相同坐标区中绘制多个线条时，坐标轴范围会调整以容纳所有数据。</span><br><span class="line">fplot(@(x) <span class="built_in">exp</span>(x),[<span class="number">-3</span> <span class="number">0</span>],<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">fplot(@(x) <span class="built_in">cos</span>(x),[<span class="number">0</span> <span class="number">3</span>],<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="built_in">hold</span> off</span><br><span class="line">grid on</span><br></pre></td></tr></table></figure><img src="/2020/01/10/matlab-func/specifyplottingintervalandplotpiecewisefunctionsexample_01_zh_CN.png" alt="img" style="zoom:50%;"><p>当然，使用<code>fplot</code>方法绘制的图像也是可以进行样式自定义的：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fplot(@(x) <span class="built_in">sin</span>(x+<span class="built_in">pi</span>/<span class="number">5</span>),<span class="string">&#x27;Linewidth&#x27;</span>,<span class="number">2</span>);</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">fplot(@(x) <span class="built_in">sin</span>(x-<span class="built_in">pi</span>/<span class="number">5</span>),<span class="string">&#x27;--or&#x27;</span>);</span><br><span class="line">fplot(@(x) <span class="built_in">sin</span>(x),<span class="string">&#x27;-.*c&#x27;</span>)</span><br><span class="line"><span class="built_in">hold</span> off</span><br></pre></td></tr></table></figure><img src="/2020/01/10/matlab-func/specifylinepropertiesanddisplaymarkersexample_01_zh_CN.png" alt="img" style="zoom:50%;"><h2 id="解方程"><a class="markdownIt-Anchor" href="#解方程"></a> 解方程</h2><p>MATLAB中有两种常用解方程的函数：<code>solve</code>和<code>vpasolve</code>。前者会返回一个符号解，它的做法就像人类手工推理一样，计算出所有的符号解。而后者则会计算方程的数值解，且只会返回其找到的第一个数值解。</p><p>当我们想要使用<code>vpasolve</code>算出某个x范围中的所有解时候，我们有两种方法：</p><h3 id="1-画出方程对应的函数图像并传给vpasolve一个猜测起点"><a class="markdownIt-Anchor" href="#1-画出方程对应的函数图像并传给vpasolve一个猜测起点"></a> 1. 画出方程对应的函数图像，并传给<code>vpasolve</code>一个猜测起点</h3><p>如给定方程<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>200</mn><mo>∗</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>3</mn></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">200*sin(x) = x^3 - 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">0</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>, 我们先画出它的图像进行观察：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms x</span><br><span class="line">eqnLeft = <span class="number">200</span>*<span class="built_in">sin</span>(x);</span><br><span class="line">eqnRight = x^<span class="number">3</span> - <span class="number">1</span>;</span><br><span class="line">fplot([eqnLeft eqnRight])</span><br><span class="line">title([texlabel(eqnLeft) <span class="string">&#x27; = &#x27;</span> texlabel(eqnRight)])</span><br></pre></td></tr></table></figure><img src="/2020/01/10/matlab-func/FindMultipleSolutionsBySpecifyingInitialGuessesExample_01.png" alt="img" style="zoom:50%;"><p>观察后发现，这个方程有三个解，分别在-3, 0, 4的附近，于是我们可以用以下语句找到其所有的三个解</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">S1 = vpasolve(eqnLeft == eqnRight, x);</span><br><span class="line">S2 = vpasolve(eqnLeft == eqnRight, x, <span class="number">-3</span>);</span><br><span class="line">S3 = vpasolve(eqnLeft == eqnRight, x, <span class="number">4</span>);</span><br></pre></td></tr></table></figure><h3 id="2-使vpasolve拥有一个随机起点并进行循环"><a class="markdownIt-Anchor" href="#2-使vpasolve拥有一个随机起点并进行循环"></a> 2. 使<code>vpasolve</code>拥有一个随机起点，并进行循环</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n = <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    S = vpasolve(f,x,[<span class="number">0</span>,<span class="number">2</span>],<span class="string">&#x27;Random&#x27;</span>,<span class="built_in">true</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-仅针对多项式函数"><a class="markdownIt-Anchor" href="#3-仅针对多项式函数"></a> 3. 仅针对多项式函数</h3><p>如果你的函数是一个标准的多相似函数，那么你可以使用<code>roots</code>函数一次性得到所有的解。详情请参阅<a href="https://www.mathworks.com/help/matlab/ref/roots.html">帮助文档</a>。</p><h2 id="寻找函数极大极小值"><a class="markdownIt-Anchor" href="#寻找函数极大极小值"></a> 寻找函数极大极小值</h2><p>在MATLAB中似乎没有直接一键求出函数的最值的办法，但我们却可以用<code>fminsearch</code>求出某个点附近的极值。</p><p>与前面提到的解方程类似，由于该函数并不会直接的把全局最值给你，所以最好先把函数图像画出来，然后观察需要求的极值在那个点附近，然后使用<code>fminsearch</code>函数把相关点的横坐标解出，如果需要最值的值，再把这个横坐标带回去求值。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syms x</span><br><span class="line">y=<span class="built_in">real</span>((<span class="number">1</span>-<span class="built_in">exp</span>(<span class="number">8</span>*<span class="built_in">i</span>*<span class="built_in">pi</span>*<span class="built_in">cos</span>(x)))/(<span class="number">1</span>-<span class="built_in">exp</span>(<span class="built_in">i</span>*<span class="built_in">pi</span>*<span class="built_in">cos</span>(x))));</span><br><span class="line">fplot(y,[<span class="number">0</span>,<span class="number">3</span>]);</span><br><span class="line">fminsearch(matlabFunction(-y),<span class="number">1.5</span>);</span><br><span class="line"><span class="comment">% ans = 1.5708</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="/2020/01/10/matlab-func/image-20200216152601687.png" alt="image-20200216152601687" style="zoom:35%;"><p>请注意，这里在search的时候我将y改为了-y，因为我要找的是极大值而非极小值。</p><p>接着，我们将函数y变为<code>matlabFunction</code>型变量，在根据刚才输出的值求出极值大小：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q=matlabFunction(y)</span><br><span class="line">q(<span class="number">1.5708</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>于是，我们就找到了函数在该点附近的极值。</p><p>你甚至可以看到MATLAB的优化路径：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">options = optimset(<span class="string">&#x27;PlotFcns&#x27;</span>,@optimplotfval);</span><br><span class="line">fun = @(x)<span class="number">100</span>*(x(<span class="number">2</span>) - x(<span class="number">1</span>)^<span class="number">2</span>)^<span class="number">2</span> + (<span class="number">1</span> - x(<span class="number">1</span>))^<span class="number">2</span>;</span><br><span class="line">x0 = [<span class="number">-1.2</span>,<span class="number">1</span>];</span><br><span class="line">x = fminsearch(fun,x0,options)</span><br></pre></td></tr></table></figure><img src="/2020/01/10/matlab-func/image-20200216153158018.png" alt="image-20200216153158018" style="zoom:33%;"><h2 id="对函数求导"><a class="markdownIt-Anchor" href="#对函数求导"></a> 对函数求导</h2><p>定义好一个符号函数后，直接使用<code>diff</code>命令即可对函数进行符号求导。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms x</span><br><span class="line">fun=<span class="number">0.5</span>*x*(<span class="built_in">exp</span>(<span class="number">-2</span>*x)+<span class="built_in">exp</span>(<span class="number">-1.5</span>*x)+<span class="built_in">exp</span>(-x))</span><br><span class="line">diff(fun) <span class="comment">% or diff(fun, x)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>需要注意的一点是，<code>diff</code>函数不但对符号函数有效，其对数列也是有效的：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = [<span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">8</span> <span class="number">13</span> <span class="number">21</span>];</span><br><span class="line">Y = diff(X)</span><br><span class="line"><span class="comment">% Y = 1×7</span></span><br><span class="line"><span class="comment">%     0     1     1     2     3     5     8</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;符号变量-syms&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#符号变量-syms&quot;&gt;&lt;/a&gt; 符号变量 syms&lt;/h2&gt;
&lt;p&gt;在MATLAB中创建或定义一个函数需要用到符号变量。一般情况下，想要绘制函数图像时，往往使用&lt;code</summary>
      
    
    
    
    
    <category term="matlab" scheme="https://www.miracleyoo.com/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Core Code Research</title>
    <link href="https://www.miracleyoo.com/2019/12/11/Pytorch-Core-Code-Research/"/>
    <id>https://www.miracleyoo.com/2019/12/11/Pytorch-Core-Code-Research/</id>
    <published>2019-12-11T21:35:01.000Z</published>
    <updated>2019-12-12T01:08:49.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pytorch-release-version-composition"><a class="markdownIt-Anchor" href="#pytorch-release-version-composition"></a> Pytorch Release Version Composition</h2><p>The repository cloned from GitHub <a href="https://github.com/pytorch/pytorch">pytorch/pytorch</a> is different from the package we download using <code>pip install</code> or <code>conda install</code>. In fact, the former contains many C/C++ based files, which consist of the basic of Pytorch, while the latter is more concise and contains compiled libraries and dll files instead.</p><p>Here, let’s discuss the release version, or the installed package at first. The package has a lot of components, Here I only pick out some most important parts to do explanation.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128191350467.png" alt="image-20191128191350467"></p><h4 id="nn"><a class="markdownIt-Anchor" href="#nn"></a> nn</h4><p>All deep learning layers’ python entrance are located here. They mainly collect parameters from init input and do some modification to the input data. After that it will send core computation operation together with parameters into <code>torch._C</code> based functions.</p><h4 id="autograd"><a class="markdownIt-Anchor" href="#autograd"></a> autograd</h4><p>Contains a series of base functions which serves for back propagation. Also, if you dig in, the core implementation is still from C libraries. Variable wrap is also put here, but now it is just omitted because of the merge of tensor and Variable.</p><h4 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h4><p>Mainly these parts are contained in <code>cuda</code> folder: Stream, Event, Broadcast and Random.</p><ul><li>A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams.</li><li>CUDA events are synchronization markers that can be used to monitor the device’s progress, to accurately measure timing, and to synchronize CUDA streams.</li><li>Broadcast related functions mainly do the jobs to make sure operations run on different GPUs and gather correctly.</li></ul><h4 id="optim"><a class="markdownIt-Anchor" href="#optim"></a> optim</h4><p><code>torch.optim</code> is a package implementing various optimization algorithms. Most commonly used methods are already supported, like <code>adam</code>, <code>sgd</code> and <code>adagrad</code>.</p><h4 id="distributed"><a class="markdownIt-Anchor" href="#distributed"></a> distributed</h4><p>The <code>distributions</code> package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization.</p><h4 id="onnx"><a class="markdownIt-Anchor" href="#onnx"></a> onnx</h4><p>The <code>torch.onnx</code> module contains functions to export models into the ONNX IR format. These models can be loaded with the ONNX library and then converted to models which run on other deep learning frameworks.</p><h4 id="tensor"><a class="markdownIt-Anchor" href="#tensor"></a> tensor</h4><p>Most basic tensor class defined here. It inherit a super class from C lib, called <code>torch._C._TensorBase</code> . And it attaches a lot of method like <code>register_hook</code>,<code>resize</code>, <code>norm</code> to tensor class. All these method eventually call C based libraries.</p><h4 id="lib"><a class="markdownIt-Anchor" href="#lib"></a> lib</h4><p>The library where compiled C/C++ files located. There are <code>.dll</code> files as well as <code>.lib</code> files. According to the bug reports on google, I believe <code>.dll</code> files are specially compiled for the compatibility of windows and <code>.lib</code> can be used in linux and some of them are also usable in Windows.(If you find a more accurate explanation, please tell me:) These files included: <code>_C.lib</code>, <code>c10.lib</code>, <code>torch.lib</code>, <code>c10_cuda.lib</code>.</p><h4 id="functional"><a class="markdownIt-Anchor" href="#functional"></a> functional</h4><p>Functions related to tensor operation are all located here. In fact, again, they are wrappers of functions from C libraries. You can find functions like <code>tensordot</code>, <code>unique</code>, <code>split</code> in this file.</p><h4 id="utils"><a class="markdownIt-Anchor" href="#utils"></a> utils</h4><p>All kinds of utilities codes are located here. This include dataset related code <code>dataloader.py</code>, <code>dataset.py</code>, <code>sampler.py</code>, also include save and output related <code>checkpoint.py</code>. Some TensorBoard support can also be found here.</p><h2 id="how-pytorch-manage-its-inner-resource"><a class="markdownIt-Anchor" href="#how-pytorch-manage-its-inner-resource"></a> How Pytorch manage its inner resource</h2><h3 id="what-is-tensor"><a class="markdownIt-Anchor" href="#what-is-tensor"></a> What is Tensor</h3><p>In <a href="https://en.wikipedia.org/wiki/Mathematics">mathematics</a>, a <strong>tensor</strong> is an algebraic object that describes a <a href="https://en.wikipedia.org/wiki/Linear_mapping">linear mapping</a> from one set of algebraic objects to another. Objects that tensors may map between include, but are not limited to, <a href="https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)">vectors</a> and <a href="https://en.wikipedia.org/wiki/Scalar_(mathematics)">scalars</a>, and, recursively, even other tensors. The tensor is the central data structure in PyTorch.  It’s an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera. We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory,  CUDA memory)</p><p>![what is tensor](Simple Tutorials on Tensors.jpg)</p><h3 id="how-tensor-organizes"><a class="markdownIt-Anchor" href="#how-tensor-organizes"></a> How Tensor organizes</h3><p>TH library is responsible for the computation,storage and memory management of Tensor. It divide the “Tensor” into two separate parts: Storage and Access/View.</p><p>Storage: <strong>THStorage</strong>. It manage the way of storing the Tensor.</p><p>Access: <strong>THTensor</strong>. It provide a access to user.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191203093005864.png" alt="image-20191203093005864"></p><h4 id="data-storage"><a class="markdownIt-Anchor" href="#data-storage"></a> Data Storage</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">THStorage</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"> real *data;</span><br><span class="line"> <span class="keyword">ptrdiff_t</span> size;</span><br><span class="line"> <span class="keyword">int</span> refcount;</span><br><span class="line"> <span class="keyword">char</span> flag;</span><br><span class="line"> THAllocator *allocator;</span><br><span class="line"> <span class="keyword">void</span> *allocatorContext;</span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">THStorage</span> *<span class="title">view</span>;</span></span><br><span class="line">&#125; THStorage;</span><br></pre></td></tr></table></figure><ul><li>All of the “Tensor” in CPU is in fact a C pointer pointing to a data structure in memory like this. And it use reference count to do memory management.</li><li><strong>refcount</strong>: Here we apply reference count method to do automatic garbage collection. When the reference number becomes 0, this struct will be freed automatically.</li></ul><h4 id="data-access"><a class="markdownIt-Anchor" href="#data-access"></a> Data Access</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">THTensor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"> <span class="keyword">long</span> *size;</span><br><span class="line"> <span class="keyword">long</span> *stride;</span><br><span class="line"> <span class="keyword">int</span> nDimension;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// Attention: storage-&gt;size might be bigger than the size of tensor.</span></span><br><span class="line"> THStorage *storage;</span><br><span class="line"> <span class="keyword">ptrdiff_t</span> storageOffset;</span><br><span class="line"> <span class="keyword">int</span> refcount;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">char</span> flag;</span><br><span class="line"></span><br><span class="line">&#125; THTensor;</span><br></pre></td></tr></table></figure><ul><li><strong>nDimension</strong>: The number of dimensions</li><li><strong>size</strong>: It contains the length information of all dimensions.</li><li><strong>refcount</strong>: Reference count</li><li><strong>storage</strong>: Pointer of this data structure</li><li><strong>stride</strong>: The size of each dimension.</li></ul><h4 id="memory-allocator"><a class="markdownIt-Anchor" href="#memory-allocator"></a> Memory Allocator</h4><h5 id="c10coreallocatorh"><a class="markdownIt-Anchor" href="#c10coreallocatorh"></a> <code>/c10/core/Allocator.h</code></h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">Allocator</span> &#123;</span></span><br><span class="line">  <span class="keyword">virtual</span> ~Allocator() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">raw_allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> dptr = allocate(n);</span><br><span class="line">    AT_ASSERT(dptr.get() == dptr.get_context());</span><br><span class="line">    <span class="keyword">return</span> dptr.release_context();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">raw_deallocate</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> d = raw_deleter();</span><br><span class="line">    AT_ASSERT(d);</span><br><span class="line">    d(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>The <code>allocate</code> function is directly included from head file <code>memory</code>.</p><h5 id="atensrcththallocatorcpp"><a class="markdownIt-Anchor" href="#atensrcththallocatorcpp"></a> <code>/aten/src/TH/THAllocator.cpp</code></h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">at::DataPtr <span class="title">THMapAllocator::makeDataPtr</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *filename, <span class="keyword">int</span> flags, <span class="keyword">size_t</span> size, <span class="keyword">size_t</span>* actual_size_out)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span>* context = <span class="keyword">new</span> THMapAllocator(filename, flags, size);</span><br><span class="line">  <span class="keyword">if</span> (actual_size_out) *actual_size_out = context-&gt;size();</span><br><span class="line">  <span class="keyword">return</span> &#123;context-&gt;data(), context, &amp;deleteTHMapAllocator, at::DeviceType::CPU&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Default allocator is malloc/free allocator. malloc and realloc raise an error (using THError) on allocation failure.</p></blockquote><h3 id="understand-parameters"><a class="markdownIt-Anchor" href="#understand-parameters"></a> Understand Parameters</h3><p>It is hard and not straightforward enough to understand stride and storage offset, so let’s borrow some images from <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">ezyang</a>, who is supposed to be an inner developer of Pytorch, to elaborate this problem.</p><p>A tensor is a mathematical concept. But to represent it on our computers, we have to define some sort of physical representation for them. The most common representation is to lay out each element of the tensor contiguously in memory (that’s where the term contiguous comes from), writing out each row to memory.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128191750794.png" alt="image-20191128191750794"></p><p>Please notice the relationship of sizes and strides. If we get a tensor with a size of (D,H,W) and this tensor is directly defined by user rather than a slice or result of some operation, the stride of it will be (H*W, W, 1). You can compare and draw a conclusion yourself. Each stride element in a certain dimension will be the product of all the following dimensions, and the stride of the last dimension will be 1.</p><p>Physically, stride means how many blocks of memory computer need to skip to get to the starting position of the next corresponding dimension. And if we use a formula to compute the memory position of a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Tensor[i,j,k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span>, it will be <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>O</mi><mi>f</mi><mi>f</mi><mi>s</mi><mi>e</mi><mi>t</mi><mo>+</mo><mi>i</mi><mo>∗</mo><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>+</mo><mi>j</mi><mo>∗</mo><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>+</mo><mi>k</mi><mo>∗</mo><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">storageOffset + i * stride[0] + j * stride[1] + k * stride[2]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mopen">[</span><span class="mord">0</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mopen">[</span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mopen">[</span><span class="mord">2</span><span class="mclose">]</span></span></span></span>.</p><p>In the example image above, I’ve specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other. To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128192741907.png" alt="image-20191128192741907"></p><p>Then comes to the memory offset. What does this mean? As we has mentioned before, a tensor storage may support multiple tensor view, and if we sliced the first N elements, then we will start from N+1 memory position. The following examples will give a further explanation.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128193018098.png" alt="image-20191128193018098"></p><p>You can see in the left example, we start at the third element block, so that means we skip two block, and here the offset is 2. Because of the slice, the two dimensional tensor becomes one dimensional tensor, and conjoint elements are continuous in physical storage, this means the strides is [1]. Size is the number of elements in this case and it is 2.</p><p>In the right example, conjoint elements are not continuous, but it do start from the beginning, so the strides is [2] and offset is 0. There are still two elements in total so the sizes don’t change.</p><p>What’s more, if you still find it somehow difficult to understand, you may try <a href="https://ezyang.github.io/stride-visualizer/index.html">this website</a> to playing with these parameters and see the dynamic process.</p><h3 id="tensor-implementation-dispatch"><a class="markdownIt-Anchor" href="#tensor-implementation-dispatch"></a> Tensor implementation dispatch</h3><p>As we know, although in Python, you can use any type of data as you wish, as the interpreter will take care of the rest of the things. However, since the basic kernels are written in C/C++, functions from Python need to be dispatched into same functions with different input and device type. To a C/C++ functions, a certain function cannot take in <code>int</code> and <code>float</code> Tensor as a same <code>X</code> at the same time, they need separate implementation.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128221930437.png" alt="image-20191128221930437"></p><h3 id="how-to-dispatch"><a class="markdownIt-Anchor" href="#how-to-dispatch"></a> How to dispatch</h3><p>As we discussed above, the basic C/C++ implementation need to dispatch according to data and device type. But in code, how to actually do this work?</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128222130891.png" alt="image-20191128222130891"></p><p>There are basically three methods.</p><ol><li>Write these functions with different data and device type separately, and manually.</li><li>Using template function to build those dispatched function in the compiling time. But this only works in C++, while many code in Pytorch is still written in C.</li><li>Apply the magic item – Macro. By defining the function name as a Macro which takes in one or some parameters, like the data type name, we can compile this function in different types by <code>#define</code> and <code>#undef</code> multiple times, setting the variables in function name macro into various type name to compile the function into many copies which support different types.</li></ol><p>Here’s a simplified example:</p><h4 id="file-structure"><a class="markdownIt-Anchor" href="#file-structure"></a> File structure:</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── add.c <span class="comment"># Used to extend generic/add.c</span></span><br><span class="line">├── add.h <span class="comment"># Used to extend generic/add.h</span></span><br><span class="line">├── general.h <span class="comment"># Including other header files</span></span><br><span class="line">└── generic</span><br><span class="line"> ├── add.c <span class="comment"># Definition of generic function add</span></span><br><span class="line"> └── add.h <span class="comment"># Definition of generic type Vector</span></span><br></pre></td></tr></table></figure><h4 id="addh"><a class="markdownIt-Anchor" href="#addh"></a> add.h</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;general.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONCAT_2_EXPAND(A, B) A ## B</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONCAT_2(A, B) CONCAT_2_EXPAND(A, B)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONCAT_3_EXPAND(A, B, C) A ## B ## C</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONCAT_3(A, B, C) CONCAT_3_EXPAND(A, B, C)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Vector_(NAME) CONCAT_3(Num, Vector_, NAME)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Vector CONCAT_2(Num, Vector)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> num float</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Num Float</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;generic/add.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> num</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> Num</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> num double</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Num Double</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;generic/add.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> num</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> Num</span></span><br></pre></td></tr></table></figure><h4 id="addc"><a class="markdownIt-Anchor" href="#addc"></a> add.c</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;add.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> num float</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Num Float</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;generic/add.c&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> num</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> Num</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> num double</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Num Double</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;generic/add.c&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> num</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">undef</span> Num</span></span><br></pre></td></tr></table></figure><h4 id="genericaddh"><a class="markdownIt-Anchor" href="#genericaddh"></a> generic/add.h</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// generic/add.h</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Vector</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">num *data;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line">&#125; Vector;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">extern</span> <span class="keyword">void</span> <span class="title">Vector_</span><span class="params">(add)</span><span class="params">(Vector *C, Vector *A, Vector *B)</span></span>;</span><br></pre></td></tr></table></figure><h4 id="genericaddc"><a class="markdownIt-Anchor" href="#genericaddc"></a> generic/add.c</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// generic/add.c</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Vector_</span><span class="params">(add)</span><span class="params">(Vector *C, Vector *A, Vector *B)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i, n;</span><br><span class="line">n = C-&gt;n;</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">&#123;</span><br><span class="line">C-&gt;data[i] = A-&gt;data[i] + B-&gt;data[i];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="an-example-finding-thstorage"><a class="markdownIt-Anchor" href="#an-example-finding-thstorage"></a> An Example finding THStorage</h2><p>I try to find the definition of THStorage, since it will give us a brief understand of the file management structure of pytorch, and we can also grab a basic idea of how those macros and includes are forming this huge project. We start from <code>torch/csrc/Storage.cpp</code>, and check step by step to the file included.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Storage.cpp                 -&gt;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;TH/TH.h&gt;          -&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;TH/THStorageFunction.h&gt;   -&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;TH/generic/THStorage.h&gt;   -&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;c10/core/StorageImpl.h&gt;</span></span></span><br></pre></td></tr></table></figure><p>Find the macro definition in <code>TH/generic/THStorage.h</code>:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THStorage at::StorageImpl</span></span><br></pre></td></tr></table></figure><p>Find the structure definition in <code>c10/core/StorageImpl.h</code>:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> c10 &#123;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">StorageImpl</span> <span class="keyword">final</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  caffe2::TypeMeta  data_type_;  <span class="comment">// Data type</span></span><br><span class="line">  DataPtr data_ptr_;             <span class="comment">// Data pointer</span></span><br><span class="line">  <span class="keyword">int64_t</span> numel_;                <span class="comment">// Data number</span></span><br><span class="line">  <span class="keyword">bool</span> resizable_;</span><br><span class="line">  <span class="keyword">bool</span> received_cuda_;</span><br><span class="line">  Allocator* allocator_;         <span class="comment">// Data&#x27;s allocator</span></span><br><span class="line">&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Therefore, the hidding real tpye of <code>THWStorage</code> is <code>at::StorageImpl</code>, and it is the implementation of data storage. Let’s look into the definition of <code>THPStorage_(pynew)</code> at first, when the value of  <code>cdata</code> is not provided, it need to create an implementation of class <code>THWStorage</code> using function <code>THWStorage_(NAME)</code>,  and the value of NAME can possibly be:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span>                <span class="comment">// New a THStorage, if size not specified, size=0, that means using default Allocator</span></span><br><span class="line"><span class="built_in">free</span></span><br><span class="line">size</span><br><span class="line">get</span><br><span class="line"><span class="built_in">set</span></span><br><span class="line">data</span><br><span class="line">newWithSize        <span class="comment">// New THStorage，specify size but use default Allocator</span></span><br><span class="line">newWithAllocator   <span class="comment">// New THStorage，specify size and Allocator</span></span><br><span class="line">copy_functions</span><br><span class="line">copyByte</span><br><span class="line">...</span><br><span class="line">copyCudaByte</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>And also some macro definitions:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THWStorage_(NAME) THStorage_(NAME)     <span class="comment">// torch/csrc/THP.h</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THStorage_(NAME) TH_CONCAT_4(TH,Real,Storage_,NAME)   <span class="comment">// TH/THStorageFunctions.h</span></span></span><br></pre></td></tr></table></figure><p>The declaration of function <code>THStorage_(NAME)</code> lives in <code>TH/generic/THStorage.h</code>, <code>TH/generic/THStorageCopy.h</code> and the implementation part lies in corresponding cpp files.</p><p>(BTW, if using cuda, the declaration of  <code>#define THWStorage_(NAME) THCStorage_(NAME)</code>lie in <code>THC/generic/THCStorage.h</code> and <code>THC/generic/THCStorageCopy.h</code>)</p><p>Take THStorage_(newWithSize) function as an example, look into <code>TH/generic/THStorage.cpp</code> and we can find the definition:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">THStorage* <span class="title">THStorage_</span><span class="params">(newWithSize)</span><span class="params">(<span class="keyword">ptrdiff_t</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THStorage* storage = c10::make_instrusive&lt;at::StorageImpl&gt;(</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> THQUANTIZED</span></span><br><span class="line">    caffe2::TypeMeta::Make&lt;<span class="keyword">quantized_t</span>&gt;(),</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    caffe2::TypeMeta::Make&lt;<span class="keyword">scalar_t</span>&gt;(),        <span class="comment">// New a scalar_t type</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    size,</span><br><span class="line">    getTHDefaultAllocator(),</span><br><span class="line">    <span class="literal">true</span>).release();</span><br><span class="line">  <span class="keyword">return</span> storage;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It’s not hard to infer from this code block that it new an <code>StorageImpl</code>, and add an intrusive pointer pointing to one of them, at last return a pointer pointing to <code>StorageImpl</code> and destroy the intrusive pointer. Macro THStorage is <code>at::StorageImpl</code>, so this method simply new a <code>StorageImpl and return a pointer pointing to it. According to the definition of</code>c10::make_instrusive`, this work will actually be done by the constructor of StorageImpl’ and it is:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StorageImpl(</span><br><span class="line">    caffe2::TypeMeta data_type,</span><br><span class="line">    int64_4 numel,</span><br><span class="line">    at::Allocator* allocator,</span><br><span class="line">    <span class="keyword">bool</span> resizable)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>We will only traced here and this show a representative example of how pytorch inner code call and implement those method.</p><h2 id="autograd-2"><a class="markdownIt-Anchor" href="#autograd-2"></a> Autograd</h2><p>Autograd is a method which support automatic computation of gradient which will be used in the back propagation. Autograd depend directly on the computational graph. Computational graph is used for defining the pipeline of a model. It combines functions with variables and shows how they connect to each other.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128215249350.png" alt="image-20191128215249350"></p><p>A directed graph with the following property:</p><ol><li>Edge: a function, or a function’s dependency</li><li>Points with input edges: a function (or operator)</li><li>Points with output edges: a variable</li></ol><p>Computational graph has two major types, they are dynamic and static computational graphs. TensorFlow applies static graph, it has the following characteristics:</p><ul><li>First define the structure of the graph, and then assign values to the leaf nodes (this is the origin of placeholder)</li><li>Then forward according to the assignment of leaf nodes</li></ul><p>Pytorch, on the other hand, utilize dynamic graph. The structure of the graph is established at the same time as the forward, so there is no need to use placeholder.</p><p>Here is an example inner code of autograd.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128215538638.png" alt="image-20191128215538638"></p><p>Here we will elaborate these parameters which get involved in this process.</p><ul><li><p><strong>Data</strong>: It’s the data a variable is holding.</p></li><li><p><strong>requires_grad</strong>: This member, if true starts tracking all the operation history and forms a backward graph for gradient calculation.</p></li><li><p><strong>grad:</strong> grad holds the value of gradient. If requires_grad is False it will hold a None value. Even if requires_grad is True, it will hold a None value unless .backward() function is called from some other node.</p></li><li><p><strong>grad_fn:</strong> This is the backward function used to calculate the gradient.</p></li><li><p><strong>is_leaf</strong>: A node is leaf if :</p><ol><li><p>It was initialized explicitly by some function like x = torch.tensor(1.0) or x = torch.randn(1, 1) (basically all the tensor initializing methods discussed at the beginning of this post).</p></li><li><p>It is created after operations on tensors which all have requires_grad = False.</p></li><li><p>It is created by calling .detach() method on some tensor.</p></li></ol></li></ul><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128215740348.png" alt="image-20191128215740348"></p><h2 id="pytorch-source-code-composition"><a class="markdownIt-Anchor" href="#pytorch-source-code-composition"></a> Pytorch Source Code Composition</h2><p>Since different data type, different devices are supported, and python code call C/C++ based code, the source code structure is not easy to understand. Here is the most important parts in the root directory.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128193909092.png" alt="image-20191128193909092"></p><p>And provide a more detailed directory comment as well as explanation below.</p><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128194349295.png" alt="image-20191128194349295"></p><h3 id="explanation-of-crucial-folders"><a class="markdownIt-Anchor" href="#explanation-of-crucial-folders"></a> Explanation of crucial folders</h3><h4 id="c10"><a class="markdownIt-Anchor" href="#c10"></a> C10</h4><p><strong>C</strong>affe <strong>Ten</strong>sor Library: Most basic tensor library. Codes here can be deployed to mobile devices as well as servers. It contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</p><h4 id="aten"><a class="markdownIt-Anchor" href="#aten"></a> ATen</h4><p><strong>A</strong> <strong>TEN</strong>sor library for C<ins>11, the C</ins> tensor library for Pytorch. It is a C++ library that implements the <strong>operations</strong> of Tensors. If you’re looking for where some kernel code lives, chances are it’s in ATen. ATen itself bifurcates into two neighborhoods of operators: the “native” operators, which are modern, C++ implementations of operators, and the “legacy” operators (TH, THC, THNN, THCUNN), which are legacy, C implementations. The legacy operators are the bad part of town; try not to spend too much time there if you can.</p><h4 id="caffe2"><a class="markdownIt-Anchor" href="#caffe2"></a> Caffe2</h4><p>This part is from the original Caffe2. After the merge of Pytorch and Caffe2, Caffe2 become a kind of backend in Pytorch.</p><h4 id="torch"><a class="markdownIt-Anchor" href="#torch"></a> Torch</h4><p>This is the part normally called by user when then use Pytorch to train or test their models. It contains what you are most familiar with: the actual Python modules that you import and use.</p><h4 id="torchcsrc"><a class="markdownIt-Anchor" href="#torchcsrc"></a> Torch/csrc</h4><p>The C++ code that implements what you might call the frontend of PyTorch. In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler. It also contains the C++ frontend code.</p><h3 id="mechanism-inside-a-simple-call"><a class="markdownIt-Anchor" href="#mechanism-inside-a-simple-call"></a> Mechanism inside a simple call</h3><p><img src="/2019/12/11/Pytorch-Core-Code-Research/image-20191128223910466.png" alt="image-20191128223910466"></p><h2 id="basic-condition-of-memory-management-in-pytorch"><a class="markdownIt-Anchor" href="#basic-condition-of-memory-management-in-pytorch"></a> Basic Condition of Memory Management in Pytorch</h2><ol><li>Every tensor will be assigned with a allocator when it is initialized.</li><li><code>c10/core/Allocator.h</code>: Pytorch default allocator class defined here.</li></ol><p>Some Policy in <code>c10/core/Allocator.h</code>:</p><ul><li><p>A DataPtr is a unique pointer (with an attached deleter and some context for the deleter) to some memory, which also records what device is for its data. nullptr DataPtrs can still have a nontrivial device; this allows us to treat zero-size allocations uniformly with non-zero allocations.</p></li><li><p>Choice of CPU here is arbitrary; if there’s an “undefined” device, we could use that too.</p></li><li><p>The deleter can be changed while running using function <code>compare_exchange_deleter</code>.</p></li><li><p>This context is used to generate DataPtr which have arbitrary <code>std::function</code> deleters associated with them.  In some user facing functions, we give a (user-friendly) interface for constructing tensors from external data which take an arbitrary <code>std::function</code> deleter.  Grep for InefficientStdFunctionContext to find these occurrences.</p><p>This context is inefficient because we have to do a dynamic allocation <code>InefficientStdFunctionContext</code>, on top of the dynamic allocation which is implied by <code>std::function</code> itself.</p></li></ul><ol start="3"><li>There is a fake allocator in Aten(<code>aten/src/ATen/CPUFixedAllocator.h</code>), which just throws exceptions if some cpu fixed operation is actually used, like <code>cpu_fixed_malloc</code>, <code>cpu_fixed_realloc</code>, <code>cpu_fixed_free</code>.</li><li><code>c10/core/CPUAllocator.cpp</code> contains functions: <code>alloc_cpu</code>, <code>free_cpu</code>, <code>memset_junk</code>,  <code>alloc_cpu</code> even has the code dealing with NUMA machine. And there is a class <code>MemoryAllocationReporter</code> which is used to report C10’s memory allocation and deallocation status.</li><li><code>c10/core/Allocator.cpp</code>: Set and get allocator for different device type.</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DeviceType::CPU</span><br><span class="line">DeviceType::CUDA</span><br><span class="line">DeviceType::OPENGL</span><br><span class="line">DeviceType::OPENCL</span><br><span class="line">DeviceType::MKLDNN</span><br><span class="line">DeviceType::IDEEP</span><br><span class="line">DeviceType::HIP</span><br><span class="line">DeviceType::FPGA</span><br><span class="line">DeviceType::MSNPU</span><br><span class="line">DeviceType::XLA</span><br></pre></td></tr></table></figure><ol start="6"><li><p><code>c10/core/StorageImpl.h</code> &amp; <code>c10/core/Storage.h</code>: Mainly allocates memory buffer using given allocator and creates a storage with it. Mark.</p></li><li><p><code>c10/cuda/CUDACachingAllocator.cpp</code> is a caching allocator for CUDA. It has the following description:</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Yet another caching allocator for CUDA device allocations.</span><br><span class="line"></span><br><span class="line">- Allocations are associated with a stream. Once freed, blocks can be</span><br><span class="line">  re-allocated on the same stream, but not on any other stream.</span><br><span class="line">- The allocator attempts to find the smallest cached block that will fit the</span><br><span class="line">  requested size. If the block is larger than the requested size, it may be</span><br><span class="line">  split. If no block is found, the allocator will delegate to cudaMalloc.</span><br><span class="line">- If the cudaMalloc fails, the allocator will free all cached blocks that</span><br><span class="line">  are not split and retry the allocation.</span><br><span class="line">- Large (&gt;1MB) and small allocations are stored in separate pools.</span><br><span class="line">  Small requests are packed into 2MB buffers. Large requests will use the</span><br><span class="line">  smallest available free block or allocate a new block using cudaMalloc.</span><br><span class="line">  To reduce fragmentation, requests between 1MB and 10MB will allocate and</span><br><span class="line">  split a 20MB block, if no free block of sufficient size is available.</span><br><span class="line"></span><br><span class="line">With this allocator, allocations and frees should logically be considered</span><br><span class="line">&quot;usages&quot; of the memory segment associated with streams, just like kernel</span><br><span class="line">launches. The programmer must insert the proper synchronization if memory</span><br><span class="line">segments are used from multiple streams.</span><br><span class="line"></span><br><span class="line">The library provides a recordStream() function to help insert the correct</span><br><span class="line">synchronization when allocations are used on multiple streams. This will</span><br><span class="line">ensure that the block is not reused before each recorded stream completes</span><br><span class="line">work.</span><br></pre></td></tr></table></figure><h2 id="how-python-interact-with-cc"><a class="markdownIt-Anchor" href="#how-python-interact-with-cc"></a> How Python interact with C/C++</h2><h3 id="compile-c-program-to-so-library-and-call-it-in-python"><a class="markdownIt-Anchor" href="#compile-c-program-to-so-library-and-call-it-in-python"></a> Compile C program to .so library and call it in python</h3><h4 id="compile-as-shared-library"><a class="markdownIt-Anchor" href="#compile-as-shared-library"></a> Compile as shared library</h4><ol><li>Finish writing your C code.</li><li>Compile it into a <code>*.so</code> file.</li><li>Import <code>ctypes</code> in python file.</li><li>Load <code>*.so</code> file inside a python file.</li><li>*Define the input type of a C function.</li><li>Call function inside the <code>*.so</code> file.</li></ol><p><strong>function.c</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">myFunction</span><span class="params">(<span class="keyword">int</span> num)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (num == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="keyword">if</span> ((num &amp; (num - <span class="number">1</span>)) == <span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Compile</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -fPIC -shared -o libfun.so function.c</span><br></pre></td></tr></table></figure><p><strong><a href="http://function.py">function.py</a></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ctypes </span><br><span class="line">NUM = <span class="number">16</span>      </span><br><span class="line">fun = ctypes.CDLL(libfun.so)   </span><br><span class="line">fun.myFunction.argtypes=[ctypes.c_int] </span><br><span class="line">returnVale = fun.myFunction(NUM)     </span><br></pre></td></tr></table></figure><h4 id="add-wrapper-in-c-file"><a class="markdownIt-Anchor" href="#add-wrapper-in-c-file"></a> Add wrapper in C++ file</h4><p>If this is a C++ file, you need to expose the function you want to use in a <code>extern &quot;C&quot;</code> wrapper.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>&#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Hello&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// Since ctypes can only talk to C functions, you need </span></span><br><span class="line"><span class="comment">// to provide those declaring them as extern &quot;C&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> &#123;</span><br><span class="line">    <span class="function">Foo* <span class="title">Foo_new</span><span class="params">()</span></span>&#123; <span class="keyword">return</span> <span class="keyword">new</span> Foo(); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Foo_bar</span><span class="params">(Foo* foo)</span></span>&#123; foo-&gt;bar(); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And then compile:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g++ -c -fPIC foo.cpp -o foo.o</span><br><span class="line">g++ -shared -Wl,-install_name,libfoo.so -o libfoo.so  foo.o</span><br></pre></td></tr></table></figure><p>Afterwards, thing in Python code are similar as those in C.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> cdll</span><br><span class="line">lib = cdll.LoadLibrary(<span class="string">&#x27;./libfoo.so&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.obj = lib.Foo_new()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span>(<span class="params">self</span>):</span></span><br><span class="line">        lib.Foo_bar(self.obj)</span><br><span class="line"><span class="comment"># Once you have that you can call it like</span></span><br><span class="line"></span><br><span class="line">f = Foo()</span><br><span class="line">f.bar() <span class="comment">#and you will see &quot;Hello&quot; on the screen</span></span><br></pre></td></tr></table></figure><h3 id="c-file-include-module-and-expose"><a class="markdownIt-Anchor" href="#c-file-include-module-and-expose"></a> C++ file include module and Expose</h3><p>Include &lt;boost/python.hpp&gt; the function in BOOST_PYTHON_MODULE</p><p>A C++ Function can be exposed to Python by writing a Boost.Python wrapper:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">char</span> <span class="keyword">const</span>* <span class="title">greet</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="string">&quot;hello, world&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BOOST_PYTHON_MODULE(hello_ext)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> boost::python;</span><br><span class="line">    def(<span class="string">&quot;greet&quot;</span>, greet);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>That’s it. We’re done. We can now build this as a shared library. The resulting DLL is now visible to Python. Here’s a sample Python session:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> hello_ext</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> hello_ext.greet()</span><br><span class="line">hello, world</span><br></pre></td></tr></table></figure><h2 id="integrating-a-ccuda-operation-with-pytorch"><a class="markdownIt-Anchor" href="#integrating-a-ccuda-operation-with-pytorch"></a> Integrating a C++/CUDA Operation with PyTorch</h2><p>When we want to build a customized method or module, we can choose whether to build it in python or C++. The former is easier but the C++ version is faster and more efficient, especially when we want to build a frequently used or time consuming module. Here comes the explanation.</p><h4 id="cpu-integration"><a class="markdownIt-Anchor" href="#cpu-integration"></a> CPU Integration</h4><p>Besides integrate C++ file in python and use it in Pytorch, Pytorch itself provides us with two quite straightforward way to finish this job. They are Building with <code>setuptools</code> and JIT Compiling Extensions.</p><p>For the “ahead of time” flavor, we build our C++ extension by writing a <code>setup.py</code> script that uses setuptools to compile our C++ code. For the LLTM, it looks as simple as this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup, Extension</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">&#x27;lltm_cpp&#x27;</span>,</span><br><span class="line">      ext_modules=[cpp_extension.CppExtension(<span class="string">&#x27;lltm_cpp&#x27;</span>, [<span class="string">&#x27;lltm.cpp&#x27;</span>])],</span><br><span class="line">      cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure><p>The JIT compilation mechanism provides you with a way of compiling and loading your extensions on the fly by calling a simple function in PyTorch’s API called <code>torch.utils.cpp_extension.load()</code>. For the LLTM, this would look as simple as this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">lltm_cpp = load(name=<span class="string">&quot;lltm_cpp&quot;</span>, sources=[<span class="string">&quot;lltm.cpp&quot;</span>])</span><br></pre></td></tr></table></figure><h4 id="cuda-integration"><a class="markdownIt-Anchor" href="#cuda-integration"></a> CUDA Integration</h4><p>Integration of our CUDA-enabled op with PyTorch is again very straightforward. If you want to write a <code>setup.py</code> script, it could look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;lltm&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(<span class="string">&#x27;lltm_cuda&#x27;</span>, [</span><br><span class="line">            <span class="string">&#x27;lltm_cuda.cpp&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;lltm_cuda_kernel.cu&#x27;</span>,</span><br><span class="line">        ])</span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><p>Instead of <code>CppExtension()</code>, we now use <code>CUDAExtension()</code>. We can just specify the <code>.cu</code> file along with the <code>.cpp</code> files – the library takes care of all the hassle this entails for you. The JIT mechanism is even simpler:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load</span><br><span class="line"></span><br><span class="line">lltm = load(name=<span class="string">&#x27;lltm&#x27;</span>, sources=[<span class="string">&#x27;lltm_cuda.cpp&#x27;</span>, <span class="string">&#x27;lltm_cuda_kernel.cu&#x27;</span>])</span><br></pre></td></tr></table></figure><h2 id="conclusion"><a class="markdownIt-Anchor" href="#conclusion"></a> Conclusion</h2><ul><li>Pytorch’s python part doesn’t have special care on memory management, means it just works in the way standard python programs work.</li><li>Current Pytorch source codes contains codes from multiple source, some of them are pure legacy, some come from caffe2, some serves as basic code, some are packed into dlls to serve python. Also, codes are different for those in CPU and CUDA, we need to focus on the right part if any optimization want to be made.</li><li>Almost all Pytorch core modules and functions are implemented in C++ based code and that will be much more efficient.</li><li>Every tensor is attached with a memory allocator, which can not only do the work of allocate and free, but also record the device on which it is located. Different kinds of allocator for different data type can be delivered as input parameter, this makes the code more compatible.</li><li>Pytorch combines multiple code dispatch method and they work well for C and C++ code.</li><li>Python can call compiled C file using ctypes, but Pytorch provides a toolset which makes it even easier.</li></ul><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ul><li><a href="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals</a></li><li><a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">PyTorch Autograd</a></li><li><a href="https://pytorch.org/docs/stable/index.html">PYTORCH DOCUMENTATION</a></li><li><a href="https://zhuanlan.zhihu.com/p/34629243">PyTorch源码浅析</a></li><li><a href="https://github.com/pytorch/pytorch">Pytorch GitHub Repo</a></li></ul><h2 id="slides"><a class="markdownIt-Anchor" href="#slides"></a> Slides</h2><p>![Final Report_1.jpg](Final Report_1.jpg)</p><p>![Final Report_1.jpg](Final Report_2.jpg)</p><p>![Final Report_1.jpg](Final Report_3.jpg)</p><p>![Final Report_1.jpg](Final Report_4.jpg)</p><p>![Final Report_1.jpg](Final Report_5.jpg)</p><p>![Final Report_1.jpg](Final Report_6.jpg)</p><p>![Final Report_1.jpg](Final Report_7.jpg)</p><p>![Final Report_1.jpg](Final Report_8.jpg)</p><p>![Final Report_1.jpg](Final Report_9.jpg)</p><p>![Final Report_1.jpg](Final Report_10.jpg)</p><p>![Final Report_1.jpg](Final Report_11.jpg)</p><p>![Final Report_1.jpg](Final Report_12.jpg)</p><p>![Final Report_1.jpg](Final Report_13.jpg)</p><p>![Final Report_1.jpg](Final Report_14.jpg)</p><p>![Final Report_1.jpg](Final Report_15.jpg)</p><p>![Final Report_1.jpg](Final Report_16.jpg)</p><p>![Final Report_1.jpg](Final Report_17.jpg)</p><p>![Final Report_1.jpg](Final Report_18.jpg)</p><p>![Final Report_1.jpg](Final Report_19.jpg)</p><p>![Final Report_1.jpg](Final Report_20.jpg)</p><p>![Final Report_1.jpg](Final Report_21.jpg)</p><p>![Final Report_1.jpg](Final Report_22.jpg)</p><p>![Final Report_1.jpg](Final Report_23.jpg)</p><p><strong>Zhongyang Zhang</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;pytorch-release-version-composition&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#pytorch-release-version-composition&quot;&gt;&lt;/a&gt; Pytorch Release V</summary>
      
    
    
    
    
    <category term="machine learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="Pytorch" scheme="https://www.miracleyoo.com/tags/Pytorch/"/>
    
    <category term="C++" scheme="https://www.miracleyoo.com/tags/C/"/>
    
    <category term="C" scheme="https://www.miracleyoo.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Video Classification Investigation Report</title>
    <link href="https://www.miracleyoo.com/2019/11/07/video-classification/"/>
    <id>https://www.miracleyoo.com/2019/11/07/video-classification/</id>
    <published>2019-11-07T18:00:06.000Z</published>
    <updated>2019-11-07T18:00:06.350Z</updated>
    
    <content type="html"><![CDATA[<h1 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h1><p>Video classification, or in our case, more specifically, action recognition, are studied for a long time. There are many traditional as well as deep learning based method developed to address this problem, and the latest action recognition result trained on a large dataset Kinetics can even reach 98% accuracy. Considering the fact that the action we need to classify is not too much, giving enough data and using the pre-trained model on Kinetics, the result can be quite promising.</p><h1 id="tough-points-in-video-classification"><a class="markdownIt-Anchor" href="#tough-points-in-video-classification"></a> Tough Points in Video Classification</h1><ol><li>The huge computational cost</li><li>How to capture long context and make decision comprehensively</li><li>How to design the classification structure which contain spatiotemporal information</li><li>How to deal with a smaller dataset</li></ol><h1 id="approaches-overview"><a class="markdownIt-Anchor" href="#approaches-overview"></a> Approaches overview</h1><h2 id="the-core-idea"><a class="markdownIt-Anchor" href="#the-core-idea"></a> The core idea</h2><ol><li>Try to build a workflow which can combine both spatial information and temporal information.</li><li>Try to focus on both frame itself and the motion near each frame.</li><li>Try to make decision based on the whole video rather than only parts of it.</li><li>Try to decrease the computational cost and remove the long pre-process.</li></ol><h2 id="two-basic-methods"><a class="markdownIt-Anchor" href="#two-basic-methods"></a> Two basic methods</h2><h3 id="single-stream-network"><a class="markdownIt-Anchor" href="#single-stream-network"></a> <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf">Single Stream Network</a></h3><p><img src="/2019/11/07/video-classification/image-20191028125241454.png" alt="image-20191028125241454"></p><p>There are four ways of fusion, which means combine the information from each frame together to derive the final answer. They are:</p><ol><li>Single frame uses single architecture that fuses information from all frames at the last stage.</li><li>Late fusion uses two nets with shared parameters, spaced 15 frames apart, and also combines predictions at the end.</li><li>Early fusion combines in the first layer by convolving over 10 frames.</li><li>Slow fusion involves fusing at multiple stages, a balance between early and late fusion.</li></ol><h3 id="two-stream-networks"><a class="markdownIt-Anchor" href="#two-stream-networks"></a> <a href="https://arxiv.org/pdf/1406.2199.pdf">Two Stream Networks</a></h3><p><img src="/2019/11/07/video-classification/image-20191028125608889.png" alt="image-20191028125608889"></p><p>Video can naturally be decomposed into spatial and temporal components.</p><ol><li>The spatial part, in the form of individual frame appearance, carries information about scenes and objects depicted in the video.</li><li>The temporal part, in the form of motion across the frames, conveys the movement of the observer (the camera) and the objects. In fact, the essence of “motion” is <a href="https://en.wikipedia.org/wiki/Optical_flow">optical flow</a>.</li></ol><h1 id="improvement-of-methods"><a class="markdownIt-Anchor" href="#improvement-of-methods"></a> Improvement of methods</h1><p>Firstly I’d like to show a graph which shows an overview of all previous action classification architectures drawn in the paper <a href="https://arxiv.org/abs/1705.07750">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a>.</p><p><img src="/2019/11/07/video-classification/image-20191028130233266.png" alt="image-20191028130233266"></p><p>To summarize, there are these kinds of improved methods:</p><ol><li><p><a href="https://arxiv.org/abs/1411.4389">LRCN</a>: Long-term Recurrent Convolutional Networks for Visual Recognition and Description</p><p><img src="/2019/11/07/video-classification/GenericLRCN_high.png" alt="2 stream architecture"></p><p>Send each frame to a CNN at first and then uses the features extracted as the input of LSTM.</p></li><li><p><a href="https://arxiv.org/pdf/1412.0767">C3D</a>: Learning Spatiotemporal Features with 3D Convolutional Networks</p><p><img src="/2019/11/07/video-classification/c3d_high-1572285955778.png" alt="SegNet Architecture"></p><p>The first time using 3D Conv to process frames.</p></li><li><p><a href="https://arxiv.org/abs/1502.08029">Conv3D &amp; Attention</a>: Describing Videos by Exploiting Temporal Structure</p><p><img src="/2019/11/07/video-classification/Larochelle_paper_high.png" alt="Attention Mechanism"></p><p>Add a attention mask before send the CNN-extracted feature into LSTM.</p></li><li><p><a href="https://arxiv.org/abs/1604.06573">TwoStreamFusion</a>: Convolutional Two-Stream Network Fusion for Video Action Recognition</p><p><img src="/2019/11/07/video-classification/fusion_strategies_high.png" alt="SegNet Architecture"></p><p>Fuse two stream in a smarter way and get a better result.</p></li><li><p><a href="https://arxiv.org/abs/1608.00859">TSN</a> :Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</p><p><img src="/2019/11/07/video-classification/tsn_high.png" alt="SegNet Architecture"></p><p>Select video snippets not completely randomly, but divide the video into k equal-length parts and choose a snippets randomly from each division.</p></li><li><p><a href="https://arxiv.org/pdf/1704.02895.pdf">ActionVlad</a>:ActionVLAD: Learning spatio-temporal aggregation for action classification</p><p><img src="/2019/11/07/video-classification/actionvlad-1572285243641.png" alt="SegNet Architecture"></p><p>In this work, the most notable contribution by the authors is the usage of learnable feature aggregation (VLAD) as compared to normal aggregation using maxpool or avgpool.</p></li><li><p><a href="https://arxiv.org/abs/1704.00389">HiddenTwoStream</a>:Hidden Two-Stream Convolutional Networks for Action Recognition</p><p><img src="/2019/11/07/video-classification/image-20191028134438690.png" alt="image-20191028134438690"></p><p>It uses a “MotionNet” to take the place of optical flow.</p></li><li><p><a href="https://arxiv.org/abs/1705.07750">I3D</a>: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</p><p>Mainly used pretrained network by ImageNet and Kinetics dataset. Also, it use different 3D network for images and optical flows.</p></li><li><p><a href="https://arxiv.org/abs/1711.08200">T3D</a>: Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification</p><p><img src="/2019/11/07/video-classification/ttl_layer_high.png" alt="SegNet Architecture"></p><p>Transfer a 2-D DenseNet to a 3D one.</p></li></ol><h1 id="result-comparation"><a class="markdownIt-Anchor" href="#result-comparation"></a> Result comparation</h1><p><img src="/2019/11/07/video-classification/image-20191028133828231.png" alt="image-20191028133828231"></p><p><img src="/2019/11/07/video-classification/image-20191028133226300.png" alt="image-20191028133226300"></p><h1 id="current-thought"><a class="markdownIt-Anchor" href="#current-thought"></a> Current Thought</h1><p>As we can see from the analysis above, the I3D is the most computational efficient and accurate method. Also, the pre-trained model of I3D is provided by the author, so we can also take advantage of it. Now I think we should collect enough data of the corresponding action. Moreover, I noticed that there are many new method on Temporal Action Proposals, Temporal Action Localization and Dense-Captioning Events in Videos appearing this year in the competition ActivityNet, I may research into it to get better result later.</p><h1 id="datasets"><a class="markdownIt-Anchor" href="#datasets"></a> Datasets</h1><ul><li><a href="https://www.crcv.ucf.edu/data/UCF101.php">UCF101</a></li><li><a href="https://cs.stanford.edu/people/karpathy/deepvideo/">Sports-1M</a></li><li><a href="https://deepmind.com/research/open-source/kinetics">Kinetics</a></li><li><a href="http://activity-net.org/download.html">ActivityNet Version 1.3 dataset</a></li></ul><h1 id="codes"><a class="markdownIt-Anchor" href="#codes"></a> Codes</h1><ul><li><a href="https://github.com/hassony2/kinetics_i3d_pytorch">I3D models transfered from Tensorflow to PyTorch</a></li><li><a href="https://github.com/deepmind/kinetics-i3d">I3D models trained on Kinetics</a></li><li><a href="https://github.com/kenshohara/video-classification-3d-cnn-pytorch">Video Classification Using 3D ResNet</a></li></ul><h1 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h1><ul><li><a href="http://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review">Deep Learning for Videos: A 2018 Guide to Action Recognition</a></li><li><a href="https://arxiv.org/abs/1705.07750">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a></li><li><a href="https://github.com/jinwchoi/awesome-action-recognition">Awesome Action Recognition</a></li><li><a href="http://activity-net.org/index.html">ActivityNet</a></li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/0004370281900242">Determining optical flow</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;overview&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#overview&quot;&gt;&lt;/a&gt; Overview&lt;/h1&gt;
&lt;p&gt;Video classification, or in our case, more specificall</summary>
      
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="cv" scheme="https://www.miracleyoo.com/tags/cv/"/>
    
    <category term="video-classification" scheme="https://www.miracleyoo.com/tags/video-classification/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading ： &quot;NOSE： A Novel Odor Sensing Engine for Ambient Monitoring of the Frying Cooking Method in Kitchen Environments&quot;</title>
    <link href="https://www.miracleyoo.com/2019/10/24/paper-rev-nose/"/>
    <id>https://www.miracleyoo.com/2019/10/24/paper-rev-nose/</id>
    <published>2019-10-24T23:40:02.000Z</published>
    <updated>2019-10-25T15:10:18.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="one-line-summary"><a class="markdownIt-Anchor" href="#one-line-summary"></a> One Line Summary</h1><p>NOSE: A device which utilize order sensing component and machine learning to detect which kind of cooking method and which kind of foods, oils are used when you are cooking. It can be used to periodically reports to users about their cooking habits.</p><h1 id="terms"><a class="markdownIt-Anchor" href="#terms"></a> Terms</h1><ol><li>MOS Gas Sensor: A sensor which is sensitive to specific target analytes and attempts to replicate the human olfactory system by detecting various types of odors.</li></ol><h1 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h1><ol><li>In the real-world environment, we cannot get the data with a start and end well defined. Here the author exploited a two-level classification approach.</li></ol><p><img src="/2019/10/24/paper-rev-nose/image-20191024193802569.png" alt="image-20191024193802569"></p><h1 id="images"><a class="markdownIt-Anchor" href="#images"></a> Images</h1><p><img src="/2019/10/24/paper-rev-nose/image-20191021152129125.png" alt="image-20191021152129125"></p><p><img src="/2019/10/24/paper-rev-nose/image-20191021154348447.png" alt="image-20191021154348447"></p><h1 id="questions"><a class="markdownIt-Anchor" href="#questions"></a> Questions</h1><ol><li>We can use multiple dimensional information to detect what is going on. For example, if we add sound detect devices or infrared sensor and use their signals to do analysis at the same time, the accuracy may get dramatically improved. If camera can also be applied, even more detailed information can be obtained.</li><li>Regarding the privacy issue, perhaps we can consider using a embedded auto-clip algorithm which make it possible to only output a limited region which only contains the main region of the Target-of-Interest.</li></ol><p><img src="/2019/10/24/paper-rev-nose/image-20191024193841882.png" alt="image-20191024193841882"></p><ol start="3"><li>We can even combine different sensor and use one as the trigger of another. For example, camera will only work when the odor sensor feels that there is someone cooking or when infrared sensor feels that someone is approaching or the microphone hears the noise of cooking.</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;one-line-summary&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#one-line-summary&quot;&gt;&lt;/a&gt; One Line Summary&lt;/h1&gt;
&lt;p&gt;NOSE: A device which utilize o</summary>
      
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="HCI" scheme="https://www.miracleyoo.com/tags/HCI/"/>
    
    <category term="mobile-health" scheme="https://www.miracleyoo.com/tags/mobile-health/"/>
    
  </entry>
  
  <entry>
    <title>Plan of Project Tomasulo Visual</title>
    <link href="https://www.miracleyoo.com/2019/10/22/tomasulo/"/>
    <id>https://www.miracleyoo.com/2019/10/22/tomasulo/</id>
    <published>2019-10-22T22:36:58.000Z</published>
    <updated>2019-10-22T22:40:12.460Z</updated>
    
    <content type="html"><![CDATA[<h1 id="aims"><a class="markdownIt-Anchor" href="#aims"></a> Aims</h1><ol><li>Build a project to visualize the workflow of Tomasulo’s algorithm.</li><li>The project should contain at least these parts: Cycle graph, Register info, Pipeline, Data info, Code info and statistics.</li><li>We can add some additional components.</li></ol><h1 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h1><ol><li>Language: Java</li><li>GUI Support: Swing</li><li>Collaboration Platform: <a href="https://github.com/miracleyoo/Tomasulo-Visual">GitHub</a></li><li>Divide pattern: By components. Each one 2 components.</li><li>DDL: Next week(Oct 29)</li><li>Algorithm Reference: <a href="https://youtu.be/jyjE6NHtkiA">Youtube</a></li></ol><h1 id="reference-images"><a class="markdownIt-Anchor" href="#reference-images"></a> Reference Images</h1><p>![WinMips64](Plan de Project Tomasulo Visual/image-20191022180533866.png)</p><p>![Slides](Plan de Project Tomasulo Visual/image-20191022180615852.png)</p><p>![Youtube Course](Plan de Project Tomasulo Visual/image-20191022180646659.png)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;aims&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#aims&quot;&gt;&lt;/a&gt; Aims&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Build a project to visualize the workflow of Tomasulo’s algo</summary>
      
    
    
    
    
    <category term="computer-architecture" scheme="https://www.miracleyoo.com/tags/computer-architecture/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading： &quot;W!NCE： Unobtrusive Sensing of Upper Facial Action Units with EOG-based Eyewear&quot;</title>
    <link href="https://www.miracleyoo.com/2019/10/20/paper-rev-wnce/"/>
    <id>https://www.miracleyoo.com/2019/10/20/paper-rev-wnce/</id>
    <published>2019-10-20T23:07:48.000Z</published>
    <updated>2019-10-20T23:17:04.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="one-line-summary"><a class="markdownIt-Anchor" href="#one-line-summary"></a> One Line Summary</h2><p>W!NCW developes a two-stage processing pipeline which can do continuously and unobtrusively sensing of upper facial action units with high fidelity. Because it doesn’t use camera so it also eliminate the privacy concerns.</p><h2 id="terms"><a class="markdownIt-Anchor" href="#terms"></a> Terms</h2><ol><li>Electrooculography(EOG, 眼球电图检查): A technique for measuring the corneo-retinal standing potential that exists between the front and the back of the human eye. The resulting signal is called the electrooculogram. Primary applications are in ophthalmological diagnosis and in recording eye movements.</li><li>Motion artifacts removal pipeline: Mainly used to remove noise across multiple EOF channels and many different head movement patterns. It is based on neural network.</li><li></li></ol><h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ol><li>There are already standard Facial Action Coding System(FACS) along with camera based methods which can be applied to check facial expressions, but their positioning is awkward and they may bring in privacy problems.</li><li>The hardware is not a lab-product, rather, it is based on commercially available comfortable daily eyeware device J!NS MEME.</li><li>EOG metrics is useful for recognizing different types of activities such as reading and writing. since each activity has its own unique eye movement pattern.</li><li>The EOG sensors are placed on the nose and the IMU sensor is embedded in the temples of the eyeglass.</li><li>W!NCE takes the body motion into consideration, while existing work work in motion artifact removal from physiological signals couldn’t do so.</li><li>The lower face action is harder to be detected because the signal are generated  in distant muscles, so it will be damped when reaches the sensor.</li><li>J!NS MEME employs stainless steel eletrodes which belong to the stiff material dry eletrodes. It has a lower price and a good electrical performance and lower possiblility of skin irritation compared to gel-based ones.</li><li>Some actions of heads will cast a similar influence on EOG sensors(like nod and lower eyebrows), while the IMU signals will be quite different, which can help confirm the real action.</li><li>We have to consider the signal variation across individuals, since the face shape, the shape of nose-bridge, the fit of the glasses behind the ear, the variation in the way individuals use upper facial muscles influence the signals captured greatly.</li><li>Personalizing with transfer learning is utilized to address the problem above. The device will take some labled data from user when they use it for the first time, and only re-train the last layer(full-connection layer).</li><li>The CNN model will not always be in the working state. In fact, the model process will only be triggered when substantial EOG activities are detected after the motion artifact removal stage. Also, the motion artifact removal model will only run when significant variation is observed in the raw EOG signal.</li></ol><h2 id="question"><a class="markdownIt-Anchor" href="#question"></a> Question</h2><ol><li>What if user sweet on there nose? Will it affect the accuracy of EOG sensor?</li><li>This eye-glass based design will be easy to accommodate for those who always wearing a glass, but to the others who don’t have the habit, it might be difficult.</li><li>For the CNN and motion artifact removal trigger, for the emotions or movement which only generate minor signal, like the lower face action, will it be detected?</li><li>How to know whether the prediction is right or not? User may express multiple emotion and movement at the same time. Same question when dataset is collected.</li><li>Why people will need, or need to buy this product? Will a normal person have the requisition to know their facial action and emotion all day? If so, what can the data collected derive?</li></ol><h2 id="images"><a class="markdownIt-Anchor" href="#images"></a> Images</h2><p><img src="/2019/10/20/paper-rev-wnce/image-20191020171322278.png" alt="image-20191020171322278"></p><p><img src="/2019/10/20/paper-rev-wnce/image-20191020172902493.png" alt="image-20191020172902493"></p><p><img src="/2019/10/20/paper-rev-wnce/image-20191020190355816.png" alt="image-20191020190355816"></p><p><img src="/2019/10/20/paper-rev-wnce/image-20191020173940080.png" alt="image-20191020173940080"></p><p><img src="/2019/10/20/paper-rev-wnce/image-20191020183735447.png" alt="image-20191020183735447"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;one-line-summary&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#one-line-summary&quot;&gt;&lt;/a&gt; One Line Summary&lt;/h2&gt;
&lt;p&gt;W!NCW developes a two-stage pr</summary>
      
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="HCI" scheme="https://www.miracleyoo.com/tags/HCI/"/>
    
    <category term="mobile-health" scheme="https://www.miracleyoo.com/tags/mobile-health/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading： &quot;wPerf： Generic Off-CPU Analysis to Identify Bottleneck Waiting Events&quot;</title>
    <link href="https://www.miracleyoo.com/2019/10/19/paper-rev-wperf/"/>
    <id>https://www.miracleyoo.com/2019/10/19/paper-rev-wperf/</id>
    <published>2019-10-20T01:40:57.000Z</published>
    <updated>2019-10-20T23:15:20.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="one-line-summary"><a class="markdownIt-Anchor" href="#one-line-summary"></a> One Line Summary</h2><p>Some waiting events can cast impact to multiple threads. A method which can computes not only the local impact of a waiting event, but also whether such impact can indirectly reach other threads is developed.</p><h2 id="important-terms"><a class="markdownIt-Anchor" href="#important-terms"></a> Important terms</h2><ol><li>On-CPU analysis: Used to identify bottlenecks created by execution.</li><li>Off-CPU analysis: Used to identify bottlenecks created by waiting.</li><li>False wakeup: A phenomenon that a thread is woken up but finds its condi- tion to continue is not satisfied, so it has to sleep again.</li><li>Knot(in the wait-for graph): A section which never wait for the outside threads. Because optimizing outside events will not influence the status inside(and will not improve overall thoughput), so each knot must contain a bottlenect. In a graph, a knot is a nonempty set K of vertices such that the reachable set of each vertex in K is exactly set K; a sink is a vertex with no edges directed from it.</li><li>Cascaded redistribution: If thread A waits for thread B from t1 to t2, wPerf checks what B is doing during t1 to t2 and if B is waiting for an- other thread, wPerf will re-distribute the corresponding weight and perform the check recursively.</li></ol><h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ol><li>wPerf act on events(Get the impact of events on all threads).</li><li>On-CPU analysis already has some tools good enough, while Off-CPU analysis is still inaccurate.</li><li>Wait-for graph: Each thread is a vertex and a directed edge from A to B means the time thread A waits for B.</li><li>Events with a small local impact usually have a small global im- pact, but events with a large local impact may not have a large global impact.</li><li>Things to be recorded: scheduling events, IRQ(interrupt request) events, information for I/O devices, information for busy waiting, call stacks.</li><li>wPerf can start and stop recording at any time.</li><li>wPerf treat I/O device as a pseudo I/O thread.</li><li>In order to minimize the overhead, recorder buffers events and flushs the buffers to trace file in the background. Also, the recorder creates a buffer and a trace file for each core to avoid contention.</li></ol><p><img src="/2019/10/19/paper-rev-wperf/image-20191019213731331.png" alt="image-20191019213731331"></p><h2 id="graphs"><a class="markdownIt-Anchor" href="#graphs"></a> Graphs</h2><p><img src="/2019/10/19/paper-rev-wperf/image-20191019184627121.png" alt="image-20191019184627121"></p><p><img src="/2019/10/19/paper-rev-wperf/image-20191019193352798.png" alt="image-20191019193352798"></p><h2 id="pros"><a class="markdownIt-Anchor" href="#pros"></a> Pros</h2><ol><li>It innovatively utilizes the “wait-for” graph method to investigate the wating relationship between threads, which make it easy to locate the bottleneck.</li><li>wPerf takes all these I/O operation, busy waiting, false wakeup into consideration, which make it more accuarte and competible to various cases.</li><li>Introduced “cascaded redistribution” which can help us find the origin bottleneck rather than simply take the waiting thread as the reason of latency.</li></ol><h2 id="cons"><a class="markdownIt-Anchor" href="#cons"></a> Cons</h2><ol><li>It can not be applied to distributed system currently.</li><li>It mainly foucuses on Off-CPU analysis, it may consider the combination of both On-CPU and Off-CPU analysis.</li><li>It brings in overheads in its recording process, especially when there are many waiting events.</li></ol><p><strong>Zhongyang Zhang</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;one-line-summary&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#one-line-summary&quot;&gt;&lt;/a&gt; One Line Summary&lt;/h2&gt;
&lt;p&gt;Some waiting events can cast i</summary>
      
    
    
    
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="system" scheme="https://www.miracleyoo.com/tags/system/"/>
    
    <category term="computer-architecture" scheme="https://www.miracleyoo.com/tags/computer-architecture/"/>
    
  </entry>
  
  <entry>
    <title>用Python抢救你的Hexo博客图床链接到本地</title>
    <link href="https://www.miracleyoo.com/2019/10/02/hexo-image-migrator/"/>
    <id>https://www.miracleyoo.com/2019/10/02/hexo-image-migrator/</id>
    <published>2019-10-03T03:47:49.000Z</published>
    <updated>2019-10-04T02:04:49.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题背景"><a class="markdownIt-Anchor" href="#问题背景"></a> 问题背景</h2><p>由于近期各大免费图床纷纷加入了防盗链机制（如新浪）并停止对个人博客用的图床链接进行访问授权，博客上的图片出现了大面积的无法显示（如本博客），严重影响了博客的浏览体验。然而现在直接使用文中链接尚还可以将图片下载到本地，但这也并无法得到任何官方保障，所以当务之急是把所有图床照片下载到本地，用hexo原生的图片插入格式进行插入。</p><p>而在免费图床渐渐不再可用的现在，当务之急其实已经不是再次更换图床，而是把这些图片抢救到本地，并直接将原图部署到服务器上；或是自己搭建图床。为了节省时间和成本，我这里采用了直接将原图部署到服务器上的操作。</p><h2 id="这个问题可以拆解为以下几点"><a class="markdownIt-Anchor" href="#这个问题可以拆解为以下几点"></a> 这个问题可以拆解为以下几点：</h2><ol><li>在_post文件夹中建立与markdown文件同名文件夹用于存放图片。</li><li>遍历文件夹中文件并用正则匹配的方式匹配得到待替换的链接。</li><li>下载所有图片文件并存储到相应位置。</li><li>将原文件中的<code>![name](link)</code>替换为可在网页上显示的语句。</li></ol><p>于是我为了方便使用python写了一个脚本，使得上面这几步可以自动完成。下面贴上主要代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    names=os.listdir(root)</span><br><span class="line">    files=[i <span class="keyword">for</span> i <span class="keyword">in</span> names <span class="keyword">if</span> i.endswith(<span class="string">&#x27;.md&#x27;</span>) <span class="keyword">and</span> <span class="keyword">not</span> os.path.isdir(os.path.join(root, i)) <span class="keyword">and</span> <span class="keyword">not</span> i.startswith(<span class="string">&#x27;.&#x27;</span>)]</span><br><span class="line">    file_paths = [os.path.join(root, i) <span class="keyword">for</span> i <span class="keyword">in</span> files]</span><br><span class="line">    dirs=[i <span class="keyword">for</span> i <span class="keyword">in</span> names <span class="keyword">if</span> os.path.isdir(os.path.join(root, i)) <span class="keyword">and</span> <span class="keyword">not</span> i.startswith(<span class="string">&#x27;.&#x27;</span>)]</span><br><span class="line">    dir_paths = [os.path.join(root, i) <span class="keyword">for</span> i <span class="keyword">in</span> dirs]</span><br><span class="line">    print(files)</span><br><span class="line">    <span class="keyword">for</span> file_iter <span class="keyword">in</span> files:</span><br><span class="line">        name_temp = os.path.splitext(os.path.split(file_iter)[-<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> name_temp <span class="keyword">not</span> <span class="keyword">in</span> dirs:</span><br><span class="line">            dir_temp = os.path.join(root, name_temp)</span><br><span class="line">            os.mkdir(dir_temp)</span><br><span class="line">        download(os.path.join(root,file_iter))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个文件中的链接分别进行下载和替换链接处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    print(<span class="string">&quot;==&gt; Now dealing with file:&quot;</span>, file_path)</span><br><span class="line">    dir_name = os.path.splitext(os.path.split(file_path)[-<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># filename = &quot;test&quot;</span></span><br><span class="line">    name = file_path.split(<span class="string">u&quot;/&quot;</span>)</span><br><span class="line">    filename = name[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">with</span> codecs.<span class="built_in">open</span>(file_path, encoding=<span class="string">&quot;UTF-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text = f.read()</span><br><span class="line">    <span class="comment"># regex</span></span><br><span class="line">    result = re.findall(<span class="string">&#x27;!\[(.*)\]\((.*)\)&#x27;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, content <span class="keyword">in</span> <span class="built_in">enumerate</span>(result):</span><br><span class="line">        image_quote = content[<span class="number">0</span>]</span><br><span class="line">        image_url = content[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># download img</span></span><br><span class="line">            img_data = requests.get(image_url).content</span><br><span class="line">            <span class="comment"># img name spell</span></span><br><span class="line">            image_name = image_url.strip(<span class="string">&quot;/&quot;</span>).split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">            image_path = os.path.join(root, dir_name, image_name)</span><br><span class="line">            print(<span class="string">&quot;==&gt;&quot;</span>, image_path, <span class="string">&#x27;~~~&#x27;</span>, image_url)</span><br><span class="line">            <span class="comment"># write to file</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(image_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> handler:</span><br><span class="line">                handler.write(img_data)</span><br><span class="line"></span><br><span class="line">            text=text.replace(<span class="string">&quot;![&quot;</span>+image_quote+<span class="string">&quot;](&quot;</span>+image_url+<span class="string">&quot;)&quot;</span>, <span class="string">&quot;![&quot;</span>+image_quote+<span class="string">&quot;](&quot;</span>+image_name+<span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">with</span> codecs.<span class="built_in">open</span>(file_path, mode=<span class="string">&quot;w+&quot;</span>, encoding=<span class="string">&quot;UTF-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(text)</span><br></pre></td></tr></table></figure><p>如有需求，推荐查看更加详细的使用说明和注意事项。项目在<a href="https://link.zhihu.com/?target=https%3A//github.com/miracleyoo/hexo-migrator">Github</a>上，并附有step-by-step的说明，即使没有编程基础也可以轻易上手。</p><p>如果有帮助到你，欢迎Star支持一下hhh~ 😃</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;问题背景&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#问题背景&quot;&gt;&lt;/a&gt; 问题背景&lt;/h2&gt;
&lt;p&gt;由于近期各大免费图床纷纷加入了防盗链机制（如新浪）并停止对个人博客用的图床链接进行访问授权，博客上的图片出现了大面积的无法显示（如本</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
    <category term="blog" scheme="https://www.miracleyoo.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>Linux Server One-Key Setup</title>
    <link href="https://www.miracleyoo.com/2019/09/20/one-key-linux/"/>
    <id>https://www.miracleyoo.com/2019/09/20/one-key-linux/</id>
    <published>2019-09-20T14:02:32.000Z</published>
    <updated>2019-10-04T02:04:49.240Z</updated>
    
    <content type="html"><![CDATA[<p>推荐一下新写的Linux系统一键装机脚本(๑´∀｀๑)一行命令获得常用命令行软件、zsh、方便好用的诸多zsh插件（如自动补全、一键解压、目录快速跳转、命令行语法高亮等。详见readme和source code）以及一个配置好的spacevim。当前只支持ubuntu。后面将会支持centos。</p><p>你将在任何一个新的Linux系统上一键得到一个稳定可靠的使用体验！当然，脚本高度可定制，以上所有内容都可以简单地增减。有问题或需求欢迎提issue～（注意⚠️在已经使用并配置过的电脑上运行可能会出现zshrc配置重复问题）</p><p><a href="https://github.com/miracleyoo/initialize-server-script">GitHub链接</a></p><p><strong>下面是英文的使用说明：</strong></p><h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2><p>This project aims to conveniently setup and deploy a Linux environment which is easy to use and help install many useful packages. It mainly have the ability to deploy zsh with a set of handy plugins, and a spacevim, which is my favorite vim distro. You will not encounter messy installation problems and the script is tested on ubuntu and WSL ubuntu.</p><p>Currently, it only support Ubuntu system, but the support of centos is also on the way, maybe also the MacOS version.</p><p>It is highly customizable and elegantly wrote, you can folk and customize your own version based on it!</p><h2 id="usage"><a class="markdownIt-Anchor" href="#usage"></a> Usage</h2><h3 id="method-1"><a class="markdownIt-Anchor" href="#method-1"></a> Method 1</h3><ol><li>Make sure you already have curl installed by “sudo apt-get install curl”.</li><li>Use command <code>curl -fsSL https://raw.githubusercontent.com/miracleyoo/initialize-server-script/master/one-key-linux-setup.sh -o minit.sh &amp;&amp; sudo bash minit.sh</code></li><li>You are all set! Here is an awesome new linux!</li></ol><h3 id="method-2"><a class="markdownIt-Anchor" href="#method-2"></a> Method 2</h3><ol><li>Make sure you already have curl installed by “sudo apt-get install git”.</li><li>Clone this repo using <code>git clone https://github.com/miracleyoo/initialize-server-script</code></li><li>Switch into this folder and run <code>./one-key-linux-setup.sh</code></li><li>You are all set!</li></ol><h2 id="content"><a class="markdownIt-Anchor" href="#content"></a> Content</h2><ol><li><code>apt-get install packags</code> like git, curl, tmux, vim, and python supports.</li><li>A <code>zsh</code> which has plenty of handy plugins like <code>oh-my-zsh</code>, <code>git</code>, <code>zsh-autosuggestions</code>, <code>zsh-syntax-highlighting</code>, <code>zsh-completions</code> , <code>extract</code>, <code>z</code>, <code>cp</code>. They are managed with <code>antigen</code>, which made it easy and decent to mange your zsh plugins. You can get even more plugins <a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Plugins-Overview">here</a>.</li><li>A <code>.zshrc</code> file which contains some basic but useful functions. You can change it to your own favorite commands and alias.</li><li>A <a href="https://github.com/SpaceVim/SpaceVim">spacevim</a>, which is a quite good version of vim. It initially installed several famous plugins, with a nice interface. You will find it a really vim distro as you use it. Certainly, you can change to your own version, while I’ve tested several distro and they all have some kinds of inconvenience, like the line number, extra space, wrong background color and so on.</li><li>More on the way!</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;推荐一下新写的Linux系统一键装机脚本(๑´∀｀๑)一行命令获得常用命令行软件、zsh、方便好用的诸多zsh插件（如自动补全、一键解压、目录快速跳转、命令行语法高亮等。详见readme和source code）以及一个配置好的spacevim。当前只支持ubuntu。后面</summary>
      
    
    
    
    
    <category term="server" scheme="https://www.miracleyoo.com/tags/server/"/>
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
  </entry>
  
</feed>
