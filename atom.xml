<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Miracleyoo</title>
  
  
  <link href="https://www.miracleyoo.com/atom.xml" rel="self"/>
  
  <link href="https://www.miracleyoo.com/"/>
  <updated>2023-04-22T20:44:51.676Z</updated>
  <id>https://www.miracleyoo.com/</id>
  
  <author>
    <name>Miracle Yoo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Curriculum Vitae -- Zhongyang Zhang</title>
    <link href="https://www.miracleyoo.com/2099/10/03/resume/"/>
    <id>https://www.miracleyoo.com/2099/10/03/resume/</id>
    <published>2099-10-03T22:44:20.000Z</published>
    <updated>2023-04-22T20:44:51.676Z</updated>
    
    <content type="html"><![CDATA[<p>The following document is my resume:</p><span id="more"></span><figure><img data-src="cv.png" alt="Resume_ZhongyangZhang_HUST"><figcaption aria-hidden="true">Resume_ZhongyangZhang_HUST</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;The following document is my resume:&lt;/p&gt;</summary>
    
    
    
    
    <category term="Resume" scheme="https://www.miracleyoo.com/tags/Resume/"/>
    
  </entry>
  
  <entry>
    <title>GNN-based Human Pose Estimation (HPE) Paper Summary</title>
    <link href="https://www.miracleyoo.com/2023/02/19/gnn-hpe/"/>
    <id>https://www.miracleyoo.com/2023/02/19/gnn-hpe/</id>
    <published>2023-02-20T01:34:18.000Z</published>
    <updated>2023-04-23T00:35:13.334Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation">DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation</h2><p><img data-src="image-20221219151203243.png" alt="image-20221219151203243" style="zoom:50%;"></p><p><img data-src="image-20221219154249782.png" alt="image-20221219154249782" style="zoom:50%;"></p><h3 id="summary">Summary</h3><ul><li>Multi-person.</li><li>Image-based. Graph is just used in their DGCM module.</li><li>Bottom-Up.</li></ul><span id="more"></span><ul><li><p>Basic Hypothesis:</p><blockquote><p>Existing bottom-up methods mainly define relations by empirically picking out edges from this graph, while omitting edges that may contain useful semantic relations.</p></blockquote><p>But actually OpenPose is using the similar idea: "Redundant PAF connections". The difference is that OpenPose redundant connections are still empirical, while in this paper, they take into account all possible connections between two arbitrary joints, instead of limited amount.</p></li><li><p>Core idea: 本文是一篇基于RGB图片的Bottom-up 2D HPE文章。文中Graph是作为其核心模块DGCM的一部分出现的，且并没有使用GNN。这里的Graph是用来建模joints之间关系的，即摆脱了传统的基于经验的骨骼链接，也不是单纯的基于dataset计算的软性链接，而是将软性链接的值当做伯努利分布中的概率来用，每次随机筛出来几个可能的邻接矩阵来用。本文中所谓Graph的使用其实完全可以被transformer代替，本质上是在建模关节点之间的关联性。</p></li><li><p>Graph:</p><ul><li>Node: Human Joints。</li><li>Node Value: 无。</li><li>Edge: Joints间的全连接。</li><li>Edge Value: 两个joints之间的相关性。</li><li>Output: 一个joints间的邻接矩阵。</li><li>Usage: 用于生成带有随机性的、基于soft adjacency matrix的人类关节点关联矩阵。</li></ul></li></ul><h3 id="points">Points</h3><ul><li><blockquote><p>Bottom-up pose estimation methods try to learn two kinds of heatmaps from the deep neural network, including keypoint heatmaps and relation heatmaps.</p></blockquote></li><li><p>Soft Adjacency Matrix:</p><ul><li>基于全部训练集中任意两个joints之间的距离（normalized by scale factor s）的倒数构建软邻接矩阵。</li><li>由于任意一个点到期自身的距离是0，相应的邻接矩阵值设为1。</li><li><span class="math inline">\(A_{s}^{ij}=\sigma(\gamma\frac{1}{M_d^{ij}}), A_s^{ii}=1\)</span>。</li></ul></li><li><p>Dynamic Adjacency Matrix：</p><ul><li><span class="math inline">\(A_d^{ij}\sim B(x,A_s^{ij})\)</span>，x是数量，<span class="math inline">\(A_s^{ij}\)</span>是概率，B是伯努利分布。</li><li>这里的伯努利分布（01分布）实质上就是把软邻接矩阵的值变成了概率。A^(i,j)值越大，概率越大，取1的可能性越大。就这样筛选几次，得到几个可能的邻接矩阵。</li></ul></li><li><p>文中使用了金字塔式多尺度feature map，理由是人的大小远近不一样，多尺度的feature更有适应性。</p></li><li><p>由于邻接矩阵里面每个joint和它自己的relation-term永远是1，就相当于加了一个skip connection一样，不用担心每个joint只被其邻居决定。</p></li><li><p>除了这里提到的动态邻接矩阵，还有一个learnable <span class="math inline">\(K\times K\)</span>的weights，共同起作用。</p></li></ul><h2 id="learning-dynamics-via-graph-neural-networks-for-human-pose-estimation-and-tracking">Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking</h2><p><img data-src="image-20221219160516007.png" alt="image-20221219160516007" style="zoom:50%;"></p><h3 id="summary-1">Summary</h3><ul><li>Multi-person 2D HPE.</li><li>Top-Down.</li><li>Frame-based.</li><li>Pipeline:<ul><li>Current Frame 2D HRNet Branch: Crop Human -&gt; Rescale -&gt; HRNet -&gt; HeatMap-F -&gt; Argmax -&gt; Joints-F</li><li>Historical tracklets GNN Branch: Build Joints based on (Tracklets' (0~t-1) joints' + current frames' all pixels')(Visual Features, Joint Locations, Joint Type) -&gt; Connect edges in time and space -&gt; GNN -&gt; HeatMap-G -&gt; Joints-G</li><li>Merging: Hungarian(Joints-F, Joints-G) -&gt; If no matching, new ID, output Joints-G; If matched, output argmax(HeatMap-F, HeatMap-G)</li></ul></li><li>本文是一个Tow-Down的、基于帧序列（视频）的、2D Multi-Human HPE+Human Tracking的工作。它先用传统的剪裁预测方式预测每帧的所有可能人的可能joints，然后再将每帧每人的每个joint都通过MLP转换为一个个node的feature map，连接这些node，过一个GNN，最后得到最终预测。</li><li>Graph：<ul><li>Node: 已经被追踪的人的<strong>所有历史joints</strong>(FIFO内的)及当前Frame的<strong>所有像素</strong>。</li><li>Node Value: 一个基于HRNet输出feature、2D相对位置、joint类型softmax的综合feature map，综合方式是分别过MLP后average。</li><li>Edge: 帧内和帧间分别全连接。</li><li>Edge Value: 无。</li><li>Output: 对于每个此前追踪过的人，输出对其每个关节点在当前帧位置的预测。由于当前帧全图像素都是nodes，所以对于每种joint type，其实输出的相当于一个HeatMap。</li><li>Usage: 通过分析过往最终过的人的历史joints location，以及当前帧的visual feature，预测这些人的每个joint在当前帧的位置。</li></ul></li></ul><h3 id="question">Question</h3><ul><li><p>怎么做的tracking？</p><ul><li>又是匈牙利算法。GNN基于tracklets预测的结果和基于当前帧HRNet预测出来的结果进行matching。其中使用的相似度是基于关键点的位置计算出来的。</li></ul></li><li><p>GNN 基于tracklets历史预测的poses和当前帧通过HRNet预测的Pose是如何结合（Aggregate and Merge）的？</p><ul><li>先匹配，匹配上的就对heatmap做平均然后argmax。</li><li>没匹配上的就给一个新ID，直接argmax。</li><li>对匹配上的人，如果FIFO已经满了，就踢一个加新的；反之直接加了。</li></ul><blockquote><p>For all the matched poses, the joint heatmaps of the two poses are first aligned according to their centers and then merged together by averaging the heatmaps.</p></blockquote></li><li><p>GNN的edge怎么定义的？</p><ul><li>Edge有两种，一种是同一帧中joints之间的链接，一种是每个joint与上一帧中所有joints的联系（包括当前帧的nodes）。</li><li>没有具体说edge的值（attr），应该是没有用。</li></ul></li></ul><h3 id="points-1">Points</h3><ul><li><p>每个tracklet都是一个unique的被记录在案的人。</p></li><li><p>对于每个tracklet，都会分别被过一遍一个GNN进行预测。注意，GNN这里也可以理解做是Top-Down的，对于每个历史上记录在案的人，都分别过一遍GNN预测在当前帧这个人每个关节点的位置。</p></li><li><p>GNN的nodes包含某个历史tracklets（追踪的关键点）中的所有joints以及当前帧的所有像素点。</p></li><li><p>当前帧<em>(t)</em>的每个像素组成的node都与tracklets FIFO中最后一帧<em>(t-1)</em>的检测结果中的每个joint相连。</p><p><img data-src="image-20221220121801045.png" alt="image-20221220121801045" style="zoom:50%;"></p></li><li><p>之所以要对Visual Features (<span class="math inline">\(v_k\)</span>), Joint Locations(<span class="math inline">\(p_k\)</span>), Joint Type(<span class="math inline">\(c_k\)</span>)向量分别做MLP，是因为它们的维度不同，且node的channel不能有多维。</p></li><li><p><span class="math inline">\(J_k=Pooling(MLP_{vis}(v_k), MLP_{pos}(p_k), MLP_{type}(c_k))\)</span> 为每个node的值。这里的pooling是average pooling。</p></li><li><p>对于<span class="math inline">\(p_k\)</span>而言，全部帧的这个location都以FIFO中最后一帧<em>(t-1)</em>中人物中心点为基准进行normalization，当前帧这些nodes也不例外。</p></li><li><p>注意，当前帧<em>(t)</em>全像素组成的这些nodes里面，在算<span class="math inline">\(J_k\)</span>时候不加<span class="math inline">\(c_k\)</span>分支，即不用joint type，因为joint type是要predict的东西。</p></li><li><p>最终预测出来的东西是一个Prob，它包含了当前帧t所有像素点可能为某个joint type的概率（classification）。</p></li><li><p>对于每个tracklets，都有一个FIFO队列保存K个过去位置。</p></li></ul><h3 id="comments">Comments</h3><ul><li>非常新颖的使用GNN的方法。用多个方面的features分别过MLP通过average pooling的方法作为node的值，既可以确保这些feature的维度不match不成问题，也可以有效降低channel number，降低cost。最重要的是，对于某些node，你甚至可以移除某些features而不影响整体维度，比如对当前frame像素nodes不添加不存在的类别features。</li><li>同时也非常暴力，直接在帧内、帧间分别用全连接，且当前frame直接把全部像素点位置都变成node，这是一个巨大的开销，并不优雅，也不符合GNN的内含逻辑。</li><li>位置norm方法值得学习。</li><li>我们的项目可能也可以用上匈牙利匹配，但是匹配的是时间上的cluster，而匹配的变量可能是cluster的平均方向向量等。</li></ul><h2 id="context-modeling-in-3d-human-pose-estimation-a-unified-perspective">Context Modeling in 3D Human Pose Estimation: A Unified Perspective</h2><figure><img data-src="image-20221221105021381.png" alt="image-20221221105021381"><figcaption aria-hidden="true">image-20221221105021381</figcaption></figure><h3 id="summary-2">Summary</h3><ul><li>Single image-based.</li><li>2D -&gt; 3D lifting.</li><li>Top-Down.</li><li>核心操作是一套Attention。首先他们预测出2D Pose，然后将这些Pose投影到3D空间，再用一套Encoder-Decoder网络来预测3D Heatmap。具体来说，每个voxel预测J个值，代表它是某个joint的可能性。而其中关键的Attention部分分为两部分，全局Attention和关节对Attention。前者预测某个voxel <span class="math inline">\(q\)</span> 含某个joint <span class="math inline">\(u\)</span>的概率，后者预测当另一个voxel <span class="math inline">\(k\)</span> 包含joint <span class="math inline">\(v\)</span> 时，<span class="math inline">\(u,v\)</span> 间的关联性。这个关联性来自于训练集上的prior，主要成分是所有物理（经验）肢体连接的距离平均值和标准差，这个关联性高时说明 <span class="math inline">\(u,v\)</span> 在当前voxel <span class="math inline">\(q, k\)</span> 中时大概率符合prior中的预测，反之说明这对joints连接的肢体可能过长/短了，所以当前分布可能性很小。</li><li>Graph:<ul><li>Node: 所有的voxels。</li><li>Node Value: 来自于2D坐标投影的features。</li><li>Edge: Voxel间的连接。</li><li>Edge Value: Voxel间的关联性（Pairiwise-Attention）。</li><li>Output: 每个3D voxel包含某个joint的概率。</li><li>Usage: 用于从2D预测升维到3D。</li><li>Note: 这里所谓的GNN其实就是ContextPose (Attentions)的一种特例。</li></ul></li></ul><h3 id="points-2">Points</h3><ul><li>Pairwise-Attention（关节对Attention）的公式是：<span class="math inline">\(P(q,k,e_{u,v}) \propto exp(-\frac{(||q-k||_2-\mu_{u,v})^2}{2\alpha\sigma^2_{u,v}+\epsilon})\)</span><br></li><li>PA被normalized了，具体公式是：<span class="math inline">\(\Sigma_{k\in\Omega}G_v(x_{v,k}) \cdot P(q,k,e_{u,v})=1\)</span>, 意味着如果voxel <span class="math inline">\(q\)</span> 包含关节 <span class="math inline">\(u\)</span>，那么所有可能包含关节 <span class="math inline">\(v\)</span> 的voxel <span class="math inline">\(k\)</span> 它本身的概率乘以它满足 <span class="math inline">\(u,v\)</span> 两个关节点物理距离prior的概率和为1。</li></ul><h2 id="optimizing-network-structure-for-3d-human-pose-estimation">Optimizing Network Structure for 3D Human Pose Estimation</h2><figure><img data-src="image-20221221122107913.png" alt="image-20221221122107913"><figcaption aria-hidden="true">image-20221221122107913</figcaption></figure><p><img data-src="image-20221221122050872.png" alt="image-20221221122050872" style="zoom:50%;"></p><h3 id="summary-3">Summary</h3><ul><li><p>本文提出了一种统一模型，它可以将2D-&gt;3D Human Pose Lifting的诸多方法（FCN，GNN，LCN）写入一个统一公式中，并取得更优的效果。</p></li><li><p>相比于普通GCN，其特点在于：</p><ul><li>普通GCN对于所有的node，做Linear的时候都用的共享的weights。</li><li>而在LCN中，计算每个输入node对输出node的贡献时所用的weights，都分别训练，不共享参数。</li></ul><p><img data-src="image-20221221135856400.png" alt="image-20221221135856400" style="zoom:50%;"></p></li><li><p>Graph:</p><ul><li>Node: J个人类Joints。</li><li>Node Value: 2D坐标的features。</li><li>Edge: Joints间的连接。</li><li>Edge Value: Joints间的关联性（邻接矩阵）。</li><li>Output: 每个Joint的3D坐标。</li><li>Usage: 用于从2D预测升维到3D。</li></ul></li></ul><h2 id="adaptive-hypergraph-neural-network-for-multi-person-pose-estimation">Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation</h2><figure><img data-src="image-20221220191107517.png" alt="image-20221220191107517"><figcaption aria-hidden="true">image-20221220191107517</figcaption></figure><h3 id="summary-4">Summary</h3><ul><li><p>Image-based.</p></li><li><p>Multi-human 2D HPE.</p></li><li><p>Top-Down.</p></li><li><p>文中提出了一种“<strong>超边</strong>”(Hyperedge)，这种边不是物理的边，也不是非物理但连接两个顶点的边，而是连接了多个顶点、有着类似<strong>区分body part</strong>作用的“大边”。这种超边的分配由训练好的网络负责，生成的依据是图中的语义关系。比如如果人在拉伸右腿，那右手和右脚踝就会被分配到一个超边中，因为它们有着紧密的语义联系。</p><p><img data-src="image-20221220191125713.png" alt="image-20221220191125713" style="zoom:50%;"></p></li><li><p>本文做的是Top-Down的单张图片2D多人HPE。其核心贡献在于提出了“超边”这种由图片语义得来、可连接两个到多个joints的广义边，从而可以更好地应对一些非典型动作的图片。</p></li><li><p>Graph:</p><ul><li>Node: Human Joints.</li><li>Node Value: d dimension feature.</li><li>Edge: m条，为hyperedges。</li><li>Edge Value: 每条超边链接的joints以及链接的紧密程度。</li><li>Output: 一个<span class="math inline">\(J\times m\)</span>的矩阵，m是超边的数量，J是关键点的数量。代表着联系的紧密程度。</li><li>Usage: 根据图片信息更好的建立人类关键点间的高维语义联系。</li></ul></li></ul><h3 id="questions">Questions</h3><ul><li>Adaptive hypergraph矩阵可以包含可变数量的边。这个是怎么处理的？</li><li>这个矩阵是如何训练的？输入是什么？</li></ul><h3 id="points-3">Points</h3><ul><li>对于一层GCN而言，它的forward公式是<span class="math inline">\(H^{(l+1)}=\sigma (\tilde{A}H^{(l)}W^{(W+1)})\)</span>，这里的<span class="math inline">\(\sigma\)</span>代表激活函数，<span class="math inline">\(\tilde{A}\)</span>代表被normalized邻接矩阵，<span class="math inline">\(W\)</span>是这层GCN的parameter。<span class="math inline">\(H\)</span>是feature map，<span class="math inline">\(l\)</span>这里代表第<span class="math inline">\(l\)</span>层GCN。正常情况下，这个步骤整体都被打包进<code>pyg.GCN</code> module一起做了，但这里我们刚好有现成的邻接矩阵<span class="math inline">\(A\)</span>，就不用专门再去构造graph，弄一个专门的<code>edge_index</code>矩阵了，因为<span class="math inline">\(A\)</span>就是了。</li></ul><h2 id="learning-skeletal-graph-neural-networks-for-hard-3d-pose-estimation">Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation</h2><figure><img data-src="image-20221221141417256.png" alt="image-20221221141417256"><figcaption aria-hidden="true">image-20221221141417256</figcaption></figure><h3 id="summary-5">Summary</h3><ul><li><p>本文提出了一种针对不同距离（hop）的nodes使用不同Aggregation层级的GCN，保证了不同距离的node得以被区分对待，降低远距离node带来的噪音。同时，它还结合了经验人类skeleton和动态可学习skeleton的优点，用同步的两个branch分别运算，进一步提升了基于video的2D-&gt;3D HPE Lifting的质量。</p></li><li><p>Graph:</p><ul><li><p>Node: J个人类Joints。</p></li><li><p>Node Value: 2D坐标的features。</p></li><li><p>Edge: Joints间的连接。分为两部分，一部分是物理连接，一部分是动态可学习的连接。</p></li><li><p>Edge Value: Joints间的关联性（邻接矩阵）。</p></li><li><p>Output: 每个Joint的3D坐标。</p></li><li><p>Usage: 用于从2D预测升维到3D。</p><blockquote><p>Given 2D keypoints <span class="math inline">\(X ∈ R^{N\times 2}\)</span>, with N nodes, the model outputs better 3D positions <span class="math inline">\(Y ∈ R^{N\times 3}\)</span>.</p></blockquote></li></ul></li></ul><h3 id="points-4">Points</h3><ul><li><p>决定文中设计思路的几个重要观察：</p><ol type="1"><li>Joints Graph中距离遥远的node虽然有时也会提供有价值的信息，但也导入了许多不相干的噪音。</li><li>动态Joints Graph的构建很有效，但在不同的动作中joints间的关联性变化很大。所以虽然动态图在表征不同的动作时候很直观，如果只给定单张图，它也很容易受到outlier的影响。所以文中引入了TCN让信息在时间轴上流动。</li></ol></li><li><p>实际设计上，基本是一层D-HCSF一层TCN这样串联。</p><p><img data-src="image-20221221150901507.png" alt="image-20221221150901507" style="zoom:50%;"></p></li><li><p>Zoom in，看HCSF层，他们的核心思想是对近程和远程的node区分对待，分不同的层级对待。对于离自己近的node，用类似skip connection的连接直连最后；而离得远的nodes则先分组aggregate几次后再连到主bus上。</p></li><li><p>Dynamic Hierarchical Channel-Squeezing Fusion (D-HCSF) 层的思想：结合了固定skeleton和动态学习skeleton的长处。蓝色是固定分支，橙色是动态。</p><p><img data-src="GNN-based HPE/image-20221221150056706.png" alt="image-20221221150056706" style="zoom:50%;"></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation&quot;&gt;DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation&lt;/h2&gt;
&lt;p&gt;&lt;img data-src=&quot;image-20221219151203243.png&quot; alt=&quot;image-20221219151203243&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;image-20221219154249782.png&quot; alt=&quot;image-20221219154249782&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multi-person.&lt;/li&gt;
&lt;li&gt;Image-based. Graph is just used in their DGCM module.&lt;/li&gt;
&lt;li&gt;Bottom-Up.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>SMPL 完全攻略 -- 从定义到文章到部署</title>
    <link href="https://www.miracleyoo.com/2023/01/21/smpl-all/"/>
    <id>https://www.miracleyoo.com/2023/01/21/smpl-all/</id>
    <published>2023-01-22T01:30:32.000Z</published>
    <updated>2023-04-23T00:31:06.584Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><ul><li>SMPL主要含有两组参数，一组是人物的体态信息β，一组是人物的姿态信息θ。</li><li>SMPL本身是“相对的”，其只包含人物本身的信息，而不包含任何与环境、相机视角、位置等信息。另外，其mesh点记录的值是相对于模板人类模型标准值的。</li><li>SMPL不包含手、脸和衣服，但后续的其他文章逐渐完善了相应参数。</li></ul><span id="more"></span><ul><li>SMPLX中的参数某种程度上包含了肌肉随动作（Pose）变化的信息。他们采用了铰链模型，并搭配PCA方法分析主要关联项，得到了当人体从静态到某个特定Pose时哪些肌肉会发生形变以及如何形变的信息，而其NB也主要NB在这里。有了这些关联模型，人体运动时重建的mesh就不是那种被奇怪地拉伸的mesh了，而是更加符合人体肌肉运动规律的mesh。</li><li>他们使用的训练数据是4维扫描，即<code>3D Mesh + Time</code>的扫描。他们也正是通过分析关节点和mesh随着时间的变化，才分析得到了Pose和Mesh变形之间的相关性。</li><li>在构建MONO/SMPL+H时，由于手非常复杂且容易被遮挡，另外在全身扫描时手部分辨率也很有限，所以他们特意为了手部额外采集了一个数据集，含有各种人物、姿态和遮挡。</li></ul><h2 id="名称辨析">名称辨析</h2><ul><li><code>SMPL</code>: A Skinned Multi-Person Linear Model。人体模型。</li><li><code>MONO</code>: A hand Model with Articulated and Non-rigid defOrmations。手部模型。</li><li><code>SMPL+H</code>: A fully articulated body and Hand model。手部模型+人体模型。</li><li><code>SMPL-X</code>: SMPL eXpressive，是一个含有姿态、表情、手部动作的人体模型。</li><li><code>SMPLify-X</code>：<code>SMPL-X</code>原文中提到的用于拟合SMPL-X模型的一种方法。具体操作是先预测2D Joints，再用Optimization的方法拟合3D模型使得投影与2D Joints尽可能重合。</li><li><code>SMPLify</code>：<code>SMPLify-X</code>的前辈。方法类似，但是效果和速度差一点。</li></ul><h2 id="d模型制作和运用中常用术语">3D模型制作和运用中常用术语</h2><p>该节摘自<a href="https://zhuanlan.zhihu.com/p/256358005">SMPL论文解读和相关基础知识介绍</a></p><blockquote><ul><li>顶点（vertex）：动画模型可以看成多个小三角形（四边形）组成，每个小三角形就可以看成一个顶点。顶点越多，动画模型越精细。</li><li>骨骼点：人体的一些关节点，类似于人体姿态估计的关键点。每个骨骼点都由一个三元组作为参数去控制（可以查看欧拉角，四元数相关概念）</li><li>骨骼蒙皮（Rig）：建立骨骼点和顶点的关联关系。每个骨骼点会关联许多顶点，并且每一个顶点权重不一样。通过这种关联关系，就可以通过控制骨骼点的旋转向量来控制整个人运动。</li><li>纹理贴图：动画人体模型的表面纹理，即衣服裤子这些。</li><li>BlendShape：控制动画角色运动有两种，一种是上面说的利用Rig，还有一种是利用BlendShape。比如：生成一种笑脸和正常脸，那么通过BlendShape就可以自动生成二者过渡的动画。这种方式相比于利用Rig，可以不定义骨骼点，比较方便。</li><li>蒙皮：将模型从一个姿态转变为另一个姿态，使用的转换矩阵叫做蒙皮矩阵。（Linear Blend Skinning算法）</li><li>顶点权重(vertex weights)：用于变形网格mesh</li><li>uv map：将3D多边形网格展开到2D平面得到 UV图像</li><li>texture map：将3D多边形网格表面的纹理展开到2D平面，得到纹理图像</li><li>拓扑(topology)：重新拓扑是将高分辨率模型转换为可用于动画的较小模型的过程。两个mesh拓扑结构相同是指两个mesh上面任一个三角面片的三个顶点的ID是一样的（如某一个三角面片三个顶点是2,5,8；另一个mesh上也必有一个2,5,8组成的三角面片）</li><li>linear blend skinning algorithm</li></ul><p>每个关节的数据结构包含：关节名字、骨骼中其父节点的索引、关节的绑定姿势之逆变换（蒙皮网格顶点绑定至骨骼时，关节的位置、定向及缩放）</p></blockquote><h2 id="三种类型的smpl文章应用">三种类型的SMPL文章/应用</h2><ol type="1"><li>提出SMPL这种模型本身的定义的文章。这些文章使用大量不同动作的3D人体扫描构筑了一个平均人类模型（Male，Female，Neutral），且定义了当人物的形体和动作发生特定变化时表皮的相应变形方式。文中的“训练”指的是训练这些运动/形态参数与实际表皮形变的函数映射。</li><li>使用单张/多张不同角度照片来推理相应人物的SMPL模型位置。如SMPLify和SMPL-X。SMPLify-X（用于拟合SMPL-X模型的方法）的pipeline是：<ol type="1"><li>先自下而上用OpenPose预测人体2D关键点。</li><li>然后再结合各种Prior（身体姿态、手部姿态、身体形状、面部姿态、面部表情、极端弯曲）和各种Loss Punishment（2D与3D在平面投影的Loss，身体Parts互相穿透的Loss）去让3D的模型拟合这些关键点。</li><li>用了Optimization算法直接去拟合的3D模型参数，这个步骤没有使用深度学习。相反，使用的是Limited-memory BFGS optimizer (L-BFGS)的强化Wolfe line search（SMPL-X）、Chumpy+OpenDR（SMPLify）。</li></ol></li><li>端到端的深度学习文章。这些文章都是在SMPL-X发表之后涌现出来的。文章中不再使用Optimization-based methods，而是转而使用SMPLify-X来通过照片或多角度照片生成Ground Truth，制造完数据集后直接使用DL进行预测。</li></ol><h2 id="smpl">SMPL</h2><ul><li><p>SMPL一文与从图片/视频/XXXX预测人体形态没有任何关系，它的贡献单纯是提出了一个更好的人体模型，而这个模型可以很好地建模人的体态和姿势，同时肌肉/蒙皮形状会随着人的运动而相应变化，从而达到拟真的效果，而不会出现滑稽的拉伸形变。</p></li><li><p>SMPL提出的人体模型的输入是由两部分组成的：</p><ul><li>β：一个10维vector</li><li>θ：一个3(K+1)维vector，K是骨架节点数，这里是23。加的1是人体中心。</li></ul></li><li><p>SMPL的输出是N个顶点的坐标，维度为3N。N: 顶点数，6890。</p></li><li><p>SMPL人类模型是可微分的，也就是说，如果你有了人物的3D扫描，想用一个deep learning model来预测这个扫描，你可以直接输入图片/视频/XXXX，中间输出是<span class="math inline">\(\beta+\theta\)</span>，然后最终输出N个顶点的位置，而这N个顶点你是可以直接去和GT做L1 loss的，因为可微也就意味着可以auto backward。</p></li><li><p><span class="math inline">\(\beta\)</span>的十个参数物理意义：</p><blockquote><p>0 代表整个人体的胖瘦和大小，初始为0的情况下，正数变瘦小，负数变大胖（±5） 1 侧面压缩拉伸，正数压缩 2 正数变胖大 3 负数肚子变大很多，人体缩小 4 代表 chest、hip、abdomen的大小，初始为0的情况下，正数变大，负数变小（±5） 5 负数表示大肚子+整体变瘦 6 正数表示肚子变得特别大的情况下，其他部位非常瘦小 7 正数表示身体被纵向挤压 8 正数表示横向表胖 9 正数表示肩膀变宽</p></blockquote></li><li><p>论文核心图的解释：</p><figure><img data-src="image-20221118192351294.png" alt="image-20221118192351294"><figcaption aria-hidden="true">image-20221118192351294</figcaption></figure><ul><li>(a)是平均人体模型，男女各一个，后续的演算都是基于标准平均人体模型的</li><li>(b)是加入了人物体态参数的结果</li><li>(c)是加入了特定动作发生时肌肉/蒙皮变形补偿后的结果。注意此时只是对即将发生的动作进行补偿，但还没有真正apply动作。</li><li>(d)是实际让人物摆出了相应动作的结果</li></ul></li><li><p>再回到上图的一些重要符号表达，</p><ul><li><p><span class="math inline">\(\bar{T}\)</span>: 3N维vector，由N个串联的顶点表示的初始状态下的平均模型。这里的3并不是xyz 3D坐标，而是每个关节点相对于其父关节的轴角旋转量。这里的坐标以父节点为原点。</p><blockquote><p><span class="math inline">\(\omega\)</span> denotes the axis-angle representation of the relative rotation of part k with respect to its parent in the kinematic tree</p></blockquote></li><li><p><span class="math inline">\(\mathcal{W}\)</span> : <span class="math inline">\(4\times 3N\)</span>维，其实应该是<span class="math inline">\(K\times 3N\)</span>维才对，但为了和现存渲染引擎同步，这里取每个顶点最多被附近4个关节点的运动影响。<span class="math inline">\(\mathcal{W}\)</span>是LBS/QBS混合权重矩阵。由于顶点和其附近的关节点存在相关性，这个相关性是每个顶点对应多个关节点，且权重不一。这里就需要这样一个矩阵来记录这种相互关系，即关节点对顶点的影响权重 (第几个顶点受哪些关节点的影响且权重分别为多少)。</p></li><li><p><span class="math inline">\(J\)</span> : 用于补偿joint position因为目标人物体态变化产生的位移。它通过表皮的形状位置来推测新的joints位置。</p></li><li><p><span class="math inline">\(B_S(\overrightarrow{\beta})\)</span>里面的这个<span class="math inline">\(B_S\)</span>的作用是把已经经过PCA筛选压缩过的10个参数恢复到正常的3N维度，即对于每个顶点，应当向哪个方向变化来适应这个人的体态。这里恢复出来的值也是相对于平均模型的。</p></li><li><p><span class="math inline">\(B_S(\overrightarrow{\theta})\)</span>同理，由于我们输入的pose <span class="math inline">\(\overrightarrow{\theta}\)</span> 也只有3(K+1)维度，而想要对因为人体做出特定动作产生的形变进行补偿，也需要一个3N维度的值来对每个顶点分别建模补偿。</p></li></ul></li><li><p>而关于训练，训练过程中对于形态和姿势的训练时分开进行的。前者在一个<code>Multi-Shape Dataset</code>上训练完成，而后者在一个<code>Multi-Pose Dataset</code>上训练完成，二者是相互独立的。</p></li><li><p>模型训练主要训练的是这些参数：形态的<span class="math inline">\(\bar{T}, \mathcal{S}\)</span>和姿势的参数<span class="math inline">\(\mathcal{J},\mathcal{W},\mathcal{P}\)</span>。除去上面介绍过的<span class="math inline">\(\bar{T},\mathcal{W}\)</span>，其他几位的介绍如下</p><ul><li><span class="math inline">\(\mathcal{J}\)</span>: <span class="math inline">\(3N\times 3K\)</span>，将rest vertices转换成rest joints 的矩阵。</li><li><span class="math inline">\(\mathcal{P}\)</span>: 矩阵形状为<span class="math inline">\(3N\times 9K\)</span>，这里之所以K前面系数是9，是因为使用时把关节点的坐标从3D空间坐标处理成了其相对于根节点的旋转矩阵，而3D旋转矩阵有9个参数。<span class="math inline">\(\mathcal{P}\)</span>是这所有27*9=207个 pose blend shapes 组成的矩阵。因此，pose blend shape 函数BP(θ→;P) 完全被矩阵 P 定义。</li></ul></li></ul><h2 id="模型本身使用说明">模型本身使用说明</h2><ul><li>模型本身在Python中的使用是相当简单直接的：<code>Load Model -&gt; Assign β&amp;θ -&gt;Dump</code></li><li>Dump出来的pkl模型可以直接在Blender/Unity等软件中读取，应用到SMPL模型中。</li></ul><h2 id="smpl-x">SMPL-X</h2><ol type="1"><li>SMPL-X其实是一个大杂烩，它结合了基本姿态用的SMPL模型，手部姿态的MANO模型和面部特征的FLAME模型。</li><li>SMPL-X模型本身分别在多个数据集上训练：<ol type="1"><li><span class="math inline">\(\{S\}\)</span>：形状空间参数（shape space parameters），在3800个A-pose捕捉不同性别的变化的数据集上训练</li><li><span class="math inline">\(\{W，P，J\}\)</span> ：身体姿态空间参数（body pose space parameters），在1786个不同姿态的数据集上训练</li><li>MANO：姿态空间及姿态相关混合形状（pose space and pose corrective blendshapes），1500 手部扫描数据</li><li>FLAME：表情空间<span class="math inline">\(\{\varepsilon\}\)</span>（expression space），3800 头部高精度扫描数据上训练</li></ol></li><li>SMPL-X区分了男女模型，用了一个性别分类器。</li><li>Pipeline（上文有提）：<ol type="1"><li>先自下而上用OpenPose预测人体2D关键点。</li><li>然后再结合各种Prior（身体姿态、手部姿态、身体形状、面部姿态、面部表情、极端弯曲）和各种Loss Punishment（2D与3D在平面投影的Loss，身体Parts互相穿透的Loss）去让3D的模型拟合这些关键点。</li><li>用了Optimization算法直接去拟合的3D模型参数，这个步骤没有使用深度学习。相反，使用的是Limited-memory BFGS optimizer (L-BFGS)的强化Wolfe line search（SMPL-X）、Chumpy+OpenDR（SMPLify）。</li></ol></li><li>既可以用于从2D joints coordinates得到SMPL-X，也可以通过3D joints coordinates得到，如论文“<em>I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</em>” 中，作者就通过SMPLify-X得到了H3M数据集的SMPL版本。</li></ol><h2 id="已有数据集转化">已有数据集转化</h2><ul><li>带有图片和2D Human Joints Label的数据集：使用<a href="https://github.com/JiangWenPL/multiperson/tree/master/misc/smplify-x">SMPLify-X</a>，或<a href="https://github.com/facebookresearch/eft">EFT</a>。</li><li>带有图片和3D Human Joints Label的数据集：使用<a href="https://github.com/JiangWenPL/multiperson">multiperson</a>库中的<a href="https://github.com/JiangWenPL/multiperson/tree/master/misc/smplify-x">SMPLify-X-for-3D</a>工具.</li><li>带有图片和多视角2D Human Joints Label的数据集：使用SMPLify</li><li>只有图片：使用<a href="https://github.com/facebookresearch/eft">EFT</a>。</li></ul><h2 id="动作的生产转化导入">动作的生产/转化/导入</h2><ul><li>我们已经将相关github repo打包成了docker，可以直接一键部署使用。具体链接和详细教程后续整理后更新。</li></ul><h2 id="other-papers">Other Papers</h2><p>在看完上面几篇最重要的鼻祖论文之后，下面是几篇最近的使用了SMPL系统的文章，通过对他们的分析可以对SMPL的应用有一个更好的理解。</p><h3 id="object-occluded-human-shape-and-pose-estimation-from-a-single-color-image">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</h3><p><img data-src="image-20221129172810204.png" alt="image-20221129172810204" style="zoom:50%;"></p><ul><li><p>数据采集：</p><ul><li>他们先用Mask-RCNN和Alphapose来预测人体Mask及2D Joints。</li><li>对于不准确的label，他们手动矫正。</li><li>最后用SMPLify-X中的多视角方法得到GT。</li><li>得到GT后，他们把这个3D SMPL模型朝着图像平面投影，若投影不在Mask覆盖范围内（即被遮挡），那就给涂黑（-0.5），反之，用表面xyz构建3通道UV Map。</li></ul></li><li><p>数据处理：</p><ul><li>文中预测的基本单位是顶点（即表面上的全部点）而非骨骼、joints。</li><li>UV Map看上去是彩色的，这个颜色其实意味着UV Map有三个通道，而这三个通道是相应每个点所对应的x、y、z坐标值。</li><li>其中，对于未被遮挡的点（Mask点亮的点），把xy给norm到<span class="math inline">\([-0.5,0.5]\)</span>即可；而对被遮挡的点，其xyz是<span class="math inline">\([-0.5,-0.5,-0.5]\)</span>，即全黑（以-0.5为黑）。</li></ul></li><li><p>Pipeline：</p><ol type="1"><li>Pipeline分为两部分，Train和Predict。</li><li>Train会先训练一个UV Map Inpainting Network。把之前处理过的、过了norm且遮挡部分给涂黑的UV Map给送到一个网络里，试图输出被涂黑区域得以被重新预测的UV Map。这就把3D SMPL HPE转成了一个image inpainting问题，即填补空缺部分图像。</li><li>Predict过程中，会先预测Mask，然后直接concatenate共同作为输入。</li><li>然后RGB Image预测全体UV Map时候用的是另一个完全不同于训练时UV Map Inpainting Network的branch，但是在这个branch里，Inpainting Branch的高维度feature被拿了进来作为constrain。这里说的不清楚，应该是把这个feature map concatenate到RGB image过了Encoder之后的feature map上作为prior。需要参考代码理解。</li></ol></li><li><p>Points：</p><ul><li><p>UV Map Inpainting Sub-Network的Loss由三部分构成：</p><ol type="1"><li><p>首先是预测的UV Map和GT UV Map的L1 Loss。</p></li><li><p>然后是一个挺有趣的term，用于保证人体的每个部位预测出的UV Map是平滑的：对每个点计算一个它与其下方和右方点的差的绝对值。然后通过最小化这个值，保证预测的UV Map的平滑性。<span class="math inline">\(L_{tv}=\sum_{k}\sum_{(i,j)\in R_k}(|P_{i+1,j}-P_{i,j}|+|P_{i,j+1}-P_{i,j}|)\)</span>。其中<span class="math inline">\(R_k\)</span>是第k个身体部位对应的区域。</p></li><li><p>最后是关于人体各部位交界处的点。这些点在多个部位的UV Map上都有坐标。这个loss term旨在计算这些点在所有UV Map上的均值与GT的L1 Loss，以保证交界点处的值也平滑。</p></li><li><p>他们把3D点投影成2D时用了weak perspective projection弱透视投影。</p></li><li><p>速度相较于基于优化的SMPLify-X快得多，从30s降低到了13ms。</p></li><li><p>他们也用了已有的数据集+Occlusion的方式在多个数据集上得到了结果，加遮挡的方式值得学习。</p><p><img data-src="image-20221129175700470.png" alt="image-20221129175700470" style="zoom:50%;"></p></li></ol></li></ul></li></ul><h2 id="monocular-one-stage-regression-of-multiple-3d-people">Monocular, One-stage, Regression of Multiple 3D People</h2><ul><li><p>Idea: 本文的中心思想是用单张图像解析生成图中的多人SMPL模型。</p></li><li><p>Questions:</p><ol type="1"><li>2D label的数据集可以通过EFT（Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation）生成，但那些只有3D joints coordinates但没有SMPL的数据集是怎么用上的？</li></ol></li><li><p>Pipeline:</p><ul><li>首先过一个Backbone提取feature map。</li><li>然后这个feature map被送到三个branch中，它们分别用来预测：<ol type="1"><li>人体中心位置的热力图<span class="math inline">\(C_m\in R^{1\times H \times W}\)</span>。它预测的是2D的人体中心位置。</li><li>相机参数map <span class="math inline">\(A_m\)</span>。这里的<span class="math inline">\(A_m\)</span>形状是<span class="math inline">\(R^{3\times W\times H}\)</span>，它的意义是：如果图片上某个pixel是某个人体中心的话，对应的这个人的相关相机参数。每个pixel预测的相机参数含有三个值，分别是<span class="math inline">\((s, t_x, t_y)\)</span>。<span class="math inline">\(s\)</span>是scale，包含了人的body size和depth。而<span class="math inline">\(t_x\)</span>和<span class="math inline">\(t_y\)</span>则是投影相对图片中心在x和y方向上的移动距离。<span class="math inline">\(t_x, t_y \in (-1,1)\)</span>，被normalized过。</li><li>SMPL map <span class="math inline">\(S_m\)</span>。这个和<span class="math inline">\(A_m\)</span>类似，也是假定每个pixel上都有一个可能的人体中心，然后这个人对应的SMPL参数。形状为<span class="math inline">\(R^{142\times H\times W}\)</span>。对于每个pixel，其中10个参数是shape，132个是pose。</li></ol></li><li>上面整个过程可以概括为：知道人大致在哪-&gt;知道这个位置的人的位移和scaling-&gt;知道这个人的形状和pose。</li></ul></li><li><p>Points:</p><ul><li><p>Collision-aware representation: 基于中心的方法往往使用的是bbox的中心，而这个中心在人体中并没有实际意义，且容易落到人体外部。本文选择了计算未被遮挡的人体躯干的几个点的均值作为中心点。如果躯干点都被遮挡了，就计算全部可见joints的中心点。</p><blockquote><p>We define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips).</p></blockquote></li><li><p>Multiple-human same-center情况的处理：使用类似于正正离子相互排斥的概念，让相近的两个人体中心尽可能远离彼此。</p></li></ul></li></ul><h2 id="exemplar-fine-tuning-for-3d-human-model-fitting-towards-in-the-wild-3d-human-pose-estimation">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation</h2><ul><li><p>Idea: 3D HPE的野外数据和2D不同，很难取得。本文旨在利用已有的大规模2D数据集，通过高质量的优化匹配得到对应的3D labels，进而辅助训练。</p></li><li><p>Questions:</p><ol type="1"><li>本文拟合的是SMPL还是3D骨架<ul><li>拟合的是SMPL，3D坐标是SMPL pose parameter <span class="math inline">\(\theta\)</span> 的计算后的结果。</li></ul></li><li>本文与SMPLify-X有何不同<ul><li>SMPLify-X只是一种单纯的fitting-based method，而本文则是结合了fitting和regression二者。</li></ul></li><li>有哪些文章使用了其公开的3D SMPL数据labels</li><li><strong>prior terms到底长什么样？怎么用到计算循环中的？</strong></li></ol></li><li><p>Pipeline：EFT方法是优化方法与回归方法的结合。具体操作是先正常训练一个神经网络，让它从2D坐标和输入图像预测对应的3D坐标。回归方法到这里就结束了，但EFT这才刚刚开始。EFT针对每一个图片实例，再得到神经网络的预测结果后，再以这个结果作为起点放到fitting method中去进一步优化，并且设定了一个限制就是新的优化后的神经网络权重<span class="math inline">\(w\)</span>与训练好的最佳权重<span class="math inline">\(w^*\)</span>之间差距不能太大。值得注意的是，这个优化后的权重<span class="math inline">\(w^+\)</span>仅服务于这一张图，用完就扔了。</p></li><li><p>Points：</p><ul><li><p>从2D坐标得到3D位置有两种做法：基于优化（fitting）的方法和基于回归（regression）的方法。前者通过构建多个Loss惩罚项、结合多种prior来逐步得到最能满足限制条件的3D坐标，而后者则直接通过深度网络来一步到位预测。</p></li><li><p>而本文提出的EFT则是结合了上面两种方法的方法。</p><figure><img data-src="image-20221205130430922.png" alt="image-20221205130430922"><figcaption aria-hidden="true">image-20221205130430922</figcaption></figure><p>上面三组公式分别是优化法、回归法，及EFT的公式。</p><p><span class="math inline">\(\Theta\)</span>代表SMPL模型的全部参数，其中，<span class="math inline">\(\theta,\beta\)</span>分别代表pose（以铰链夹角的形式展现）和shape参数。<span class="math inline">\(J\)</span>是3D location，<span class="math inline">\(\hat{J}\)</span>是2D location，<span class="math inline">\(\pi\)</span>是Projection matrix，用于把3D坐标投影到图像的2D平面，<span class="math inline">\(M\)</span>用于把SMPL参数转换为absolute 3D坐标。<span class="math inline">\(L_{pose}、L_{shape}\)</span>分别是pose、shape的prior。<span class="math inline">\(\Phi\)</span>是神经网络,<span class="math inline">\(w\)</span>是神经网络的权重。<span class="math inline">\(I\)</span>是输入图片。其中，<span class="math inline">\(\Theta{w}=\Phi_w(I)\)</span></p></li><li><p>优化方法的Loss包含三部分：</p><ol type="1"><li>2D Loss</li><li>Pose Prior</li><li>Shape Prior</li></ol></li><li><p>回归方法的Loss包含三部分：</p><ol type="1"><li><p>2D Loss</p></li><li><p>3D Loss</p></li><li><p>SMPL Loss</p></li></ol></li><li><p>EFT方法的Loss包含三部分：</p><ol type="1"><li>2D Loss</li><li>进一步优化（fitting）后的神经网络参数与预训练网络的神经网络参数的差距Loss</li><li>Shape Prior</li></ol></li><li><p>EFT中有个参数<span class="math inline">\(\gamma\)</span>，这个参数决定了优化后的神经网络参数<span class="math inline">\(w\)</span>与预训练的<span class="math inline">\(w^*\)</span>的相似程度。如果<span class="math inline">\(\gamma\)</span>很大，那么基本这个优化步骤就没用了，因为<span class="math inline">\(w\)</span>不敢跑远，结果将会和神经网络直接预测出来的结果类似。再fitting-based method中，也有这个<span class="math inline">\(\gamma\)</span>，作用相似，如果太大的话，得到的结果将会是pose prior的平均pose。</p></li><li><p>与纯优化方案不同，这里EFT优化的不是<span class="math inline">\(\Theta, \pi\)</span>，而是神经网络的参数<span class="math inline">\(w\)</span>。</p></li><li><p>EFT中没有使用pose prior，因为它假定了神经网络已经隐式地编码了prior。</p></li><li><p>文章中帮我们跑了这些数据集（2D joints -&gt; SMPL）：<code>COCO, MPII, LSPet, PoseTrack, and OCHuman</code> datasets。</p></li><li><p>值得一提的是，由于优化方案有最低可见joints限制，所以不符合要求的人物labels已经被删掉了，所以得到的3D标签并非与原来的一一对应。</p></li></ul></li></ul><h2 id="问题">问题</h2><ul><li>已有的SMPL数据集都是怎么来的，他们是：<ul><li>仿真</li><li>3D扫描</li><li>采集RGB的图片，同时通过mocap得到pose（2D/3D），结合SMPLify-X/<a href="https://files.is.tue.mpg.de/black/papers/MoSh.pdf">MoSh</a>/<a href="https://amass.is.tue.mpg.de/">Mosh++</a>得到SMPL-X模型GT。这里mocap可以是使用marker的，也可以是不使用marker的multi-view vision-based mocap system。</li><li>录多角度/单角度Video，全程逐帧应用SMPL预测，得到GT。过程中需要先用openpose预测得到2D坐标。</li></ul></li><li>最重要的几个SMPL数据集？<ol type="1"><li>AMASS，这个数据集是个整合数据集，把一大堆mocap数据集都给整合进来了，然后用他们自己的Mosh++跑出来SMPL，如果有手部mocap的还会把手部的mesh也进行分别优化。</li></ol></li></ul><h2 id="实践操作">实践/操作</h2><ul><li><p>首先，SMPL有一系列论文，其为：</p><ul><li><p>SMPL：最开始的一篇</p></li><li><p>SMPL+H：加了手部参数的SMPL</p></li><li><p>SMPLX：加了面部和手部参数的SMPL</p><blockquote><p>A new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face.</p></blockquote></li></ul></li></ul><h2 id="reference">Reference</h2><h3 id="papers">Papers</h3><ol type="1"><li><a href="https://smpl.is.tue.mpg.de/">SMPL</a>: <a href="https://smpl.is.tue.mpg.de/download.php">Code</a></li><li><a href="https://smpl-x.is.tue.mpg.de/index.html">SMPL-X</a>: <a href="https://github.com/vchoutas/smplx">Original</a>, <a href="https://github.com/vchoutas/smplify-x">Multi-View</a></li><li><a href="https://mano.is.tue.mpg.de/">Embodied Hands (MANO)</a></li><li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object-Occluded_Human_Shape_and_Pose_Estimation_From_a_Single_Color_CVPR_2020_paper.pdf">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</a></li></ol><h3 id="blogs">Blogs</h3><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/256358005">SMPL论文解读和相关基础知识介绍</a></li><li><a href="https://www.zhihu.com/question/292017089">想弄懂SMPL模型该如何学习</a></li><li><a href="https://zhuanlan.zhihu.com/p/419779571">SMPL-X论文学习笔记</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SMPL主要含有两组参数，一组是人物的体态信息β，一组是人物的姿态信息θ。&lt;/li&gt;
&lt;li&gt;SMPL本身是“相对的”，其只包含人物本身的信息，而不包含任何与环境、相机视角、位置等信息。另外，其mesh点记录的值是相对于模板人类模型标准值的。&lt;/li&gt;
&lt;li&gt;SMPL不包含手、脸和衣服，但后续的其他文章逐渐完善了相应参数。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
    <category term="SMPL" scheme="https://www.miracleyoo.com/tags/SMPL/"/>
    
  </entry>
  
  <entry>
    <title>WSL的克隆，WSL1与WSL2的克隆共存</title>
    <link href="https://www.miracleyoo.com/2022/11/24/wsl-clone/"/>
    <id>https://www.miracleyoo.com/2022/11/24/wsl-clone/</id>
    <published>2022-11-25T01:09:03.000Z</published>
    <updated>2023-04-23T00:12:45.458Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：WSL1和WSL2各有各的特点。WSL2支持GPU和外接USB硬件设备访问，效率也更高；但一旦涉及到Windows内部的文件系统访问，那只能说是慢的不能行，尤其是运行涉及到大量文件操作的脚本时。但是我用WSL很久了，也有了不少个性化设置，安装了很多Library。如果重头再来一遍，耗时耗力且可能某些内容与之前的环境不兼容。于是想法来了：能不能直接把已有的WSL克隆一份，并升级为WSL2，或者相反？</p><span id="more"></span><h2 id="具体操作">具体操作</h2><ol type="1"><li><p>导出已有的WSL Distribution到一个tar压缩包。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--export</span> &lt;distribution name&gt; &lt;export file name&gt;</span><br></pre></td></tr></table></figure><p>例如如果你安装的是Ubuntu，想把它导出到当前目录，文件名为<code>ubuntu.tar</code>，则：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--export</span> Ubuntu ubuntu.tar</span><br></pre></td></tr></table></figure></li><li><p>导出之后，自然就是导入了。选好导入后的压缩包准备解压的位置，然后：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--import</span> &lt;new distribution name&gt; &lt;install location&gt; &lt;export file name&gt;</span><br></pre></td></tr></table></figure><p>如果你想给新的系统命名为<code>Ubuntu-WSL2</code>,新WSL位置放<code>.\Ubuntu-WSL2</code>，则：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--import</span> Ubuntu<span class="literal">-WSL2</span> .\Ubuntu<span class="literal">-WSL2</span> ubuntu.tar</span><br></pre></td></tr></table></figure><p>值得一提的是，如果你想把新的克隆版也装到类似原本WSL的安装位置，这个位置在：</p><p><code>**%USERPROFILE%\AppData\Local\Packages\&lt;distribution package name&gt;\LocalState\ext4.vhdx**</code></p><p>以Ubuntu为例，这个目录是：<code>%USERPROFILE%\AppData\Local\Packages\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\LocalState\ext4.vhdx</code></p></li><li><p>导入完之后，必须要先运行一次，以把新的系统写入Windows Terminal的配置文件中。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> &lt;new distribution name&gt;</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> Ubuntu<span class="literal">-WSL2</span></span><br></pre></td></tr></table></figure><p><img data-src="image-20221124151115574.png" alt="image-20221124151115574" style="zoom:50%;"></p><p>我这里是把WSL2作为Main，克隆后输出的WSL1，所以图示如上。</p></li><li><p>如果你的目的只是单纯的克隆一下WSL，那到第三步其实已经完成了。但如果你还想更改WSL版本，那么：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--set-version</span> &lt;new distribution name&gt; <span class="number">2</span></span><br></pre></td></tr></table></figure><p>这里的<code>2</code>代表WSL2，如果你原本的WSL是WSL2，希望把克隆后的WSL版本降至<code>1</code>，那么把这里的<code>2</code>替换为<code>1</code>即可。如：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--set-version</span> Ubuntu<span class="literal">-WSL1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>此时你会注意到，虽然登录进去了，但是你的user是root，不再是之前你自己的用户名。当然这个不需要慌，很容易就改了。</p><p>只需要在<code>/etc/wsl.conf</code>文件中加上这样两行即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[user]</span><br><span class="line">default=&lt;YOUR_PREVIOUS_USERNAME&gt;</span><br></pre></td></tr></table></figure><p>如果没有这个文件，创建一个该文件即可。</p><p>做完这步之后，你需要彻底关闭该WSL，重新进入后生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --terminate Ubuntu-WSL1</span><br></pre></td></tr></table></figure><p>另外，如果你不想要更改默认用户，但想要某次以该用户进入WSL，可以使用以下命令：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> Ubuntu<span class="literal">-WSL1</span> <span class="literal">-u</span> &lt;YOUR_PREVIOUS_USERNAME&gt;</span><br></pre></td></tr></table></figure></li><li><p>最后，如果你想查看确认当前安装的所有WSL系统版本：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-l</span> <span class="literal">-v</span></span><br></pre></td></tr></table></figure><p>有类似如下这两项说明已经成功：</p><p><img data-src="image-20221124152610692.png" alt="image-20221124152610692" style="zoom:50%;"></p></li></ol><h2 id="reference">Reference</h2><ul><li><a href="https://endjin.com/blog/2021/11/setting-up-multiple-wsl-distribution-instances">Setting up multiple WSL distribution instances</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：WSL1和WSL2各有各的特点。WSL2支持GPU和外接USB硬件设备访问，效率也更高；但一旦涉及到Windows内部的文件系统访问，那只能说是慢的不能行，尤其是运行涉及到大量文件操作的脚本时。但是我用WSL很久了，也有了不少个性化设置，安装了很多Library。如果重头再来一遍，耗时耗力且可能某些内容与之前的环境不兼容。于是想法来了：能不能直接把已有的WSL克隆一份，并升级为WSL2，或者相反？&lt;/p&gt;</summary>
    
    
    
    
    <category term="tech" scheme="https://www.miracleyoo.com/tags/tech/"/>
    
    <category term="wsl" scheme="https://www.miracleyoo.com/tags/wsl/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV与Qt不兼容问题</title>
    <link href="https://www.miracleyoo.com/2022/11/22/opencv-qt-comp/"/>
    <id>https://www.miracleyoo.com/2022/11/22/opencv-qt-comp/</id>
    <published>2022-11-23T01:12:42.000Z</published>
    <updated>2023-04-23T00:13:01.350Z</updated>
    
    <content type="html"><![CDATA[<p>今天测试WSL2时，发现OpenCV弹窗显示图片一直会报错，所以就试着解决了一下。</p><p>报的错是：<code>QObject::moveToThread:&lt;XXXX&gt; Current thread is not the object's thread &lt;XXXX&gt;</code>，且是一大堆错连着。</p><span id="more"></span><p>Google了一会儿，发现很多人指出是安装的qtpy和OpenCV存在兼容性问题，只要将OpenCV Downgrade一下就好了。具体的命令是：</p><p><code>pip install opencv-python==4.1.2.30</code></p><p>这个版本是确认可用的，你也可以试试其附近的版本。</p><p>我用的测试代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> qtpy</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="built_in">print</span>(qtpy.__version__)</span><br><span class="line">img=cv2.imread(<span class="string">&#x27;~/workdir/output.png&#x27;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;test&#x27;</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">1</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><p>如果跑完不报错，问题就解决了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天测试WSL2时，发现OpenCV弹窗显示图片一直会报错，所以就试着解决了一下。&lt;/p&gt;
&lt;p&gt;报的错是：&lt;code&gt;QObject::moveToThread:&amp;lt;XXXX&amp;gt; Current thread is not the object&#39;s thread &amp;lt;XXXX&amp;gt;&lt;/code&gt;，且是一大堆错连着。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="opencv" scheme="https://www.miracleyoo.com/tags/opencv/"/>
    
    <category term="qt" scheme="https://www.miracleyoo.com/tags/qt/"/>
    
  </entry>
  
  <entry>
    <title>新浪微博图床失效应对方法</title>
    <link href="https://www.miracleyoo.com/2022/11/18/sina-img-recover/"/>
    <id>https://www.miracleyoo.com/2022/11/18/sina-img-recover/</id>
    <published>2022-11-19T01:10:32.000Z</published>
    <updated>2023-04-23T00:10:49.690Z</updated>
    
    <content type="html"><![CDATA[<p>浏览之前的笔记时发现Markdown中所有上传至新浪图床上的内容都无法显示了，说实话还是很急的，毕竟内容比较多。首先明确一点，虽然你无法访问这些图了，但它们还在新浪服务器上，只是需要换一个口进去。</p>然后在网上转了一圈，有不少人提这是因为防盗链问题之类，加<meta name="referrer" content="no-referrer"><p>保平安。但问题是我是Markdown，加这个根本无从谈起。网上一些迁移脚本试了一下也并没有效果，因为下载不下来就是下载不下来嘛。</p><span id="more"></span><p>最后在<a href="https://www.31du.cn/blog/sinaimgurl.html">这里</a>发现了答案：</p><blockquote><p>因为<strong>wx1/2/3/4</strong>、<strong>ww1/2/3/4</strong> 与 <strong>ws1/2/3/4</strong> 为前缀的节点目前都被限制了，而 <strong>tva1/2/3/4</strong> 为前缀的节点目前仍可顺利打开。那么<strong>只要把网址前缀中的 wx、ww 与 ws 都改成 tva 系列应该就可以暂时继续使用微博图床的外链图片了</strong>。</p></blockquote><p>如果在Typora里，只需要正则匹配一下<code>ws\d</code>替换为<code>tva1</code>即可，然后在<code>格式-图像-移动所有图片到...</code>选项中将这些图片全部拉到本地即可。如果需要批量处理，可以参考这个<a href="https://github.com/wangshub/markdown-img-backup">脚本</a>，但需要修改一下，把上述替换加进代码中去。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;浏览之前的笔记时发现Markdown中所有上传至新浪图床上的内容都无法显示了，说实话还是很急的，毕竟内容比较多。首先明确一点，虽然你无法访问这些图了，但它们还在新浪服务器上，只是需要换一个口进去。&lt;/p&gt;
然后在网上转了一圈，有不少人提这是因为防盗链问题之类，加
&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;
&lt;p&gt;保平安。但问题是我是Markdown，加这个根本无从谈起。网上一些迁移脚本试了一下也并没有效果，因为下载不下来就是下载不下来嘛。&lt;/p&gt;</summary>
    
    
    
    
    <category term="web" scheme="https://www.miracleyoo.com/tags/web/"/>
    
    <category term="tech" scheme="https://www.miracleyoo.com/tags/tech/"/>
    
    <category term="hexo" scheme="https://www.miracleyoo.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>SLURM集群中多次登录时保存的Tmux Session不见/消失/时隐时现/随机出现的问题</title>
    <link href="https://www.miracleyoo.com/2022/11/17/slurm-tmux/"/>
    <id>https://www.miracleyoo.com/2022/11/17/slurm-tmux/</id>
    <published>2022-11-18T01:17:05.000Z</published>
    <updated>2023-04-23T00:17:38.936Z</updated>
    
    <content type="html"><![CDATA[<p>先说结论，不少Slurm集群使用了两到多个login node来缓解入口机器的压力，均衡负载。这些node每个实际上都可以被看做是一个独立的服务器，只是共享了文件系统和其他一些服务。而Tmux是依赖于node，即这里是login node的。如果换了node，自然也就找不到你之前detach后存档的tmux窗口了。</p><span id="more"></span><p>不幸的是，你能登录到哪个node不说是完全随机的，但是由系统根据负载动态分配的，所以说到底你并无法控制你ssh到cluster后被自动分配到了哪个主机。当然，你可以通过<code>hostname -s</code>或<code>cat /etc/hostname</code>来获知你当前的主机名，以判断你之前save的tmux session是不是在相同的node上。</p><p>至于解决方案，有三种：</p><ol type="1"><li><p>如果你的SLURM集群支持集群内部node相互ssh，那问题就好办了，直接ssh过去就好了。例如，如果你已知自己的tmux session建立在<code>login2</code> node上，而经过上一步的check hostname，你得知当前的主机名称是<code>login1</code>，那解决方案就简单直接了：<code>ssh login2</code>或<code>ssh -Y login2</code>. 来源：<a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/tmux/">Link</a></p><blockquote><p>注意：如果你登录server时候用的是pub和private key file认证，那么很有可能你无法直接在多个login node中跳转。若想做到这点，需要你将自己的private key上传到server，并使用<code>ssh -i &lt;your/id_rsa.pub&gt; loginN</code> 命令登录。</p></blockquote></li><li><p>如果很遗憾，由于安全设定原因，你无法在cluster内从一个node ssh到另一个，那其实解决方案就比较脏了，建议多次登录，开多个tab登录，使用多个可能的机器登录，挑选被分配到<code>login2</code> node的那个session使用吧。据我的经验来看，如果一个机器登不上<code>login2</code>，比起在同一个机器上继续尝试，使用另一台机器试试成功概率更大。当然这也许有点玄学成分了。</p></li><li><p>参考这篇文章，他们的大致做法是每次启动tmux时写一个文件保存当前login node，然后你本地登录的时候对此进行解析，如果不在一个node上就重新来。但看下来他们需要不同的login node有一个专属的名字，可以直接ssh到特定login node。<a href="https://sumner.verhaaklab.com/slurm/how_to_interactive_via_tmux/">Webpage Link</a>，以及<a href="https://github.com/TheJacksonLaboratory/sumnerdocs/tree/master/docs/confs/bin">GitHub Link</a>。</p></li></ol><p>小Tips：如果可能，把tmux放login1上，一般负载没那么高的时候经验来看更倾向于被分配到第一个login node。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;先说结论，不少Slurm集群使用了两到多个login node来缓解入口机器的压力，均衡负载。这些node每个实际上都可以被看做是一个独立的服务器，只是共享了文件系统和其他一些服务。而Tmux是依赖于node，即这里是login node的。如果换了node，自然也就找不到你之前detach后存档的tmux窗口了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="server" scheme="https://www.miracleyoo.com/tags/server/"/>
    
    <category term="slurm" scheme="https://www.miracleyoo.com/tags/slurm/"/>
    
  </entry>
  
  <entry>
    <title>相机校准 相机标定 Intrinsic/Extrinsic Calibration详解 绝对Extrinsic矩阵测得实操 Event Camera/DVS</title>
    <link href="https://www.miracleyoo.com/2022/11/04/camera-calibration/"/>
    <id>https://www.miracleyoo.com/2022/11/04/camera-calibration/</id>
    <published>2022-11-05T05:07:36.000Z</published>
    <updated>2023-04-22T18:35:11.525Z</updated>
    
    <content type="html"><![CDATA[<p>先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关于如何通过实验方法标定得到相对于坐标原点（O点，Origin）的Extrinsic Matrix的讲解，尤其是缺少中文教程。于是我就把参考各种英文资料（主要是一些大学的slides和有关采集Mocap数据集的文献）总结的一些理论推导和实验方法，以及实际能用的代码整理得到了本文。</p><span id="more"></span><p>首先是几张非常重要的Slides，后面都会refer到，可以先自行熟悉下。另外，本篇不是100%从零开始的教程，篇幅限制并无法展开所有的细节，若想深度理解，请自行结合几个大学（CMU，Stanford）相应的Slides一起学习。</p><img data-src="image-20221021152707045-1667622796441-1.png" alt="image-20221021152707045" style="zoom:33%;"><p><img data-src="image-20221021165722788-1667622796442-2.png" alt="image-20221021165722788"></p><h2 id="coordinates"><a class="markdownIt-Anchor" href="#coordinates"></a> Coordinates</h2><ul><li>在整个相机的投影与校准过程中，一共涉及3个坐标系。它们分别是：<ol><li>世界坐标系：以空间中某点为原点建立欧拉坐标系，设定xyz方向后形成的坐标系。</li><li>相机坐标系。该坐标系的原点是相机的焦点。焦点一般在相机内部，也可能落在的相机外部，这取决于focal length。坐标系的指向：x和y就是相平面的横纵坐标方向（相机视角方向），z是与xy平面垂直的方向，亦即镜头指向的前方。</li><li>图像坐标系（也可以分成两个：图像坐标系(m)和像素坐标系(pixel)）。值得注意的一点是每个像素并不是真正的一个点，pixel坐标系所代表的整数值是每个像素点的中心。</li></ol></li><li>考虑功能性，还有一个同质坐标系，用于实际运算。</li></ul><img data-src="image-20221021144051423-1667622796442-3.png" alt="image-20221021144051423" style="zoom: 25%;"><h2 id="intrinsic"><a class="markdownIt-Anchor" href="#intrinsic"></a> Intrinsic</h2><ul><li><p>理想状况下（无Skewness和Distortion），Intrinsic 矩阵Encode的信息有：Focal Length、Image Sensor的长宽（in pixel），每像素代表的米数(pixel/m)，也即相机的分辨率。</p></li><li><p>非理想情况下，Skewness和Distortion也会被放到Intrinsic中。</p></li><li><p>关于当提高/降低分辨率时候的Intrinsic变化：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi><mo separator="true">,</mo><msub><mi>c</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">k,l,c_x,c_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>都要乘以分辨率提高的系数。</p><p><img data-src="image-20221026155238653-1667622796442-4.png" alt="image-20221026155238653"></p></li><li><p>参见Intrinsic的计算过程，由于计算时已经考虑了目标物体深度对成像位置的影响，所以Intrinsic其实是包含了透视(perspective)信息的。</p></li><li><p>Intrinsic可使相机坐标系转化为图片坐标系。</p></li></ul><h2 id="extrinsic"><a class="markdownIt-Anchor" href="#extrinsic"></a> Extrinsic</h2><ul><li><p>Extrinsic 可以看作是两个矩阵写在了一起：旋转矩阵R和平移矩阵T。前三列是R，最后一列是T。 其实，虽然经常写作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>R</mi><mi mathvariant="normal">∣</mi><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[R|T]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">]</span></span></span></span>，但事实上还有一个相似变换S，这个S是个对角线矩阵，对角线上的值为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>S</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>S</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>S</mi><mi>z</mi></msub><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[S_x, S_y, S_z, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>。S直接和R乘在一起，与T无关。</p><img data-src="image-20221026162455644-1667622796442-5.png" alt="image-20221026162455644" style="zoom:50%;"></li><li><p>Extrinsic可使世界坐标系转化为相机坐标系。</p></li></ul><h2 id="skewness-and-distortion"><a class="markdownIt-Anchor" href="#skewness-and-distortion"></a> Skewness and Distortion</h2><ul><li><p>Skewness指的是相机Sensor的两个轴不垂直，即xy之间有一个小夹角。通常这不会发生，但如果有制造方面的问题，这也是可能的。</p></li><li><p>相机的Skewness</p></li><li><p>Skewness的解决方法是把这个夹角找到，并在Intrinsic中反映出来。</p><p><img data-src="image-20221026154924465-1667622796442-6.png" alt="image-20221026154924465"></p></li><li><p>Distortion包含：</p><ul><li><em>Radial Distortion</em> (径向畸变)：</li><li><em>Tangential distortion</em> (切向畸变)：本质上是相平面和相机坐标系存在一个夹角，即“图像Sensor和镜头截面不平行”。</li></ul></li><li><p>关于Distortion的计算：</p><p><img data-src="image-20221026163826434-1667622796442-7.png" alt="image-20221026163826434"></p></li></ul><h2 id="homogeneous-coordinates"><a class="markdownIt-Anchor" href="#homogeneous-coordinates"></a> Homogeneous Coordinates</h2><ul><li>同质坐标的主要用意是把本来在分母上的z（深度）给挪走，以便让投影这个Transformation从non-linear变成Linear。</li><li>注意同质坐标虽然在视觉效果上是在原本的坐标(u,v)或(x,y,z)下面加了一个1，但是实际上这个1在欧式坐标系中并不存在。当我们后面列出方程校准时，应该回到原本的欧式坐标系解。</li></ul><h2 id="imu"><a class="markdownIt-Anchor" href="#imu"></a> IMU</h2><ul><li>IMU输出三个方向角速度和三个轴向加速度的值，使用时也需要校准。</li><li>具体校准方法参见Kalibr和DV，因为我没用上，所以不多展开。</li></ul><h2 id="dvs"><a class="markdownIt-Anchor" href="#dvs"></a> DVS</h2><ul><li>DVS的校准主要分为两种方法：<ul><li>一种是直接用paired的RGB进行校准，毕竟这里的RGB和DVS share同一组透镜。</li><li>如果没有这个RGB，就直接用accumulate的frame做校准。</li></ul></li></ul><h2 id="methods"><a class="markdownIt-Anchor" href="#methods"></a> Methods</h2><h3 id="解方程直接校准p矩阵"><a class="markdownIt-Anchor" href="#解方程直接校准p矩阵"></a> 解方程直接校准P矩阵</h3><ul><li><p>在Paper <em>DHP19: Dynamic Vision Sensor 3D Human Pose Dataset</em>里， 他们采用的方法是：直接在经过Mocap校准的空间中放置一系列Markers，然后在DVS的RGB（APS）输出frame中直接进行手动标注，得到其在image plane中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(u, v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span>坐标， 然后解方程。</p><img data-src="image-20221021155115039-1667622796442-8.png" alt="image-20221021155115039" style="zoom:40%;"></li><li><p>上图中提到了一个点：从投影矩阵计算相机坐标系的原点，即相机的焦点位置的方法：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><msup><mi>Q</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>c</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">C=Q^{-1}c_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。具体的推理其实很简单，主要就靠一个条件公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>C</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">PC=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>，即原点的投影是0。</p></li><li><p>细节上，他们用了38个Marker，并8次改变它们的位置，通过最小平方法解得最接近的11个P中参数值。这里的最小平方法的意义在于通过增加数据点取平均P值来减小误差。其实11组式子就够了，但这里还是用了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>38</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">8\times38\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>个公式，就在于此。</p></li><li><p>具体的最小平方法介绍及代码：<a href="https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html">Link</a></p></li><li><p>这个全矩阵P其实包含了Camera Intrinsic <em>K</em>， Camera Extrinsic <em>RT</em>, 以及Camera Skewness。</p></li><li><p>理论：</p><p><img data-src="image-20221026175034098-1667622796442-9.png" alt="image-20221026175034098"></p><img data-src="image-20221026175051047-1667622796442-10.png" alt="image-20221026175051047" style="zoom:50%;"></li><li><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix using Numpy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svd_calibration</span>(<span class="params">points_3d, points_2d</span>):</span><br><span class="line">    <span class="comment"># points_3d: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># points_2d: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">0</span>] == points_2d.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">1</span>] == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> points_2d.shape[<span class="number">1</span>] == <span class="number">2</span></span><br><span class="line">    num_points = points_3d.shape[<span class="number">0</span>]</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">0</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">1</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">least_square_calibrate_camera_projection_matrix_np</span>(<span class="params">x,y,z,u,v</span>):</span><br><span class="line">    <span class="comment"># x,y,z: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># u,v: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(x) == <span class="built_in">len</span>(y) == <span class="built_in">len</span>(z) == <span class="built_in">len</span>(u) == <span class="built_in">len</span>(v)</span><br><span class="line">    num_points = <span class="built_in">len</span>(x)</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = -u[i] * x[i], -u[i] * y[i], -u[i] * z[i], -u[i]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = -v[i] * x[i], -v[i] * y[i], -v[i] * z[i], -v[i]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br></pre></td></tr></table></figure></li><li><p>注意：SVD这里是用于解决Least Squares Problem的，如果直接用<code>np.linalg.lstsq</code>函数的话（b取全0），会解得一个全0矩阵（因为0永远是一个解）。</p></li><li><p>SVD的解法细节：</p><p><img data-src="image-20221027230723755-1667622796442-11.png" alt="image-20221027230723755"></p></li><li><p>解SVD的时候可以选择把P矩阵右下角<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo separator="true">,</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">P_{(3,4)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.03853em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">3</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span></span></span></span>设为1。不设是homogeneous解法，设了之后是inhomogeneous。</p></li></ul><h3 id="kalibr"><a class="markdownIt-Anchor" href="#kalibr"></a> kalibr</h3><ul><li><p><a href="https://github.com/ethz-asl/kalibr">Link</a></p></li><li><p>Used for:</p><ol><li><strong>Multi-Camera Calibration</strong>: Intrinsic and extrinsic calibration of a camera-systems with non-globally shared overlapping fields of view</li><li><strong>Visual-Inertial Calibration (CAM-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a camera-system along with IMU intrinsic parameters</li><li><strong>Multi-Inertial Calibration (IMU-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a base inertial sensor along with IMU intrinsic parameters (requires 1-aiding camera sensor).</li><li><strong>Rolling Shutter Camera Calibration</strong>: Full intrinsic calibration (projection, distortion and shutter parameters) of rolling shutter cameras.</li></ol></li><li><p>简单说就是主攻多相机/IMU系统。<a href="https://github.com/ethz-asl/kalibr/wiki/multiple-camera-calibration">多个相机</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/Multi-IMU-and-IMU-intrinsic-calibration">多个IMU</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/camera-imu-calibration">相机+IMU</a>等。</p></li><li><p>校准出来的Extrinsic结果并不是相对原点绝对的，而是多个设备间相对的。比如IMU+Cam校准出来的Extrinsic就是IMU相对于Cam坐标的变换。</p><p>引用一段<a href="https://github.com/ethz-asl/kalibr/wiki/yaml-formats">原话</a>：</p><blockquote><ul><li><strong>T_cn_cnm1</strong><br>camera extrinsic transformation, always with respect to the last camera in the chain<br>(e.g. cam1: T_cn_cnm1 = T_c1_c0, takes cam0 to cam1 coordinates)</li><li><strong>T_cam_imu</strong><br>IMU extrinsics: transformation from IMU to camera coordinates (T_c_i)</li><li><strong>timeshift_cam_imu</strong><br>timeshift between camera and IMU timestamps in seconds (t_imu = t_cam + shift)</li></ul></blockquote></li><li><p>综上所述，Kalibr并不是满足我们需求的校准方案。</p></li></ul><h3 id="dv-calibration"><a class="markdownIt-Anchor" href="#dv-calibration"></a> DV Calibration</h3><ul><li><a href="https://inivation.gitlab.io/dv/dv-docs/docs/tutorial-calibration/">Tutorial Link</a>, <a href="https://gitlab.com/inivation/dv/dv-imu-cam-calibration">Code Link</a></li><li>单个多个DVS都可以。</li><li>基于Kalibr的方案。</li><li>对于单个DVS，校准主要进行的是undistortion，且可以在校准后直接应用于相机后续的图像，让后面的record都不再有失真。</li><li>这里的校准可以有效应对之前Upal教授提出的扭曲问题，应在后续操作中应用。</li></ul><h3 id="opencv-camera-calibration"><a class="markdownIt-Anchor" href="#opencv-camera-calibration"></a> OpenCV Camera Calibration</h3><ul><li><p>这个校准会使用chessboard，而关于3d坐标，他们用了棋盘上两个相邻的点的实际距离是已知的这个特性（因为打印的标准棋盘，间距是固定的，如30mm），来提供相应的3D坐标信息。</p></li><li><p>这个校准会分别输出Intrinsic matrix (mtx), rotation matrix (R, rvecs), translation matrix (T, tvecs), Distortion coefficients (dist)。这些输出可以直接被用来纠偏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate camera matrixes</span></span><br><span class="line">ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-<span class="number">1</span>], <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">&#x27;left12.jpg&#x27;</span>)</span><br><span class="line">h,  w = img.shape[:<span class="number">2</span>]</span><br><span class="line">newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), <span class="number">1</span>, (w,h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># undistort</span></span><br><span class="line">dst = cv.undistort(img, mtx, dist, <span class="literal">None</span>, newcameramtx)</span><br><span class="line"><span class="comment"># crop the image</span></span><br><span class="line">x, y, w, h = roi</span><br><span class="line">dst = dst[y:y+h, x:x+w]</span><br><span class="line">cv.imwrite(<span class="string">&#x27;calibresult.png&#x27;</span>, dst)</span><br></pre></td></tr></table></figure></li><li><p>关于OpenCV校准出来的Extrinsic Matrix，由于世界坐标系必定有一个原点，所以它们也是毫无疑问有一个原点的。但这个世界坐标系原点实际上只有参考意义（第一张校准图的左上角棋盘点），并无法直接使用。同时，相机坐标系的原点是相机的焦点，而这个焦点也是几乎不可预知和测量位置的（它可能在相机内部或外部，但校准并不会告诉你这个点位置，所以你也无法通过直接测量相机O点和实际世界O点之间的相对位置来纠正Extrinsic。）</p><blockquote><p><a href="https://www.appsloveworld.com/opencv/100/91/opencv-camera-calibration-world-coordinate-system-origin">Link</a>: I believe it used to be the position of the top-left corner of the checkerboard in the first calibration image, but it may have changed. You can visualized it by writing a few lines of code that project point (0,0,0) (in calibrated scene coordinates) in all the calibration images, then plotting its projected image coordinates on top of the image themselves.</p><p>You should really not depend on it being anything meaningful, and instead locate a separate feature in 3D and roto-translate the reference frame to it after calibration.</p></blockquote></li><li><p>实际上，不要想通过OpenCV的校准来直接得到有实际意义的Extrinsic，若想得到，请自行用前面提到的Method 1来实际label一些已知3D坐标的Markers对应的2D点，用Least Squares解得。</p></li><li><p>但是，OpenCV的校准可以提供有效的Distortion Coefficient和Intrinsic，并可直接被用于畸变补偿。</p></li></ul><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><h3 id="blogswebsites"><a class="markdownIt-Anchor" href="#blogswebsites"></a> Blogs/Websites</h3><ul><li><a href="****https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html****">Least Squares Regression in Python</a></li><li><a href="https://math.stackexchange.com/questions/974193/why-does-svd-provide-the-least-squares-and-least-norm-solution-to-a-x-b">Why does SVD provide the least squares and least norm solution to 𝐴𝑥=𝑏?</a></li><li><a href="https://math.stackexchange.com/questions/772039/how-does-the-svd-solve-the-least-squares-problem">How does the SVD solve the least squares problem?</a></li><li><a href="https://www.quora.com/What-is-the-real-world-coordinate-system-camera-calibration-refer-to-in-computer-vision">What is the “real world coordinate system” camera calibration refer to in computer vision?</a></li><li><a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">numpy linalg svd</a></li><li><a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html">OpenCV Camera Calibration</a></li></ul><h3 id="slides"><a class="markdownIt-Anchor" href="#slides"></a> Slides</h3><ul><li><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud810/slides/Unit-3/3C-L3.pdf#fromHistory">Udacity CS4495/6495 Introduction to Computer Vision 3C-L3 Calibrating cameras</a></li><li><a href="https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf">CMU - Camera Matrix</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture2_camera_models.pdf">Stanford - Lecture 2</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture3_camera_calibration.pdf">Stanford - Lecture 3</a></li></ul><h3 id="papers"><a class="markdownIt-Anchor" href="#papers"></a> Papers</h3><ul><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">A Flexible New Technique for Camera Calibration</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关于如何通过实验方法标定得到相对于坐标原点（O点，Origin）的Extrinsic Matrix的讲解，尤其是缺少中文教程。于是我就把参考各种英文资料（主要是一些大学的slides和有关采集Mocap数据集的文献）总结的一些理论推导和实验方法，以及实际能用的代码整理得到了本文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="camera calibration" scheme="https://www.miracleyoo.com/tags/camera-calibration/"/>
    
    <category term="camera matrix" scheme="https://www.miracleyoo.com/tags/camera-matrix/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Lighting 完全攻略</title>
    <link href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/"/>
    <id>https://www.miracleyoo.com/2021/03/11/pytorch-lightning/</id>
    <published>2021-03-12T04:09:57.000Z</published>
    <updated>2021-03-12T22:05:55.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h2><p>Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？</p><span id="more"></span><p>于是我第二次发现了Pytorch-Lightning。</p><p>真香。</p><p>但是问题还是来了。这个框架并没有因为香而变得更加易学。官网的教程很丰富，可以看出来开发者们在努力做了。但是很多相连的知识点都被分布在了不同的版块里，还有一些核心的理解要点并没有被强调出来，而是小字带过，这让我想做一个普惠的教程，包含所有我在学习过程中认为重要的概念，好用的参数，一些注意点、坑点，大量的示例代码段和一些核心问题的集中讲解。</p><p>最后，第三部分提供了一个我总结出来的易用于大型项目、容易迁移、易于复用的模板，有兴趣的可以去<a href="https://github.com/miracleyoo/pytorch-lightning-template">GitHub</a>试用。</p><h2 id="crucial"><a class="markdownIt-Anchor" href="#crucial"></a> Crucial</h2><ul><li><p>Pytorch-Lighting 的一大特点是把模型和系统分开来看。模型是像Resnet18， RNN之类的纯模型， 而系统定义了一组模型如何相互交互，如GAN（生成器网络与判别器网络）、Seq2Seq（Encoder与Decoder网络）和Bert。同时，有时候问题只涉及一个模型，那么这个系统则可以是一个通用的系统，用于描述模型如何使用，并可以被复用到很多其他项目。</p></li><li><p>Pytorch-Lighting 的核心设计思想是“自给自足”。每个网络也同时包含了如何训练、如何测试、优化器定义等内容。</p></li></ul><p><img data-src="plres.png" alt="img"></p><h2 id="推荐使用方法"><a class="markdownIt-Anchor" href="#推荐使用方法"></a> 推荐使用方法</h2><p>这一部分放在最前面，因为全文内容太长，如果放后面容易忽略掉这部分精华。</p><p>Pytorch-Lightning 是一个很好的库，或者说是pytorch的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。缺点也很明显，这个包需要学习和理解的内容还是挺多的，或者换句话说，很重。如果直接按照官方的模板写代码，小型project还好，如果是大型项目，有复数个需要调试验证的模型和数据集，那就不太好办，甚至更加麻烦了。经过几天的摸索和调试，我总结出了下面这样一套好用的模板，也可以说是对Pytorch-Lightning的进一步抽象。</p><p>欢迎大家尝试这一套代码风格，如果用习惯的话还是相当方便复用的，也不容易半道退坑。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">|-data</span><br><span class="line">|-__init__.py</span><br><span class="line">|-data_interface.py</span><br><span class="line">|-xxxdataset1.py</span><br><span class="line">|-xxxdataset2.py</span><br><span class="line">|-...</span><br><span class="line">|-model</span><br><span class="line">|-__init__.py</span><br><span class="line">|-model_interface.py</span><br><span class="line">|-xxxmodel1.py</span><br><span class="line">|-xxxmodel2.py</span><br><span class="line">|-...</span><br><span class="line">|-main.py</span><br></pre></td></tr></table></figure><p>如果对每个模型直接上plmodule，对于已有项目、别人的代码等的转换将相当耗时。另外，这样的话，你需要给每个模型都加上一些相似的代码，如<code>training_step</code>，<code>validation_step</code>。显然，这并不是我们想要的，如果真的这样做，不但不易于维护，反而可能会更加杂乱。同理，如果把每个数据集类都直接转换成pl的DataModule，也会面临相似的问题。基于这样的考量，我建议使用上述架构：</p><ul><li><p>主目录下只放一个<code>main.py</code>文件。</p></li><li><p><code>data</code>和<code>modle</code>两个文件夹中放入<code>__init__.py</code>文件，做成包。这样方便导入。两个<code>init</code>文件分别是：</p><ul><li><code>from .data_interface import DInterface</code></li><li><code>from .model_interface import MInterface</code></li></ul></li><li><p>在<code>data_interface </code>中建立一个<code>class DInterface(pl.LightningDataModule):</code>用作所有数据集文件的接口。<code>__init__()</code>函数中import相应Dataset类，<code>setup()</code>进行实例化，并老老实实加入所需要的的<code>train_dataloader</code>, <code>val_dataloader</code>, <code>test_dataloader</code>函数。这些函数往往都是相似的，可以用几个输入args控制不同的部分。</p></li><li><p>同理，在<code>model_interface </code>中建立<code>class MInterface(pl.LightningModule):</code>类，作为模型的中间接口。<code>__init__()</code>函数中import相应模型类，然后老老实实加入<code>configure_optimizers</code>, <code>training_step</code>, <code>validation_step</code>等函数，用一个接口类控制所有模型。不同部分使用输入参数控制。</p></li><li><p><code>main.py</code>函数只负责：</p><ul><li>定义parser，添加parse项。</li><li>选好需要的<code>callback</code>函数们。</li><li>实例化<code>MInterface</code>, <code>DInterface</code>, <code>Trainer</code>。</li></ul><p>完事。</p></li></ul><h2 id="lightning-module"><a class="markdownIt-Anchor" href="#lightning-module"></a> Lightning Module</h2><h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">主页面</a></p><ul><li><p>三个核心组件：</p><ul><li>模型</li><li>优化器</li><li>Train/Val/Test步骤</li></ul></li><li><p>数据流伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">    out = training_step(batch)</span><br><span class="line">    outs.append(out)</span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><p>等价Lightning代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    prediction = ...</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        <span class="comment"># do something with these</span></span><br></pre></td></tr></table></figure><p>我们需要做的，就是像填空一样，填这些函数。</p></li></ul><h3 id="组件与函数"><a class="markdownIt-Anchor" href="#组件与函数"></a> 组件与函数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api">API页面</a></p><ul><li><p>一个Pytorch-Lighting 模型必须含有的部件是：</p><ul><li><p><code>init</code>: 初始化，包括模型和系统的定义。</p></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.training_step"><code>training_step(self, batch, batch_idx)</code></a>: 即每个batch的处理函数。</p><blockquote><p>参数：</p><ul><li><strong>batch</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> | (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …) | [<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …]) – The output of your <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. A tensor, tuple or list.</li><li><strong>batch_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Integer displaying index of this batch</li><li><strong>optimizer_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – When using multiple optimizers, this argument will also be present.</li><li><strong>hiddens</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>) – Passed in if <a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.params.truncated_bptt_steps"><code>truncated_bptt_steps</code></a> &gt; 0.</li></ul><p>返回值：Any of.</p><ul><li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> - The loss tensor</li><li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li><li><code>None</code> - Training will skip to the next batch</li></ul></blockquote><p>返回值无论如何也需要有一个loss量。如果是字典，要有这个key。没loss这个batch就被跳过了。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y, z = batch</span><br><span class="line">    out = self.encoder(x)</span><br><span class="line">    loss = self.loss(out, x)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple optimizers (e.g.: GANs)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, optimizer_idx</span>):</span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># do training_step with encoder</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># do training_step with decoder</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Truncated back-propagation through time</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, hiddens</span>):</span><br><span class="line">    <span class="comment"># hiddens are the hidden states from the previous truncated backprop step</span></span><br><span class="line">    ...</span><br><span class="line">    out, hiddens = self.lstm(data, hiddens)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;hiddens&#x27;</span>: hiddens&#125;</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization"><code>configure_optimizers</code></a>: 优化器定义，返回一个优化器，或数个优化器，或两个List（优化器，Scheduler）。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most cases</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    opt = Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">return</span> opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> generator_opt, disriminator_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [generator_opt, disriminator_opt], [discriminator_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sched = &#123;<span class="string">&#x27;scheduler&#x27;</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">                 <span class="string">&#x27;interval&#x27;</span>: <span class="string">&#x27;step&#x27;</span>&#125;  <span class="comment"># called after each training step</span></span><br><span class="line">    dis_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sched, dis_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: dis_opt, <span class="string">&#x27;frequency&#x27;</span>: n_critic&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: gen_opt, <span class="string">&#x27;frequency&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li></ul></li><li><p>可以指定的部件有：</p><ul><li><code>forward</code>: 和正常的<code>nn.Module</code>一样，用于inference。内部调用时：<code>y=self(batch)</code></li><li><code>training_step_end</code>: 只在使用多个node进行训练且结果涉及如softmax之类需要全部输出联合运算的步骤时使用该函数。同理，<code>validation_step_end</code>/<code>test_step_end</code>。</li><li><code>training_epoch_end</code>:<ul><li>在一个训练epoch结尾处被调用。</li><li>输入参数：一个List，List的内容是前面<code>training_step()</code>所返回的每次的内容。</li><li>返回：None</li></ul></li><li><code>validation_step(self, batch, batch_idx)</code>/<code>test_step(self, batch, batch_idx)</code>:<ul><li>没有返回值限制，不一定非要输出一个<code>val_loss</code>。</li></ul></li><li><code>validation_epoch_end</code>/<code>test_epoch_end</code>:</li></ul></li><li><p>工具函数有：</p><ul><li><p><code>freeze</code>：冻结所有权重以供预测时候使用。仅当已经训练完成且后面只测试时使用。</p></li><li><p><code>print</code>：尽管自带的<code>print</code>函数也可以使用，但如果程序运行在分布式系统时，会打印多次。而使用<code>self.print()</code>则只会打印一次。</p></li><li><p><code>log</code>：像是TensorBoard等log记录器，对于每个log的标量，都会有一个相对应的横坐标，它可能是batch number或epoch number。而<code>on_step</code>就表示把这个log出去的量的横坐标表示为当前batch，而<code>on_epoch</code>则表示将log的量在整个epoch上进行累积后log，横坐标为当前epoch。</p><table><thead><tr><th>LightningMoule Hook</th><th>on_step</th><th>on_epoch</th><th>prog_bar</th><th>logger</th></tr></thead><tbody><tr><td>training_step</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_step_end</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_epoch_end</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_epoch_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr></tbody></table><p><code>*</code> also applies to the test loop</p><blockquote><p>参数</p><ul><li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – key name</li><li><strong>value</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>) – value name</li><li><strong>prog_bar</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the progress bar</li><li><strong>logger</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the logger</li><li><strong>on_step</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs at this step. None auto-logs at the training_step but not validation/test_step</li><li><strong>on_epoch</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</li><li><strong>reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – reduction function over step values for end of epoch. Torch.mean by default</li><li><strong>tbptt_reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – function to reduce on truncated back prop</li><li><strong>tbptt_pad_token</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – token to use for padding</li><li><strong>enable_graph</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, will not auto detach the graph</li><li><strong>sync_dist</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, reduces the metric across GPUs/TPUs</li><li><strong>sync_dist_op</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>, <a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>]) – the op to sync across GPUs/TPUs</li><li><strong>sync_dist_group</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>]) – the ddp group</li></ul></blockquote></li><li><p><code>log_dict</code>：和<code>log</code>函数唯一的区别就是，<code>name</code>和<code>value</code>变量由一个字典替换。表示同时log多个值。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;acc&#x27;</span>: acc, ..., <span class="string">&#x27;metric_n&#x27;</span>: metric_n&#125;</span><br><span class="line">self.log_dict(values)</span><br></pre></td></tr></table></figure></li><li><p><code>save_hyperparameters</code>：储存<code>init</code>中输入的所有超参。后续访问可以由<code>self.hparams.argX</code>方式进行。同时，超参表也会被存到文件中。</p></li></ul></li><li><p>函数内建变量：</p><ul><li><code>device</code>：可以使用<code>self.device</code>来构建设备无关型tensor。如：<code>z = torch.rand(2, 3, device=self.device)</code>。</li><li><code>hparams</code>：含有所有前面存下来的输入超参。</li><li><code>precision</code>：精确度。常见32和16。</li></ul></li></ul><h3 id="要点"><a class="markdownIt-Anchor" href="#要点"></a> 要点</h3><ul><li>如果准备使用DataParallel，在写<code>training_step</code>的时候需要调用forward函数，<code>z=self(x)</code></li></ul><h3 id="模板"><a class="markdownIt-Anchor" href="#模板"></a> 模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">any_extra_hook</span>(<span class="params">...</span>)</span><br></pre></td></tr></table></figure><h2 id="trainer"><a class="markdownIt-Anchor" href="#trainer"></a> Trainer</h2><h3 id="基础使用"><a class="markdownIt-Anchor" href="#基础使用"></a> 基础使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightningModule()</span><br><span class="line"></span><br><span class="line">trainer = Trainer()</span><br><span class="line">trainer.fit(model, train_dataloader, val_dataloader)</span><br></pre></td></tr></table></figure><p>如果连<code>validation_step</code>都没有，那<code>val_dataloader</code>也就算了。</p><h3 id="伪代码与hooks"><a class="markdownIt-Anchor" href="#伪代码与hooks"></a> 伪代码与hooks</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks">Hooks页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">...</span>):</span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> global_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># prepare data is called on GLOBAL_ZERO only</span></span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> gpu/tpu <span class="keyword">in</span> gpu/tpus:</span><br><span class="line">        train_on_device(model.copy())</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_on_device</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># setup is called PER DEVICE</span></span><br><span class="line">    setup()</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_pretrain_routine_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_loop()</span><br><span class="line"></span><br><span class="line">    teardown()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>():</span><br><span class="line">    on_train_epoch_start()</span><br><span class="line">    train_outs = []</span><br><span class="line">    <span class="keyword">for</span> train_batch <span class="keyword">in</span> train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----- train_step methods -------</span></span><br><span class="line">        out = training_step(batch)</span><br><span class="line">        train_outs.append(out)</span><br><span class="line"></span><br><span class="line">        loss = out.loss</span><br><span class="line"></span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line">        optimizer_step()</span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end training epoch</span></span><br><span class="line">    logs = training_epoch_end(outs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_loop</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line">    val_outs = []</span><br><span class="line">    <span class="keyword">for</span> val_batch <span class="keyword">in</span> val_dataloader():</span><br><span class="line">        on_validation_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------- val step methods -------</span></span><br><span class="line">        out = validation_step(val_batch)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up for train</span></span><br><span class="line">    model.train()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="推荐参数"><a class="markdownIt-Anchor" href="#推荐参数"></a> 推荐参数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags">参数介绍（附视频）</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api">类定义与默认参数</a></p><ul><li><p><code>default_root_dir</code>：默认存储地址。所有的实验变量和权重全部会被存到这个文件夹里面。推荐是，每个模型有一个独立的文件夹。每次重新训练会产生一个新的<code>version_x</code>子文件夹。</p></li><li><p><code>max_epochs</code>：最大训练周期数。<code>trainer = Trainer(max_epochs=1000)</code></p></li><li><p><code>min_epochs</code>：至少训练周期数。当有Early Stop时使用。</p></li><li><p><code>auto_scale_batch_size</code>：在进行任何训练前自动选择合适的batch size。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (no scaling of batch size)</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run batch size scaling, result overrides hparams.batch_size</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="string">&#x27;binsearch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the batch size</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>auto_select_gpus</code>：自动选择合适的GPU。尤其是在有GPU处于独占模式时候，非常有用。</p></li><li><p><code>auto_lr_find</code>：自动找到合适的初始学习率。使用了该<a href="https://arxiv.org/abs/1506.01186">论文</a>的技术。当且仅当执行<code>trainer.tune(model)</code>代码时工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run learning rate finder, results override hparams.learning_rate</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run learning rate finder, results override hparams.my_lr_arg</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="string">&#x27;my_lr_arg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the lr</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>precision</code>：精确度。正常是32，使用16可以减小内存消耗，增大batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 16-bit precision</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">16</span>, gpus=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>val_check_interval</code>：进行Validation测试的周期。正常为1，训练1个epoch测试4次是0.25，每1000 batch测试一次是1000。</p><blockquote><ul><li>use (float) to check within a training epoch：此时这个值为一个epoch的百分比。每百分之多少测试一次。</li><li>use (int) to check every n steps (batches)：每多少个batch测试一次。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set 4 times during a training epoch</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set every 1000 training batches</span></span><br><span class="line"><span class="comment"># use this when using iterableDataset and your dataset has no length</span></span><br><span class="line"><span class="comment"># (ie: production cases with streaming data)</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#gpus"><code>gpus</code></a>：控制使用的GPU数。当设定为None时，使用cpu。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (ie: train on CPU)</span></span><br><span class="line">trainer = Trainer(gpus=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># int: train on 2 gpus</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list: train on GPUs 1, 4 (by bus ordering)</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;1, 4&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1: train on all gpus</span></span><br><span class="line">trainer = Trainer(gpus=-<span class="number">1</span>)</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;-1&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine with num_nodes to train on multiple GPUs across nodes</span></span><br><span class="line"><span class="comment"># uses 8 gpus in total</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>, num_nodes=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train only on GPUs 1 and 4 across nodes</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>], num_nodes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#limit-train-batches"><code>limit_train_batches</code></a>：使用训练数据的百分比。如果数据过多，或正在调试，可以使用这个。值的范围为0~1。同样，有<code>limit_test_batches</code>，<code>limit_val_batches</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 25% of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 10 batches of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run"><code>fast_dev_run</code></a>：bool量。如果设定为true，会只执行一个batch的train, val 和 test，然后结束。仅用于debug。</p><blockquote><p>Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like <code>LearningRateLogger</code> and runs for only 1 epoch</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 1 train, val, test batch and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 7 train, val, test batches and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="number">7</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="fit函数"><a class="markdownIt-Anchor" href="#fit函数"></a> .fit()函数</h3><p><code>Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None)</code>：输入第一个量一定是model，然后可以跟一个LigntningDataModule或一个普通的Train DataLoader。如果定义了Val step，也要有Val DataLoader。</p><blockquote><p>参数</p><ul><li><strong>datamodule</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule"><code>LightningDataModule</code></a>]) – A instance of <code>LightningDataModule</code>.</li><li><strong>model</strong> (<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule"><code>LightningModule</code></a>) – Model to fit.</li><li><strong>train_dataloader</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>]) – A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.</li><li><strong>val_dataloaders</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>, <a href="https://docs.python.org/3/library/typing.html#typing.List"><code>List</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>], <a href="https://docs.python.org/3/library/constants.html#None"><code>None</code></a>]) – Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped</li></ul></blockquote><h3 id="其他要点"><a class="markdownIt-Anchor" href="#其他要点"></a> 其他要点</h3><ul><li><code>.test()</code>若非直接调用，不会运行。<code>trainer.test()</code></li><li><code>.test()</code>会自动load最优模型。</li><li><code>model.eval()</code> and <code>torch.no_grad()</code> 在进行测试时会被自动调用。</li><li>默认情况下，<code>Trainer()</code>运行于CPU上。</li></ul><h3 id="使用样例"><a class="markdownIt-Anchor" href="#使用样例"></a> 使用样例</h3><ol><li>手动添加命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">hparams</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer(gpus=hparams.gpus)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gpus&#x27;</span>, default=<span class="literal">None</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="2"><li>自动添加所有<code>Trainer</code>会用到的命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="3"><li>混合式，既使用<code>Trainer</code>相关参数，又使用一些自定义参数，如各种模型超参：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> LightningModule, Trainer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">32</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hidden_dim&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">128</span>)</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="所有参数"><a class="markdownIt-Anchor" href="#所有参数"></a> 所有参数</h3><blockquote><p><code>Trainer.``__init__</code>(<em>logger=True</em>, <em>checkpoint_callback=True</em>, <em>callbacks=None</em>, <em>default_root_dir=None</em>, <em>gradient_clip_val=0</em>, <em>process_position=0</em>, <em>num_nodes=1</em>, <em>num_processes=1</em>, <em>gpus=None</em>, <em>auto_select_gpus=False</em>, <em>tpu_cores=None</em>, <em>log_gpu_memory=None</em>, <em>progress_bar_refresh_rate=None</em>, <em>overfit_batches=0.0</em>, <em>track_grad_norm=- 1</em>, <em>check_val_every_n_epoch=1</em>, <em>fast_dev_run=False</em>, <em>accumulate_grad_batches=1</em>, <em>max_epochs=None</em>, <em>min_epochs=None</em>, <em>max_steps=None</em>, <em>min_steps=None</em>, <em>limit_train_batches=1.0</em>, <em>limit_val_batches=1.0</em>, <em>limit_test_batches=1.0</em>, <em>limit_predict_batches=1.0</em>, <em>val_check_interval=1.0</em>, <em>flush_logs_every_n_steps=100</em>, <em>log_every_n_steps=50</em>, <em>accelerator=None</em>, <em>sync_batchnorm=False</em>, <em>precision=32</em>, <em>weights_summary=‘top’</em>, <em>weights_save_path=None</em>, <em>num_sanity_val_steps=2</em>, <em>truncated_bptt_steps=None</em>, <em>resume_from_checkpoint=None</em>, <em>profiler=None</em>, <em>benchmark=False</em>, <em>deterministic=False</em>, <em>reload_dataloaders_every_epoch=False</em>, <em>auto_lr_find=False</em>, <em>replace_sampler_ddp=True</em>, <em>terminate_on_nan=False</em>, <em>auto_scale_batch_size=False</em>, <em>prepare_data_per_node=True</em>, <em>plugins=None</em>, <em>amp_backend=‘native’</em>, <em>amp_level=‘O2’</em>, <em>distributed_backend=None</em>, <em>move_metrics_to_cpu=False</em>, <em>multiple_trainloader_mode=‘max_size_cycle’</em>, <em>stochastic_weight_avg=False</em>)</p></blockquote><h3 id="log和return-loss到底在做什么"><a class="markdownIt-Anchor" href="#log和return-loss到底在做什么"></a> Log和return loss到底在做什么</h3><p>To add a training loop use the training_step method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitClassifier</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = model</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">         x, y = batch</span><br><span class="line">         y_hat = self.model(x)</span><br><span class="line">         loss = F.cross_entropy(y_hat, y)</span><br><span class="line">         <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>无论是<code>training_step</code>，还是<code>validation_step</code>，<code>test_step</code>返回值都是<code>loss</code>。返回的loss会被用一个list收集起来。</li></ul><p>Under the hood, Lightning does the following (pseudocode):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put model in train mode</span></span><br><span class="line">model.train()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    loss = training_step(batch)</span><br><span class="line">    losses.append(loss.detach())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><h4 id="training-epoch-level-metrics"><a class="markdownIt-Anchor" href="#training-epoch-level-metrics"></a> Training epoch-level metrics</h4><p>If you want to calculate epoch-level metrics and log them, use the <code>.log</code> method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logs metrics for each training_step,</span></span><br><span class="line">    <span class="comment"># and the average across the epoch, to the progress bar and logger</span></span><br><span class="line">    self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>如果在<code>x_step</code>函数中使用了<code>.log()</code>函数，那么这个量将会被逐步记录下来。每一个<code>log</code>出去的变量都会被记录下来，每一个<code>step</code>会集中生成一个字典dict，而每个epoch都会把这些字典收集起来，形成一个字典的list。</li></ul><p>The .log object automatically reduces the requested metrics across the full epoch. Here’s the pseudocode of what it does under the hood:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">epoch_metric = torch.mean(torch.stack([x[<span class="string">&#x27;train_loss&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> outs]))</span><br></pre></td></tr></table></figure><h4 id="train-epoch-level-operations"><a class="markdownIt-Anchor" href="#train-epoch-level-operations"></a> Train epoch-level operations</h4><p>If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    preds = ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;other_stuff&#x27;</span>: preds&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">   <span class="keyword">for</span> pred <span class="keyword">in</span> training_step_outputs:</span><br><span class="line">       <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><p>The matching pseudocode is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><h2 id="datamodule"><a class="markdownIt-Anchor" href="#datamodule"></a> DataModule</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html">主页面</a></p><h3 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h3><ul><li><p>首先，这个<code>DataModule</code>和之前写的Dataset完全不冲突。前者是后者的一个包装，并且这个包装可以被用于多个torch Dataset 中。在我看来，其最大的作用就是把各种train/val/test划分、DataLoader初始化之类的重复代码通过包装类的方式得以被简单的复用。</p></li><li><p>具体作用项目：</p><ul><li>Download instructions：下载</li><li>Processing instructions：处理</li><li>Split instructions：分割</li><li>Train dataloader：训练集Dataloader</li><li>Val dataloader(s)：验证集Dataloader</li><li>Test dataloader(s)：测试集Dataloader</li></ul></li><li><p>其次，<code>pl.LightningDataModule</code>相当于一个功能加强版的torch Dataset，加强的功能包括：</p><ul><li><code>prepare_data(self)</code>：<ul><li>最最开始的时候，进行一些无论GPU有多少只要执行一次的操作，如写入磁盘的下载操作、分词操作(tokenize)等。</li><li>这里是一劳永逸式准备数据的函数。</li><li>由于只在单线程中调用，不要在这个函数中进行<code>self.x=y</code>似的赋值操作。</li><li>但如果是自己用而不是给大众分发的话，这个函数可能并不需要调用，因为数据提前处理好就好了。</li></ul></li><li><code>setup(self, stage=None)</code>：<ul><li>实例化数据集（Dataset），并进行相关操作，如：清点类数，划分train/val/test集合等。</li><li>参数<code>stage</code>用于指示是处于训练周期(<code>fit</code>)还是测试周期(<code>test</code>)，其中，<code>fit</code>周期需要构建train和val两者的数据集。</li><li>setup函数不需要返回值。初始化好的train/val/test set直接赋值给self即可。</li></ul></li><li><code>train_dataloader/val_dataloader/test_dataloader</code>：<ul><li>初始化<code>DataLoader</code>。</li><li>返回一个DataLoader量。</li></ul></li></ul></li></ul><h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MNISTDataModule</span>(pl.LightningDataModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir: <span class="built_in">str</span> = <span class="string">&#x27;./&#x27;</span>, batch_size: <span class="built_in">int</span> = <span class="number">64</span>, num_workers: <span class="built_in">int</span> = <span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line"></span><br><span class="line">        self.transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.dims is returned when you call dm.size()</span></span><br><span class="line">        <span class="comment"># Setting default dims here because we know them.</span></span><br><span class="line">        <span class="comment"># Could optionally be assigned dynamically in dm.setup()</span></span><br><span class="line">        self.dims = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        self.num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># download</span></span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;fit&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">            self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assign test dataset for use in dataloader(s)</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;test&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mnist_test = MNIST(self.data_dir, train=<span class="literal">False</span>, transform=self.transform)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)</span><br></pre></td></tr></table></figure><h3 id="要点-2"><a class="markdownIt-Anchor" href="#要点-2"></a> 要点</h3><ul><li>若在DataModule中定义了一个<code>self.dims</code> 变量，后面可以调用<code>dm.size()</code>获取该变量。</li></ul><h2 id="saving-and-loading"><a class="markdownIt-Anchor" href="#saving-and-loading"></a> Saving and Loading</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html">主页面</a></p><h3 id="saving"><a class="markdownIt-Anchor" href="#saving"></a> Saving</h3><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint">ModelCheckpoint</a>: 自动储存的callback module。默认情况下training过程中只会自动储存最新的模型与相关参数，而用户可以通过这个module自定义。如观测一个<code>val_loss</code>的量，并储存top 3好的模型，且同时储存最后一个epoch的模型，等等。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span></span><br><span class="line">checkpoint_callback = ModelCheckpoint(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    filename=<span class="string">&#x27;sample-mnist-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">    save_top_k=<span class="number">3</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    save_last=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer(gpus=<span class="number">1</span>, max_epochs=<span class="number">3</span>, progress_bar_refresh_rate=<span class="number">20</span>, callbacks=[checkpoint_callback])</span><br></pre></td></tr></table></figure></li><li><p>另外，也可以手动存储checkpoint: <code>trainer.save_checkpoint(&quot;example.ckpt&quot;)</code></p></li><li><p><code>ModelCheckpoint</code> Callback中，如果<code>save_weights_only =True</code>，那么将会只储存模型的权重（相当于<code>model.save_weights(filepath)</code>），反之会储存整个模型（相当于<code>model.save(filepath)</code>）。</p></li></ul><h3 id="loading"><a class="markdownIt-Anchor" href="#loading"></a> Loading</h3><ul><li><p>load一个模型，包括它的weights、biases和超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightingModule.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.learning_rate)</span><br><span class="line"><span class="comment"># prints the learning_rate you used in this checkpoint</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">y_hat = model(x)</span><br></pre></td></tr></table></figure></li><li><p>load模型时替换一些超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.l1 = nn.Linear(self.hparams.in_dim, self.hparams.out_dim)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># if you train and save the model like this it will use these values when loading</span></span><br><span class="line"><span class="comment"># the weights. But you can overwrite this</span></span><br><span class="line">LitModel(in_dim=<span class="number">32</span>, out_dim=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=32, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=128, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH, in_dim=<span class="number">128</span>, out_dim=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p>完全load训练状态：load包括模型的一切，以及和训练相关的一切参数，如<code>model, epoch, step, LR schedulers, apex</code>等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = LitModel()</span><br><span class="line">trainer = Trainer(resume_from_checkpoint=<span class="string">&#x27;some/path/to/my_checkpoint.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically restores model, epoch, step, LR schedulers, apex, etc...</span></span><br><span class="line">trainer.fit(model)</span><br></pre></td></tr></table></figure></li></ul><h2 id="callbacks"><a class="markdownIt-Anchor" href="#callbacks"></a> Callbacks</h2><ul><li>Callback 是一个自包含的程序，可以与训练流程交织在一起，而不会污染主要的研究逻辑。</li><li>Callback 并非只会在epoch结尾调用。pytorch-lightning 提供了数十个hook（接口，调用位置）可供选择，也可以自定义callback，实现任何想实现的模块。</li><li>推荐使用方式是，随问题和项目变化的操作，这些函数写到lightning module里面，而相对独立，相对辅助性的，需要复用的内容则可以定义单独的模块，供后续方便地插拔使用。</li></ul><h3 id="callbacks推荐"><a class="markdownIt-Anchor" href="#callbacks推荐"></a> Callbacks推荐</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks">内建Callbacks</a></p><ul><li><p><code>EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)</code>：根据某个值，在数个epoch没有提升的情况下提前停止训练。</p><blockquote><p>参数：</p><ul><li><strong>monitor</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – quantity to be monitored. Default: <code>'early_stop_on'</code>.</li><li><strong>min_delta</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><code>float</code></a>) – minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: <code>0.0</code>.</li><li><strong>patience</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – number of validation epochs with no improvement after which training will be stopped. Default: <code>3</code>.</li><li><strong>verbose</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – verbosity mode. Default: <code>False</code>.</li><li><strong>mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – one of <code>'min'</code>, <code>'max'</code>. In <code>'min'</code> mode, training will stop when the quantity monitored has stopped decreasing and in <code>'max'</code> mode it will stop when the quantity monitored has stopped increasing.</li><li><strong>strict</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – whether to crash the training if monitor is not found in the validation metrics. Default: <code>True</code>.</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">trainer = Trainer(callbacks=[early_stopping])</span><br></pre></td></tr></table></figure></li><li><p><code>ModelCheckpoint</code>：见上文<strong>Saving and Loading</strong>.</p></li><li><p><code>PrintTableMetricsCallback</code>：在每个epoch结束后打印一份结果整理表格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pl_bolts.callbacks <span class="keyword">import</span> PrintTableMetricsCallback</span><br><span class="line"></span><br><span class="line">callback = PrintTableMetricsCallback()</span><br><span class="line">trainer = pl.Trainer(callbacks=[callback])</span><br><span class="line">trainer.fit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># at the end of every epoch it will print</span></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss│train_loss│val_loss│epoch</span></span><br><span class="line"><span class="comment"># ──────────────────────────────</span></span><br><span class="line"><span class="comment"># 2.2541470527648926│2.2541470527648926│2.2158432006835938│0</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="logging"><a class="markdownIt-Anchor" href="#logging"></a> Logging</h2><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html">Logging</a>：Logger默认是TensorBoard，但可以指定各种主流Logger<a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#supported-loggers">框架</a>，<a href="http://xn--Comet-gv5i.ml">如Comet.ml</a>，MLflow，Netpune，或直接CSV文件。可以同时使用复数个logger。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> loggers <span class="keyword">as</span> pl_loggers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(</span><br><span class="line">    save_dir=os.getcwd(),</span><br><span class="line">    version=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;lightning_logs&#x27;</span></span><br><span class="line">)</span><br><span class="line">trainer = Trainer(logger=tb_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or use the same format as others</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One Logger</span></span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=comet_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save code snapshot</span></span><br><span class="line">logger = pl_loggers.TestTubeLogger(<span class="string">&#x27;logs/&#x27;</span>, create_git_tag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Logger</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=[tb_logger, comet_logger])</span><br></pre></td></tr></table></figure><p>默认情况下，每50个batch log一次，可以通过调整参数</p></li><li><p>如果想要log输出非scalar（标量）的内容，如图片，文本，直方图等等，可以直接调用<code>self.logger.experiment.add_xxx()</code>来实现所需操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># the logger you used (in this case tensorboard)</span></span><br><span class="line">    tensorboard = self.logger.experiment</span><br><span class="line">    tensorboard.add_image()</span><br><span class="line">    tensorboard.add_histogram(...)</span><br><span class="line">    tensorboard.add_figure(...)</span><br></pre></td></tr></table></figure></li><li><p>使用log：如果是TensorBoard，那么：<code>tensorboard --logdir ./lightning_logs</code>。在Jupyter Notebook中，可以使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start tensorboard.</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir lightning_logs/</span><br></pre></td></tr></table></figure><p>在行内打开TensorBoard。</p></li><li><p>小技巧：如果在局域网内开启了TensorBoard，加上flag <code>--bind_all</code>即可使用主机名访问：</p><p><code>tensorboard --logdir lightning_logs --bind_all</code> -&gt; <code>http://SERVER-NAME:6006/</code></p></li></ul><h3 id="同时使用tensorboard和csv-logger"><a class="markdownIt-Anchor" href="#同时使用tensorboard和csv-logger"></a> 同时使用TensorBoard和CSV Logger</h3><p>如果同时使用两个Logger，PL会有睿智操作：如果保存根目录相同，他们会依次建立两个version文件夹，令人窒息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> TensorBoardLogger, CSVLogger</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_loggers</span>():</span><br><span class="line">    loggers = []</span><br><span class="line">    loggers.append(TensorBoardLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tb&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CSVLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;csv&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CometLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tt&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loggers</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_callbacks</span>(<span class="params">logger</span>):</span><br><span class="line">    callbacks = []</span><br><span class="line">    dirpath = <span class="string">f&#x27;lightning_logs/<span class="subst">&#123;logger.name&#125;</span>/version_<span class="subst">&#123;logger.version&#125;</span>/checkpoints&#x27;</span></span><br><span class="line">    callbacks.append(ModelCheckpoint(</span><br><span class="line">        dirpath=dirpath,</span><br><span class="line">        monitor=<span class="string">&#x27;loss_epoch&#x27;</span>,</span><br><span class="line">        filename=<span class="string">&#x27;&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">        save_top_k=<span class="number">3</span>,</span><br><span class="line">        mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">        save_last=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> callbacks</span><br><span class="line"></span><br><span class="line">loggers = load_loggers()</span><br><span class="line">callbacks = load_callbacks(loggers[<span class="number">0</span>])</span><br><span class="line">trainer = pl.Trainer(logger=loggers, callbacks=callbacks)</span><br></pre></td></tr></table></figure><h2 id="transfer-learning"><a class="markdownIt-Anchor" href="#transfer-learning"></a> Transfer Learning</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html#transfer-learning">主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImagenetTransferLearning</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a pretrained resnet</span></span><br><span class="line">        backbone = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">        num_filters = backbone.fc.in_features</span><br><span class="line">        layers = <span class="built_in">list</span>(backbone.children())[:-<span class="number">1</span>]</span><br><span class="line">        self.feature_extractor = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use the pretrained model to classify cifar-10 (10 image classes)</span></span><br><span class="line">        num_target_classes = <span class="number">10</span></span><br><span class="line">        self.classifier = nn.Linear(num_filters, num_target_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            representations = self.feature_extractor(x).flatten(<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(representations)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h2 id="关于device操作"><a class="markdownIt-Anchor" href="#关于device操作"></a> 关于device操作</h2><p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># good (self is LightningModule)</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br></pre></td></tr></table></figure><p>For tensors that need to be model attributes, it is best practice to register them as buffers in the modules’s <code>__init__</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">self.t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;t&quot;</span>, torch.rand(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>前面两段是教程中的文本。然而实际上有一个暗坑：</p><p>如果你使用了一个中继的<code>pl.LightningModule</code>，而这个module里面实例化了某个普通的<code>nn.Module</code>，而这个模型中又需要内部生成一些tensor，比如图片每个通道的mean，std之类，那么如果你从<code>pl.LightningModule</code>中pass一个<code>self.device</code>，实际上在一开始这个<code>self.device</code>永远是<code>cpu</code>。所以如果你在调用的<code>nn.Module</code>的<code>__init__()</code>中初始化，使用<code>to(device)</code>或干脆什么都不用，结果就是它永远都在<code>cpu</code>上。</p><p>但是，经过实验，虽然<code>pl.LightningModule</code>在<code>__init__()</code>阶段<code>self.device</code>还是<code>cpu</code>，当进入了<code>training_step()</code>之后，就迅速变为了<code>cuda</code>。所以，对于子模块，最佳方案是，使用一个<code>forward</code>中传入的量，如<code>x</code>，作为一个reference变量，用<code>type_as</code>函数将在模型中生成的tensor都放到和这个参考变量相同的device上即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RDNFuse</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_norm_func</span>(<span class="params">self, ref</span>):</span><br><span class="line">        self.mean = torch.tensor(np.array(self.mean_sen), dtype=torch.float32).type_as(ref)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mean&#x27;</span>):</span><br><span class="line">            self.init_norm_func(x)</span><br></pre></td></tr></table></figure><h2 id="关于limit_train_batches选项"><a class="markdownIt-Anchor" href="#关于limit_train_batches选项"></a> 关于<code>limit_train_batches</code>选项</h2><p>这里涉及到一个问题，就是每个epoch使用部分数据而非全部时，程序将会怎么工作。</p><blockquote><p>The shuffling happens when the iterator is created. In the case of the for loop, that happens just before the for loop starts. You can create the iterator manually with:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator gets created, the data has been shuffled at this point.</span></span><br><span class="line">data_iterator = <span class="built_in">iter</span>(namesTrainLoader)</span><br></pre></td></tr></table></figure><blockquote><p>By default the data loader uses <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler"><code>torch.utils.data.RandomSampler</code></a> if you set <code>shuffle=True</code> (without providing your own sampler). Its implementation is very straight forward and you can see where the data is shuffled when the iterator is created by looking at the <a href="https://github.com/pytorch/pytorch/blob/f3e620ee83f080283445aa1a7242d40e30eb6a7f/torch/utils/data/sampler.py#L103-L107"><code>RandomSampler.__iter__</code></a> method:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">    <span class="keyword">if</span> self.replacement:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br></pre></td></tr></table></figure><blockquote><p>The return statement is the important part, where the shuffling takes place. It simply creates a random permutation of the indices.</p><p>That means you will see your entire dataset every time you fully consume the iterator, just in a different order every time. Therefore there is no data lost (not including cases with <code>drop_last=True</code>) and your model will see all data at every epoch.</p></blockquote><p>总结下来，如果使用了<code>shuffle=True</code>选项，那么即使每次都不跑完整个epoch，你还是有机会见到所有的数据的。数据集的shuffle发生在<code>iter</code>被创建的时候，在我们一般的代码中，也就是内层for循环开始时。但如果你没有选择<code>shuffle=True</code>，那你将永远只能看到你设定的前面N个数据。</p><h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ul><li><p><code>pl.seed_everything(1234)</code>：对所有相关的随机量固定种子。</p></li><li><p>使用LR Scheduler时候，不用自己<code>.step()</code>。它也被Trainer自动处理了。<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html?highlight=scheduler#">Optimization 主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single optimizer</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">        loss = model.training_step(batch, batch_idx, ...)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Multiple optimizers</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">     <span class="keyword">for</span> opt <span class="keyword">in</span> optimizers:</span><br><span class="line">        disable_grads_for_other_optimizers()</span><br><span class="line">        train_step(opt)</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">     scheduler.step()</span><br></pre></td></tr></table></figure></li><li><p>关于划分train和val集合的方法。与PL无关，但很常用，两个例子：</p><ol><li><code>random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</code></li><li>如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br></pre></td></tr></table></figure><p>Parameters：</p><ul><li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</li><li><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</li><li><strong>generator</strong> (<a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>Generator</em></a>) – Generator used for the random permutation.</li></ul></li><li><p>如果使用了<code>PrintTableMetricsCallback</code>，那么<code>validation_step</code>不要return内容，否则会炸。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面&lt;/h2&gt;
&lt;p&gt;Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
    <category term="pytorch-lightning" scheme="https://www.miracleyoo.com/tags/pytorch-lightning/"/>
    
  </entry>
  
  <entry>
    <title>Analyzing Geography Data (Beginners&#39; Tutorial)</title>
    <link href="https://www.miracleyoo.com/2021/02/03/satellite-basic/"/>
    <id>https://www.miracleyoo.com/2021/02/03/satellite-basic/</id>
    <published>2021-02-04T02:18:19.000Z</published>
    <updated>2021-03-12T23:19:59.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据来源"><a class="markdownIt-Anchor" href="#数据来源"></a> 数据来源</h2><h3 id="卫星类型"><a class="markdownIt-Anchor" href="#卫星类型"></a> 卫星类型</h3><ol><li><p>Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi>m</mi><mo>×</mo><mn>10</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">10m \times 10m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn><mi>m</mi><mo>×</mo><mn>20</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">20m \times 20m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>60</mn><mi>m</mi><mo>×</mo><mn>60</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">60m \times 60m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，bands中心波长从442.3nm到2185.7nm。</p><img data-src="image-20201213154533413.png" alt="image-20201213154533413" style="zoom:33%;"><span id="more"></span></li><li><p>Planet：这个数据源提供<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>m</mi><mo>×</mo><mn>3</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">3m \times 3m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathnormal">m</span></span></span></span>的4 bands MSI image。分别是R、G、B、NIR。</p><ul><li>这个数据源似乎只对美国国内的院校机构开放，不过不是很确定，需要的可以试试。</li></ul><p><img data-src="image-20201213150111581.png" alt="Planet Specification"></p></li><li><p>RapidEyes</p></li></ol><h3 id="sentinel-2-data-source"><a class="markdownIt-Anchor" href="#sentinel-2-data-source"></a> Sentinel-2 Data Source</h3><ol><li><p>USGS Earth Explorer: <a href="https://earthexplorer.usgs.gov/">Link</a>. It support downloading data within five years. And it not only support sentinel-2 data, but also contains many other satellite datasource.</p><p><img data-src="image-20201227143234319.png" alt="image-20201227143234319"></p></li><li><p>Copernicus Open Access Hub: <a href="https://scihub.copernicus.eu/dhus/#/home">Link</a>. It supports only one year’s old data. You can request the older data, but not guarante the fetch time.</p><p><img data-src="image-20201227145324164.png" alt="image-20201227145324164"></p></li><li><p>Amazon AWS Sentinel-2 Service: <a href="https://registry.opendata.aws/sentinel-2/">Link</a></p></li></ol><h3 id="planet-data-source"><a class="markdownIt-Anchor" href="#planet-data-source"></a> Planet Data Source</h3><ul><li>官网：<a href="https://www.planet.com/">Link</a></li></ul><h3 id="specification"><a class="markdownIt-Anchor" href="#specification"></a> Specification</h3><ol><li><a href="https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/msi-instrument">Sentinel-2 MultiSpectral Instrument (MSI) Overview</a></li><li><a href="https://www.planet.com/products/satellite-imagery/files/Planet_Combined_Imagery_Product_Specs_December2017.pdf">Sentinel-2 Specification Doc</a></li><li><a href="https://www.planet.com/products/planet-imagery/">Planet Specification</a></li></ol><h2 id="软件与python包简介"><a class="markdownIt-Anchor" href="#软件与python包简介"></a> 软件与Python包简介</h2><ul><li><a href="http://www.gdal.org/">GDAL</a> –&gt; Fundamental package for processing vector and raster data formats (many modules below depend on this). Used for raster processing.</li><li><a href="https://github.com/mapbox/rasterio">Rasterio</a> –&gt; Clean and fast and geospatial raster I/O for Python. <a href="https://rasterio.readthedocs.io/en/latest/quickstart.html">Guidebook</a>.</li><li><a href="http://geopandas.org/#description">Geopandas</a> –&gt; Working with geospatial data in Python made easier, combines the capabilities of pandas and shapely.</li><li><a href="http://toblerity.org/shapely/manual.html">Shapely</a> –&gt; Python package for manipulation and analysis of planar geometric objects (based on widely deployed <a href="https://trac.osgeo.org/geos/">GEOS</a>).</li><li><a href="https://pypi.python.org/pypi/Fiona">Fiona</a> –&gt; Reading and writing spatial data (alternative for geopandas).</li><li><a href="https://pypi.python.org/pypi/pyproj?">Pyproj</a> –&gt; Performs cartographic transformations and geodetic computations (based on <a href="http://trac.osgeo.org/proj">PROJ.4</a>).</li><li><a href="https://pysal.readthedocs.org/en/latest/">Pysal</a> –&gt; Library of spatial analysis functions written in Python.</li><li><a href="http://geopy.readthedocs.io/en/latest/">Geopy</a> –&gt; Geocoding library: coordinates to address &lt;-&gt; address to coordinates.</li><li><a href="http://geo.holoviews.org/index.html">GeoViews</a> –&gt; Interactive Maps for the web.</li><li><a href="https://networkx.github.io/documentation/networkx-1.10/overview.html">Networkx</a> –&gt; Network analysis and routing in Python (e.g. Dijkstra and A* -algorithms), see <a href="http://gis.stackexchange.com/questions/65056/is-it-possible-to-route-shapefiles-using-python-and-without-arcgis-qgis-or-pgr">this post</a>.</li><li><a href="http://scitools.org.uk/cartopy/docs/latest/index.html">Cartopy</a> –&gt; Make drawing maps for data analysis and visualisation as easy as possible.</li><li><a href="http://docs.scipy.org/doc/scipy/reference/spatial.html">Scipy.spatial</a> –&gt; Spatial algorithms and data structures.</li><li><a href="http://toblerity.org/rtree/">Rtree</a> –&gt; Spatial indexing for Python for quick spatial lookups.</li><li><a href="http://www.rsgislib.org/index.html#python-documentation">RSGISLib</a> –&gt; Remote Sensing and GIS Software Library for Python.</li><li><a href="https://python-geojson.readthedocs.io/en/latest/">python-geojson</a>-&gt; Deal with geojson format files.</li></ul><h2 id="sentinel-2-大气校正"><a class="markdownIt-Anchor" href="#sentinel-2-大气校正"></a> Sentinel-2 大气校正</h2><p>Sentinel-2一般有两种standard，一个是A级别，一个是C级别。C级别的数据是你可以从任意网站上下载到的数据，它没有经过大气校正，是粗数据，每个区块的反射率可能有不同，不适合直接用作深度学习数据。</p><p>经过Sen2Cor软件校正后可以得到A级别的数据。</p><p>Sen2Cor是欧空局发布的一个软件，它即可以作为SNAP的插件安装，也可以作为独立命令行软件使用。推荐后者，更为快速、稳定，尤其适用于大量数据时，可以用脚本批处理。<a href="http://step.esa.int/main/snap-supported-plugins/sen2cor/sen2cor_v2-8/">官网链接</a></p><h3 id="查询坐标映射"><a class="markdownIt-Anchor" href="#查询坐标映射"></a> 查询坐标映射</h3><ul><li>地球是圆的。</li><li>卫星上拍的一张矩形照片所对应的区域并非是矩形的。</li><li>使用卫星图片时要先将其映射到二维展开的坐标系中。</li><li>整个地球被分为了许多预先订好的区域。</li><li>每个区域有一个编号。</li><li>编号可以在<a href="http://epsg.io/">EPSG</a>网站查询。</li></ul><h2 id="snap"><a class="markdownIt-Anchor" href="#snap"></a> SNAP</h2><ul><li><p>欧空局自己用作处理Sentinel-2的软件。<a href="https://step.esa.int/main/download/snap-download/">Link</a></p></li><li><p>只用来处理Sentinel-2，尤其是预处理，包括校正，reprojection，粗crop，统一各个bands分辨率等等，非常好用。因为是亲儿子，所以甚至可以直接读取Sentinel-2每个文件的压缩包，总之十分便利。</p></li><li><p>比较古老，interface有年代感，功能也相较于其他软件比较局限。</p></li><li><p>推荐用作第一步预处理。</p></li><li><p>使用流程：<code>读取-&gt;剪裁-&gt;correction-&gt;resize-&gt;reprojection-&gt;导出</code>。</p></li><li><p>大部分需要用到的功能都在<code>Raster-&gt;Geometric</code>中</p></li><li><p>经过尝试、搜索、确认，SNAP并不提供便利的选定区域截图，或是依据shapefile剪裁，所以目前最佳的方法是，先通过zoom地图和改变窗口大小确保需要的部分大致在view的可视范围内，然后右键，选择<code>Spatial Subset from View</code>，然后可视区域即可被剪裁。注意，这只是粗剪裁，所以尽量多包括一点，也不要少任何一部分。剪裁后的内容可以后续在ArcGIS Pro中进一步处理。</p><img data-src="image-20201227155911702.png" alt="image-20201227155911702" style="zoom:40%;"></li><li><p>展示图：</p><img data-src="image-20201227155421761.png" alt="image-20201227155421761" style="zoom:33%;"><p><img data-src="image-20201227155558109.png" alt="image-20201227155558109"></p></li></ul><h2 id="arcgis-pro"><a class="markdownIt-Anchor" href="#arcgis-pro"></a> ArcGIS Pro</h2><ul><li><p>我能找到的最强大的可用于卫星图像处理的软件。</p></li><li><p>专业，美观，支持format多，有着强大的raster functions。</p></li><li><p>版权软件，下载之前先确认自己学校或公司是否提供License。</p></li><li><p>有时导出raster会出现随机bug，导致：导出可能是纯黑的图片，导出部分没有按照期望剪裁等。很恶心，但似乎也没有更好的选择。</p></li><li><p>解决导出问题：</p><ol><li>关闭导出窗口，重新操作，多试几次。可以解决绝大部分问题。</li><li>和剪裁、mask等有关的问题可以先使用raster function进行这些操作，再直接导出前面操作的结果layer。</li></ol></li><li><p>关于剪裁后的卫星图片和原始图片有着明显亮度对比度区别的问题：</p><ol><li><p>首先，这不是一个bug，而是一个feature。。。实际上的卫星图都很暗沉的，所以软件原生提供一个“显示方法”的函数，调整图片曲线，使得显示的图片比较亮，容易看清楚细节。然而，剪裁后的图片有着不一样的统计值，所以在有些显示函数下，显示的结果和剪裁前结果不同。</p><img data-src="image-20201227155342029.png" alt="image-20201227155342029" style="zoom:33%;"></li><li><p>但是不用担心，因为导出时候并不会考虑这个显示函数。即：剪裁前后的导出图像数值是相同的。</p></li></ol></li><li><p>下面是两张展示图：</p></li></ul><p><img data-src="image-20201227155051608.png" alt="image-20201227155051608"></p><p><img data-src="image-20201227155259214.png" alt="image-20201227155259214"></p><h2 id="qgis"><a class="markdownIt-Anchor" href="#qgis"></a> QGIS</h2><ul><li><p>开源软件，支持很多插件，比如直接下载Sentinel-2数据，Sen2Cor等。<a href="https://qgis.org/en/site/">Link</a></p></li><li><p>支持很多format的数据。</p></li><li><p>问题是，不稳定，效率低，容易崩溃（软件&amp;心态），甚至左侧Explorer遇到大文件夹都要经常转很久才能进去，或是干脆就直接转崩了。我怀疑他们要分析每个文件夹所有文件之后再显示列表。。。总之，慎重。</p><p><img data-src="image-20201227160344273.png" alt="image-20201227160344273"></p></li></ul><h2 id="pythonic-method"><a class="markdownIt-Anchor" href="#pythonic-method"></a> Pythonic Method</h2><p>虽然前面介绍了几个软件，但是说实话，处理一两个可以，批量处理几十几百甚至几十万就有点力不从心了。所以最后还是狠下心研究了一遍Python处理这些数据的方法，写出了一批适合我项目用的函数。不一定适合所有人，但可以作为参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd</span><br><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Point, LineString, Polygon</span><br><span class="line"><span class="keyword">import</span> rasterio <span class="keyword">as</span> rio</span><br><span class="line"><span class="keyword">from</span> rasterio.mask <span class="keyword">import</span> mask</span><br><span class="line"><span class="keyword">from</span> rasterio.warp <span class="keyword">import</span> calculate_default_transform, reproject, Resampling</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    piece = np.array([i.bounds <span class="keyword">for</span> i <span class="keyword">in</span> shp[<span class="string">&#x27;geometry&#x27;</span>]])</span><br><span class="line">    minx = piece[:,<span class="number">0</span>].<span class="built_in">min</span>()</span><br><span class="line">    miny = piece[:,<span class="number">1</span>].<span class="built_in">min</span>()</span><br><span class="line">    maxx = piece[:,<span class="number">2</span>].<span class="built_in">max</span>()</span><br><span class="line">    maxy = piece[:,<span class="number">3</span>].<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">return</span> minx, miny, maxx, maxy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">edge_length</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the x and y edge length for a ceratin shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(maxx-minx,<span class="number">3</span>), <span class="built_in">round</span>(maxy-miny,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shape2latlong</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the shapefile unit from meters/other units to lat/long.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.to_crs(epsg=<span class="number">4326</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_latlong</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the latitude-longitude bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shp = shape2latlong(shp)</span><br><span class="line">    <span class="keyword">return</span> bbox(shp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_polygon</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Return the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> Polygon([(minx, miny), (minx, maxy), (maxx,maxy), (maxx, miny)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_polygon</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Merge a shapefile to one single polygon.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.dissolve(by=<span class="string">&#x27;Id&#x27;</span>).iloc[<span class="number">0</span>].geometry</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">polygon2geojson</span>(<span class="params">polygon</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn a polygon to a geojson format string.</span></span><br><span class="line"><span class="string">        This is used for rasterio mask operation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(polygon) == Polygon:</span><br><span class="line">        polygon = gpd.GeoSeries(polygon)</span><br><span class="line">    <span class="keyword">return</span> [json.loads(polygon.to_json())[<span class="string">&#x27;features&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;geometry&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sen2rgb</span>(<span class="params">img, scale=<span class="number">30</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the 12 channel float32 format sentinel-2 images to a RGB uint8 image. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (img[(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>),]/<span class="number">256</span>*scale).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cropbyshp</span>(<span class="params">raster, shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Crop a raster using a shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Reproject the shapefile to the same crs of raster.</span></span><br><span class="line">    shp = shp.to_crs(&#123;<span class="string">&quot;init&quot;</span>: <span class="built_in">str</span>(raster.crs)&#125;)</span><br><span class="line">    <span class="comment"># Compute the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line">    bbpoly = bbox_polygon(shp)</span><br><span class="line">    <span class="comment"># Execute the mask operation.</span></span><br><span class="line">    out_img, out_transform = mask(dataset=raster, shapes=polygon2geojson(bbpoly), crop=<span class="literal">True</span>, all_touched=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> out_img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_raster</span>(<span class="params">raster, path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Write a created raster object to file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(</span><br><span class="line">        path,</span><br><span class="line">        <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        **raster.meta</span><br><span class="line">    ) <span class="keyword">as</span> dst:</span><br><span class="line">        dst.write(raster.read())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sen_reproject</span>(<span class="params">src, dst_crs, out_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Reproject a raster to a new CRS coordinate, and save it in out_path.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        src: Input raster.</span></span><br><span class="line"><span class="string">        dst_crs: Target CRS. String.</span></span><br><span class="line"><span class="string">        out_path: The path of the output file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    transform, width, height = calculate_default_transform(</span><br><span class="line">        src.crs, dst_crs, src.width, src.height, *src.bounds)</span><br><span class="line">    kwargs = src.meta.copy()</span><br><span class="line">    kwargs.update(&#123;</span><br><span class="line">        <span class="string">&#x27;crs&#x27;</span>: dst_crs,</span><br><span class="line">        <span class="string">&#x27;transform&#x27;</span>: transform,</span><br><span class="line">        <span class="string">&#x27;width&#x27;</span>: width,</span><br><span class="line">        <span class="string">&#x27;height&#x27;</span>: height</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(out_path, <span class="string">&#x27;w&#x27;</span>, **kwargs) <span class="keyword">as</span> dst:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, src.count + <span class="number">1</span>):</span><br><span class="line">            reproject(</span><br><span class="line">                source=rio.band(src, i),</span><br><span class="line">                destination=rio.band(dst, i),</span><br><span class="line">                src_transform=src.transform,</span><br><span class="line">                src_crs=src.crs,</span><br><span class="line">                dst_transform=transform,</span><br><span class="line">                dst_crs=dst_crs,</span><br><span class="line">                resampling=Resampling.cubic)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mask_A_by_B</span>(<span class="params">A, B</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Generate a mask from B, and applied it to A.</span></span><br><span class="line"><span class="string">        All 0 values are excluded.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mask = B.<span class="built_in">sum</span>(axis=<span class="number">0</span>)&gt;<span class="number">1e-3</span></span><br><span class="line">    masked_A = mask*A</span><br><span class="line">    <span class="keyword">return</span> masked_A</span><br></pre></td></tr></table></figure><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ol><li><a href="https://automating-gis-processes.github.io/2016/course-info.html">Python GIS 超完整教程</a></li><li><a href="https://zia207.github.io/geospatial-python.io/">Professor Zia’s Personal Website</a></li><li><a href="https://blog.csdn.net/sinat_28853941/article/details/78511167">sentinel-2数据下载 大气校正 转ENVI格式</a></li><li><a href="https://blog.csdn.net/lidahuilidahui/article/details/102765420">03-SNAP处理Sentinel-2 L2A级数据（一）</a></li><li><a href="https://clouds.eos.ubc.ca/~phil/courses/atsc301/html/rasterio_demo.html">UBC Course Notebook</a></li><li><a href="https://zhuanlan.zhihu.com/p/31010043">利用Sen2cor对哨兵2号（Sentinel-2）L1C多光谱数据进行辐射定标和大气校正</a></li><li><a href="https://sentinelhub-py.readthedocs.io/en/latest/">Documentation of Sentinel Hub Python package</a></li><li><a href="https://github.com/sentinel-hub/sentinelhub-py">sentinelhub-py</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;数据来源&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#数据来源&quot;&gt;&lt;/a&gt; 数据来源&lt;/h2&gt;
&lt;h3 id=&quot;卫星类型&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#卫星类型&quot;&gt;&lt;/a&gt; 卫星类型&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;10m &#92;times 10m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;20m &#92;times 20m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;60&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;60&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;60m &#92;times 60m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，bands中心波长从442.3nm到2185.7nm。&lt;/p&gt;
&lt;img data-src=&quot;image-20201213154533413.png&quot; alt=&quot;image-20201213154533413&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="satellite" scheme="https://www.miracleyoo.com/tags/satellite/"/>
    
  </entry>
  
  <entry>
    <title>全自动种子追番系统</title>
    <link href="https://www.miracleyoo.com/2021/02/01/seed-anime-system/"/>
    <id>https://www.miracleyoo.com/2021/02/01/seed-anime-system/</id>
    <published>2021-02-02T02:53:44.000Z</published>
    <updated>2023-04-22T20:44:14.485Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：你喜欢的番剧更新了。是你喜欢的字幕组的高清资源。里面没有放肆的圣光和暗牧。尽管它也许没在国内放送。你可以在任何设备上观看它，并且可以无缝切换。电视，手机，iPad，电脑都没有问题。它很快。</p><p>这将是本篇介绍的全自动全平台订阅追番系统。</p><p>出于各种原因，有许多番剧在B站并找不到。即使开了大会员，从国外驾梯回国，还是无法看到一些喜欢的内容。但同时，各大动漫种子站却提供了几乎无所不包的资源，其中包括新番。但依靠种子追番最常见的问题就是难以track，经常会忘记更新，而且每次都需要经过一个<code>搜索-下载种子-下载-整理-观看</code>的过程，确实还是很劝退的。如何搭建一个易于配置、足够简单、全自动化抓取、下载、串流的追番系统，便成为了一个追番人的核心需求。</p><span id="more"></span><p>下面本文将会带领你认识整套流程，如果你足够经常折腾，大致一个小时之内就可以搭建完成。祝你好运:)</p><p>PS: 本文涉及的所有脚本都可以在该<a href="https://github.com/miracleyoo/anime_renamer">GitHub</a>库上找到。</p><h2 id="软件与配置">软件与配置</h2><ul><li>源：动漫种子站。如Nyaa、ACGRIP、动漫花园、萌番组。为了避免河蟹这里就不放链接了，请自行搜索。</li><li>订阅与自动下载种子：RSS。</li><li>下载器：utorrent。</li><li>正则匹配与自动重命名：Python。</li><li>串流与自动整理：Plex。</li></ul><p>由于上面提到的软件都支持Windows和Mac，所以认为该系统适用于两者。</p><h2 id="流程">流程</h2><p>首先放一张整理出来的流程图：</p><figure><img data-src="image-20210129014429864.png" alt="image-20210129014429864"><figcaption aria-hidden="true">image-20210129014429864</figcaption></figure><p>该系统的整体思路是，使用RSS自动嗅探订阅的内容，用utorrent自动下载到番剧指定的目录，使用Python重命名成Plex可以支持的剧集命名格式后，使用Plex整理、匹配元数据，并提供串流服务，让你的电脑本身成为一个视频内容服务器，从而使你可以在家中任何设备上迅速、便捷地观看下载整理好的视频。这对于习惯在电视大屏幕、投影屏或iPad上观看视频的用户而言是十分友好的。</p><p>最终达到的效果是：一旦种子源网站上更新了一个你订阅的番剧的种子，你就可以第一时间打开电视、掏出iPad直接观看内容。</p><p>另外，由于Plex可以记住你所有视频的观看进度，所以你完全可以在手机上看了前半部分，然后坐到电视机前，从容地无缝继续观看。</p><p>效果如下：</p><figure><img data-src="image-20210129015121641.png" alt="image-20210129015121641"><figcaption aria-hidden="true">image-20210129015121641</figcaption></figure><figure><img data-src="image-20210129015202322.png" alt="image-20210129015202322"><figcaption aria-hidden="true">image-20210129015202322</figcaption></figure><figure><img data-src="image-20210129015424295.png" alt="image-20210129015424295"><figcaption aria-hidden="true">image-20210129015424295</figcaption></figure><h2 id="源站">源站</h2><p>各大动漫压制组、汉化组和搬运组（统称发布组）都会将他们的资源发布到这些源站上。有的源站互为镜像，有的站是爬取的其他站的信息。它们在追新番这项任务上的表现其实都还可以，选一个你自己喜欢的站点即可。</p><p>每个发布组都有自己的偏好的番剧，也有他们独特的规矩和讲究。如VCB-Studio就偏好做高清的压制资源，但经常不附带字幕；LIlith由于主要是搬运，所以发布速度快；喵萌开的番比较多，视频质量较高，但并不是所有番剧都是及时发布的。热门番剧往往会在第一时间就有发布组发布，而一些冷门内容的更新速度就无法保证了。另外，A发布组发布X番剧可能是及时的，Y番剧要隔几天；可能B发布组则相反。最简单的方法就是检查上一集最先是由哪个组发布的，哪个组放出的资源又有最多人下载，然后进行综合判断。</p><h2 id="rss">RSS</h2><p>引用一段Wikipedia的定义：</p><blockquote><p><strong>RSS</strong>（全称：<a href="https://zh.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> Site Summary；Really Simple Syndication[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-powers-2003-1-2">2]</a>），中文译作<strong>简易信息聚合</strong>[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-3">3]</a>，也称<strong>聚合内容</strong>[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-张锐2015-4">4]</a>，是一种<a href="https://zh.wikipedia.org/wiki/消息來源">消息来源</a>格式规范，用以<strong>聚合经常发布更新资料的网站</strong>，例如<a href="https://zh.wikipedia.org/wiki/部落格">博客</a>文章、新闻、<a href="https://zh.wikipedia.org/wiki/音訊">音频</a>或<a href="https://zh.wikipedia.org/wiki/視訊">视频</a>的网摘。RSS文件（或称做摘要、网络摘要、或频更新，提供到频道）包含全文或是节录的文字，再加上发布者所订阅之网摘资料和授权的元数据。简单来说 RSS 能够让用户订阅个人网站个人博客，当订阅的网站有新文章是能够获得通知。</p></blockquote><p>RSS的本质是订阅某个信源。信源可以不断更新和推送信息，它往往表现为一个链接。让我们举一个例子。</p><p><img data-src="image-20210129020139987.png" alt="image-20210129020139987" style="zoom:50%;"></p><p>假设我们想要追一个由喵萌奶茶屋发布的新番<code>IDOLY PRIDE</code>，并且只看简体的1080p版本，那么我们就可以根据其命名风格来生成对应的搜索关键词：<code>【喵萌Production】★01月新番★[偶像荣耀/IDOLY PRIDE][03][1080p][简日双语][招募翻译]</code>是其中一集的命名。那么我们只需要在搜索框中搜索：<code>喵萌Production 偶像荣耀 1080p 简日</code>，即可定位我们需要追的番的特定版本的所有后续资源。</p><figure><img data-src="image-20210129020710605.png" alt="image-20210129020710605"><figcaption aria-hidden="true">image-20210129020710605</figcaption></figure><p>然后我们即可把这个搜索结果的网页当做一个信源。具体获取RSS订阅链接的方法每个网站都各不相同，但大致都会有一个类似的按钮出现。点击这个按钮，在弹出的窗口的地址栏即可找到我们需要的RSS订阅链接。</p><figure><img data-src="image-20210129021051012.png" alt="image-20210129021051012"><figcaption aria-hidden="true">image-20210129021051012</figcaption></figure><p>复制这个链接，这一步就完成了。你成功地得到了你所需要内容的订阅链接。</p><h2 id="utorrent">utorrent</h2><ul><li><p>utorrent是一个可以下载种子文件和磁力链接的软件，支持Windows、Mac和Linux，也有网页版。</p></li><li><p>utorrent支持RSS订阅，相当于每当RSS订阅的信源有了新的内容，utorrent就会自动把它列出来。你也可以设置每当有新内容就自动下载，不过这要视订阅内容而定。每个RSS的自动下载位置可以是不同的文件夹。效果如下：</p><p><img data-src="image-20210130113430927.png" alt="image-20210130113430927" style="zoom:50%;"></p></li><li><p>utorrent支持文件下载完成或状态改变后执行脚本。这个特点非常重要，下一步需要用。</p></li></ul><h3 id="rss配置">RSS配置</h3><ol type="1"><li>首先，将上面提到的RSS链接添加到utorrent的订阅中：点击<code>订阅</code>后页面中的小Wi-Fi图标即可添加，也可以右键<code>添加RSS订阅</code>。这时候先不用设置什么，把URL粘贴到第一项，然后定义一个方便识别的别名即可，订阅栏先选择<code>不要自动下载所有项目</code>。待会儿会在下载器设定中详细设置。</li></ol><p><img data-src="image-20210130113824679.png" alt="image-20210130113824679" style="zoom:50%;"></p><p><img data-src="image-20210130114135639.png" alt="image-20210130114135639" style="zoom:50%;"></p><ol start="2" type="1"><li><p>右键左栏<code>订阅</code>中出现的新项，选择<code>RSS下载器</code>，进入下载器详细配置：</p><p><img data-src="image-20210130114636832.png" alt="image-20210130114636832" style="zoom:50%;"></p><p>这里我们需要修改的只有<code>保存在</code>和<code>订阅</code>两项，前者是这个订阅中所有文件的保存位置，订阅是这套设置将应用于哪个订阅。需要注意的是，文件夹最好为这个番剧的名字，且命名最好遵守以下规则，方便后续匹配：</p><ul><li>只有一个主名字。如果一个番剧既有中文名，又有日文名，还有罗马字或英语名，请只使用一个名字。使用哪个看你自己，最好是广泛接受的名字。其他语言的名字，其他信息，如字幕组，简体繁体，清晰度等，请分别使用一个中括号括住。整理串流软件由于主要遵循西方影视的命名规则，会直接忽略方括号内的所有内容，所以在方括号里面你可以放入任何你想备注的信息。</li><li>一套规则对应一个订阅。整个链条的对应关系是：<code>一个番剧名字-&gt;一条RSS链接-&gt;utorrent中一项RSS订阅-&gt;RSS下载器中一套规则-&gt;一个独立文件夹</code></li><li>过滤器如果没有需求，可以留空或<code>*</code>，表示匹配所有。若是在这个订阅链接中还没有完全筛选，那你可以着这里进行另外一波过滤。</li></ul></li></ol><h2 id="重命名">重命名</h2><p>首先再解释一下为何需要重命名。正如前文所说，Plex等媒体管理软件主要还是针对欧美剧集的命名方法设计的，且会忽略方括号中所有内容。但国内，至少是番剧的命名，对于集数的命名，恰恰大多会放到方括号中，如<code>[c.c動漫][1月新番][工作細胞 第二季][04][BIG5][1080P][MP4]</code>里面的这个<code>[04]</code>就是集数。另外还有的命名是<code>第04话</code>，<code>[第04话]</code>，<code>- 04 -</code>等。另外，由于某些技术性错误，有些发布组会在某集发布后重新发布修复版本，此时的命名为<code>04v2</code>等。还有很多番剧的最后一集会被添加上END字样表示完结。而这些内容都会对后续的解析造成困难，最好可以将集数这个变量转为某种固定格式。</p><p>Python可以很好的完成这项任务。通过正则匹配上述内容，并重新组合，可以轻易的得到想要的结果。写好的脚本如下：</p><h3 id="reg_match.py"><code>reg_match.py</code>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> pathlib2 <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">log_name=op.join(op.dirname(op.realpath(__file__)), <span class="string">&#x27;log.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Episode Regular Expression Matching Rules</span></span><br><span class="line">episode_rules = [<span class="string">r&#x27;(.*)\[(\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[E(\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[第(\d*\.*\d*)话(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[第(\d*\.*\d*)話(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)第(\d*\.*\d*)话(?:END)?(.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)第(\d*\.*\d*)話(?:END)?(.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)- (\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)? (.*)&#x27;</span>]</span><br><span class="line"><span class="comment"># Suffixs of files we are going to rename</span></span><br><span class="line">suffixs = [<span class="string">&#x27;mp4&#x27;</span>, <span class="string">&#x27;mkv&#x27;</span>, <span class="string">&#x27;avi&#x27;</span>, <span class="string">&#x27;mov&#x27;</span>, <span class="string">&#x27;flv&#x27;</span>, <span class="string">&#x27;rmvb&#x27;</span>, <span class="string">&#x27;ass&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>]</span><br><span class="line">sys.stdout = io.TextIOWrapper(buffer=sys.stdout.buffer,encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parse the input arguments. You can whether input only root, or only path, or both root and name.</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Regular Expression Match&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--root&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The root directory of the input file.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The file name of the input file.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--path&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The file full path of the input file.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rename</span>(<span class="params">root, name</span>):</span><br><span class="line">    root = Path(root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> episode_rules:</span><br><span class="line">        matchObj = re.<span class="keyword">match</span>(rule, name, re.I)</span><br><span class="line">        <span class="keyword">if</span> matchObj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            new_name = <span class="string">f&#x27;<span class="subst">&#123;matchObj.group(<span class="number">1</span>)&#125;</span> E<span class="subst">&#123;matchObj.group(<span class="number">2</span>)&#125;</span> <span class="subst">&#123;matchObj.group(<span class="number">3</span>)&#125;</span>&#x27;</span></span><br><span class="line">            <span class="comment"># print(matchObj.group())</span></span><br><span class="line">            <span class="comment"># print(new_name)</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">with</span> codecs.<span class="built_in">open</span>(log_name, <span class="string">&#x27;a+&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="comment"># f.writelines(f&#x27;&#123;name&#125; -&gt; &#123;new_name&#125;&#x27;)</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>, file=f)</span><br><span class="line"></span><br><span class="line">            os.rename(<span class="built_in">str</span>(root/name), <span class="built_in">str</span>(root/new_name))</span><br><span class="line">            general_check(root, new_name)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    general_check(root, name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">general_check</span>(<span class="params">root, name</span>):</span><br><span class="line">    new_name = <span class="string">&#x27; &#x27;</span>.join(name.split())</span><br><span class="line">    <span class="keyword">if</span> new_name != name:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">with</span> codecs.<span class="built_in">open</span>(log_name, <span class="string">&#x27;a+&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>, file=f)</span><br><span class="line">        os.rename(<span class="built_in">str</span>(root/name), <span class="built_in">str</span>(root/new_name))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">if</span> op.isdir(args.path):</span><br><span class="line">        args.root = args.path</span><br><span class="line">        args.path = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name != <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        temp = <span class="built_in">str</span>(args.root/args.name)</span><br><span class="line">        <span class="keyword">if</span> op.isdir(temp):</span><br><span class="line">            args.root = temp</span><br><span class="line">            args.name = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.path != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        root, name = op.split(args.path)</span><br><span class="line">        rename(root, name)</span><br><span class="line">    <span class="keyword">elif</span> args.name != <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        rename(args.root, args.name)</span><br><span class="line">    <span class="keyword">elif</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        files = []</span><br><span class="line">        <span class="keyword">for</span> suffix <span class="keyword">in</span> suffixs:</span><br><span class="line">            files.extend(Path(args.root).rglob(<span class="string">&#x27;*.&#x27;</span>+suffix))</span><br><span class="line">            files.extend(Path(args.root).rglob(<span class="string">&#x27;*.&#x27;</span>+suffix.upper()))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Total Files Number: <span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> path <span class="keyword">in</span> files:</span><br><span class="line">            root, name = op.split(path)</span><br><span class="line">            rename(root, name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Please input whether only root, or only path, or both root and name&#x27;</span>)</span><br></pre></td></tr></table></figure><p>使用的话直接将该文件存到一个新建的<code>reg_match.py</code>文件中即可。使用Python3.6编写，理论支持所有Python3版本。</p><p>但是，仅用Python无法直接被utorrent调用，它想要的是像<code>exe</code>或<code>cmd</code>之类的文件，并想传递<strong>下载好的文件所在目录</strong>和<strong>下载好的文件名</strong>进去。这样我们最好创建一个<code>cmd</code>文件，用于接受参数，并传递给Python脚本。</p><h3 id="run_after_done.cmd"><code>run_after_done.cmd</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set file_name=%1%</span><br><span class="line">set directory=%2%</span><br><span class="line"></span><br><span class="line">python &lt;YOUR_SCRIPT_PATH&gt;\reg_match.py --name=%file_name% --root=%directory%</span><br></pre></td></tr></table></figure><p>其中，<code>&lt;YOUR_SCRIPT_PATH&gt;</code>是你存放上面提到的Python脚本的目录位置。</p><h3 id="utorrent配置">utorrent配置</h3><p>完成了上面的两个脚本，让我们将其配置到utorrent的<code>当下载完成时运行此程序</code>中。效果就是，每当一个文件下载完成，utorrent都会自动调用这个<code>cmd</code>文件，并且将文件目录和名字都作为参数传递进去。</p><p><img data-src="image-20210130122024176.png" alt="image-20210130122024176" style="zoom:50%;"></p><p>命令为：<code>&lt;YOUR_CMD_PATH&gt;\run_after_done.cmd "%F" "%D"</code></p><p>其中，<code>&lt;YOUR_CMD_PATH&gt;</code>为你存放脚本<code>run_after_done.cmd</code>的目录。推荐将它和Python脚本放在同一个目录下。</p><h2 id="plex">Plex</h2><p>引用一段Wiki，以方便不熟悉的朋友们了解：</p><blockquote><p><strong>Plex</strong>是一套<a href="https://zh.wikipedia.org/wiki/媒体播放器">媒体播放器</a>及<a href="https://zh.wikipedia.org/w/index.php?title=媒體伺服器&amp;action=edit&amp;redlink=1">媒体服务器</a>软件，让用户整理在设备上的<a href="https://zh.wikipedia.org/wiki/有聲書">有声书</a>、<a href="https://zh.wikipedia.org/wiki/音樂">音乐</a>、<a href="https://zh.wikipedia.org/wiki/播客">播客</a>、<a href="https://zh.wikipedia.org/wiki/圖片">图片</a>和<a href="https://zh.wikipedia.org/wiki/影片">视频</a>文件，以供<a href="https://zh.wikipedia.org/wiki/串流">流</a>至<a href="https://zh.wikipedia.org/wiki/流動裝置">移动设备</a>、<a href="https://zh.wikipedia.org/wiki/智能電視">智能电视</a>和<a href="https://zh.wikipedia.org/w/index.php?title=電子媒體播放器&amp;action=edit&amp;redlink=1">电子媒体播放器</a>上。Plex可用于<a href="https://zh.wikipedia.org/wiki/Microsoft_Windows">Windows</a>、<a href="https://zh.wikipedia.org/wiki/Android">Android</a>、<a href="https://zh.wikipedia.org/wiki/Linux">Linux</a>、<a href="https://zh.wikipedia.org/wiki/OS_X">OS X</a>和<a href="https://zh.wikipedia.org/wiki/FreeBSD">FreeBSD</a>[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-Gigaom-2">2]</a>。另外，Plex亦让用户透过该平台观看来自<a href="https://zh.wikipedia.org/wiki/YouTube">YouTube</a>、<a href="https://zh.wikipedia.org/wiki/Vimeo">Vimeo</a>和<a href="https://zh.wikipedia.org/wiki/TED大會">TED</a>等内容提供商的视频。Plex亦与<a href="https://zh.wikipedia.org/w/index.php?title=Bitcasa&amp;action=edit&amp;redlink=1">Bitcasa</a>、<a href="https://zh.wikipedia.org/wiki/Box公司">Box</a>和<a href="https://zh.wikipedia.org/wiki/Dropbox">Dropbox</a>等云端服务兼容[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-3">3]</a>[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-4">4]</a>。</p><p>用户可透过Plex<a href="https://zh.wikipedia.org/wiki/前端和后端">前端</a>媒体播放器“Plex Media Player”管理及播放在一台运行“Plex Media Server”的远程电脑上的多媒体文件。另外，用户可使用“Plex Online”服务以社区开发的插件收看<a href="https://zh.wikipedia.org/wiki/Netflix">Netflix</a>、<a href="https://zh.wikipedia.org/wiki/Hulu">Hulu</a>和<a href="https://zh.wikipedia.org/wiki/CNN">CNN</a>的视频。[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-CrunchGear_Interview-5">5]</a></p></blockquote><p>简单说，就是Plex可以管理你的连续剧集（如番剧）和电影，解析出它们的元数据（如海报，封面，演职员表，简介，分集标题等等），在你的电脑上自动搭建一个<strong>服务器</strong>，向你在其他设备上的Plex软件提供像是Bilibili或Netflix一样的串流服务。形象点说就是，你自己搭建了一个简单的视频网站，数据库是你电脑上的内容，供你自己使用。</p><p>另外，Plex不仅支持内网访问，在外网访问也是可以的。内网访问要求终端设备连接到和你的电脑同一个Wi-Fi，而外网则是只要有网络连接就行。当然，内网的速度和清晰度都是更高的。</p><p>Plex支持在Windows，Mac，NAS，Docker等位置安装其server，安装过程十分简单，基本一路Next即可。</p><p>有了前面的设置，这里就很简单了。假设你的文件结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Anime</span><br><span class="line">|- 工作细胞</span><br><span class="line">|- Season 1</span><br><span class="line">        |- 01.mp4</span><br><span class="line">        |- 02.mp4</span><br><span class="line">        |...</span><br><span class="line">        |- 12.mp4</span><br><span class="line">        |- Season 2</span><br><span class="line">        |- 01.mp4</span><br><span class="line">        |- ...</span><br><span class="line">|- 偶像荣耀</span><br><span class="line">|- 01.mp4</span><br><span class="line">        |- ...</span><br></pre></td></tr></table></figure><p>那么，你所需要做的只是：创建一个账户，然后启动你的服务器页面（地址栏输入<a href="https://www.plex.tv/">plex.tv</a>，右上角Launch即可，或是直接点击<a href="https://app.plex.tv/desktop#">Link</a>）：</p><p><img data-src="image-20210130123759950.png" alt="image-20210130123759950" style="zoom:50%;"></p><p>点击<code>+</code>号，新建一个库，</p><p><img data-src="image-20210130123947605.png" alt="image-20210130123947605" style="zoom:50%;"></p><p>简单两步，完成！</p><p>当然我们这里介绍的仅仅是添加剧集的方法，电影库、音乐库和图片库的添加方法类似。你完全可以把你的照片文件夹放到Plex上，这样你就可以在任何地方看到你的照片啦！另外，由于<strong>服务器</strong>是你自己的电脑，它并不会上传你的文件到某个云端，所以安全性也是较有保障的。</p><p>其中电影和电视节目库的最大区别是是否分集和分季。电影的解析针对的是单个的视频文件，而剧集的解析针对的是某个子文件夹。</p><p>之后，你可以进入你的库，查看效果。很大一部分内容会直接成功抓取元数据得到漂亮的封面等内容，另一些则会失败。对于失败的内容，如果你不在意其实也不影响观看，只是没有封面和介绍等。如果你介意，可以选择手动协助匹配：</p><figure><img data-src="image-20210130125127348.png" alt="image-20210130125127348"><figcaption aria-hidden="true">image-20210130125127348</figcaption></figure><p>匹配失败的主要原因是命名问题。在<code>修复匹配-搜索选项</code>中可以更改用于搜索的命名。另外，有的时候是因为搜索的数据库中没有这个数据，此时，你可以切换<code>代理</code>，很多时候是可以找到匹配的。如果你的命名是日语或英语，那么把搜索的语言相应修改。之后点击搜索即可。</p><h2 id="其他">其他</h2><h3 id="注意">注意</h3><ul><li>该方法适用但不仅适用于动漫。其他类型影视剧集也可，只要你能找到合适的订阅源。</li><li>本文涉及的所有脚本都可以在该<a href="https://github.com/miracleyoo/anime_renamer">GitHub</a>库上找到。</li><li>动漫正版化事业不易，各大网站都投入了巨大的资源来建设更全的正版资源库，请尽最大可能支持正版。请将本方法主要用于观看网盘见类剧集和存在删减的剧集。</li></ul><h3 id="trouble-shooting">Trouble Shooting</h3><p>需要注意的是，由于utorrent在上传时会占用文件，所以有一定概率重命名会失败。解决办法也很简单，如果你觉得有问题的时候，关闭utorrent解除占用，往往需要等上几秒，然后直接将整个<code>Anime</code>文件夹（存放动漫的根文件夹）拖放到下面这个<code>cmd</code>文件上即可：</p><p><code>rename_episodes.cmd</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> %%i <span class="keyword">in</span> (%*) <span class="keyword">do</span> python &lt;YOUR_SCRIPT_PATH&gt;\reg_match.py --path=%%i</span><br></pre></td></tr></table></figure><p>该文件会调用前面提到的Python脚本，并对整个文件夹中的所有有问题的文件名进行重命名。你也可以将有问题的子目录或特定文件拖放上去，或是同时拖放多个文件/文件夹。Python脚本中有着相应的适配。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：你喜欢的番剧更新了。是你喜欢的字幕组的高清资源。里面没有放肆的圣光和暗牧。尽管它也许没在国内放送。你可以在任何设备上观看它，并且可以无缝切换。电视，手机，iPad，电脑都没有问题。它很快。&lt;/p&gt;
&lt;p&gt;这将是本篇介绍的全自动全平台订阅追番系统。&lt;/p&gt;
&lt;p&gt;出于各种原因，有许多番剧在B站并找不到。即使开了大会员，从国外驾梯回国，还是无法看到一些喜欢的内容。但同时，各大动漫种子站却提供了几乎无所不包的资源，其中包括新番。但依靠种子追番最常见的问题就是难以track，经常会忘记更新，而且每次都需要经过一个&lt;code&gt;搜索-下载种子-下载-整理-观看&lt;/code&gt;的过程，确实还是很劝退的。如何搭建一个易于配置、足够简单、全自动化抓取、下载、串流的追番系统，便成为了一个追番人的核心需求。&lt;/p&gt;</summary>
    
    
    
    
    <category term="acg" scheme="https://www.miracleyoo.com/tags/acg/"/>
    
    <category term="anime" scheme="https://www.miracleyoo.com/tags/anime/"/>
    
    <category term="system" scheme="https://www.miracleyoo.com/tags/system/"/>
    
    <category term="torrent" scheme="https://www.miracleyoo.com/tags/torrent/"/>
    
  </entry>
  
  <entry>
    <title>Git Submodule 攻略</title>
    <link href="https://www.miracleyoo.com/2020/10/22/git-sub-module/"/>
    <id>https://www.miracleyoo.com/2020/10/22/git-sub-module/</id>
    <published>2020-10-23T02:26:00.000Z</published>
    <updated>2021-03-12T22:26:24.061Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add submodule</span></span><br><span class="line">git submodule add</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clone a project with submodules</span></span><br><span class="line">git <span class="built_in">clone</span> --recursive</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update when submodeule remote repo changed</span></span><br><span class="line">git submodule update --remote</span><br><span class="line"></span><br><span class="line"><span class="comment"># When cloned without recursive</span></span><br><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br><span class="line"></span><br><span class="line"><span class="comment"># Push submodule change to its remote origin master</span></span><br><span class="line"><span class="built_in">cd</span> &lt;submodule_name&gt;</span><br><span class="line">git add -A .</span><br><span class="line">git commit -m <span class="string">&quot;xxx&quot;</span></span><br><span class="line">git checkout &lt;detached branch name/number&gt;</span><br><span class="line">git merge master</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h2><p><code>git submodule</code>允许用户将一个 Git 仓库作为另一个 Git 仓库的子目录。 它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立性。</p><h2 id="作用"><a class="markdownIt-Anchor" href="#作用"></a> 作用</h2><p>在我这里，它的作用非常明确，即给在各个项目中都会用到的代码段一个公共栖息地，做到“一处改，处处改”。</p><h2 id="常用命令"><a class="markdownIt-Anchor" href="#常用命令"></a> 常用命令</h2><h3 id="添加"><a class="markdownIt-Anchor" href="#添加"></a> 添加</h3><p><code>git submodule add</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接clone，会在当前目录生成一个someSubmodule目录存放仓库内容</span></span><br><span class="line">git submodule add https://github.com/miracleyoo/someSubmodule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定文件目录</span></span><br><span class="line">git submodule add https://github.com/miracleyoo/someSubmodule  src/submodulePath</span><br></pre></td></tr></table></figure><p>添加完之后，子模块目录还是空的（似乎新版不会了），此时需要执行：</p><p><code>git submodule update --init --recursive</code></p><p>来真正将子模块中的内容clone下来。同时，如果你的主目录在其他机器也有了一份clone，它们也需要执行上面的命令来把远端关于子模块的更改实际应用。</p><h3 id="clone时子模块初始化"><a class="markdownIt-Anchor" href="#clone时子模块初始化"></a> Clone时子模块初始化</h3><p><code>clone</code>父仓库的时候加上<code>--recursive</code>，会自动初始化并更新仓库中的每一个子模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recursive</span><br></pre></td></tr></table></figure><p>或：</p><p>如果已经正常的<code>clone</code>了，那也可以做以下补救：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br></pre></td></tr></table></figure><p>正常<code>clone</code>包含子模块的函数之后，由于.submodule文件的存在<code>someSubmodule</code>已经自动生成，但是里面是空的。上面的两条命令分别：</p><ol><li>初始化的本地配置文件</li><li>从该项目中抓取所有数据并检出到主项目中。</li></ol><h3 id="更新"><a class="markdownIt-Anchor" href="#更新"></a> 更新</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule update --remote</span><br></pre></td></tr></table></figure><p>Git 将会进入所有子模块，分别抓取并更新，默认更新master分支。</p><p>不带<code>--remote</code>的<code>update</code>只会在本地没有子模块或它是空的的时候才会有效果。</p><h3 id="推送子模块修改"><a class="markdownIt-Anchor" href="#推送子模块修改"></a> 推送子模块修改</h3><p>这里有一个概念，就是主repo中的子模块被拉到本地时默认是一个子模块远程仓库master分支的<code>detached branch</code>。这个分支是master的拷贝，但它不会被推送到远端。如果在子模块中做了修改，并且已经<code>add</code>，<code>commit</code>，那你会发现当你想要<code>push</code>的时候会报错：<code>Updates were rejected because a pushed branch tip is behind its remote</code>。这便是所谓的<code>detached branch</code>的最直接的体现。</p><p>解决方法是：在子模块中先<code>git checkout master</code>，然后在<code>git merge &lt;detached branch name/number&gt;</code>，最后<code>git push -u origin master</code>即可。</p><p>这里解释一下<code>&lt;detached branch name/number&gt;</code>这个东西可以使用<code>git branch</code>命令查看。如果你使用的是<code>zsh</code>，那么问题就更简单了，直接在命令提示符处就可以找到。</p><p><img data-src="image-20200704184550817.png" alt="image-20200704184550817"></p><p><img data-src="image-20200704184656815.png" alt="image-20200704184656815"></p><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><ol><li><p><a href="https://juejin.im/post/5d5ca6e06fb9a06b1a568e32">来说说坑爹的 git submodule</a></p></li><li><p><a href="https://juejin.im/post/5ca47a84e51d4565372e46e0">Git submodule使用指南（一）</a></p></li><li><p><a href="https://github.blog/author/jaw6/">Working with submodules</a></p></li><li><p><a href="https://stackoverflow.com/questions/18770545/why-is-my-git-submodule-head-detached-from-master">Why is my Git Submodule HEAD detached from master?</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;tldr&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#tldr&quot;&gt;&lt;/a&gt; TL;DR&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Add submodule&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule add&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Clone a project with submodules&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; --recursive&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Update when submodeule remote repo changed&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule update --remote&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# When cloned without recursive&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule init&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule update&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Push submodule change to its remote origin master&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; &amp;lt;submodule_name&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git add -A .&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git commit -m &lt;span class=&quot;string&quot;&gt;&amp;quot;xxx&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git checkout &amp;lt;detached branch name/number&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git merge master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git push -u origin master&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="git" scheme="https://www.miracleyoo.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Spectral Imaging 数据采集前期调研</title>
    <link href="https://www.miracleyoo.com/2020/10/16/hsi-pre-data-collection/"/>
    <id>https://www.miracleyoo.com/2020/10/16/hsi-pre-data-collection/</id>
    <published>2020-10-16T23:57:04.000Z</published>
    <updated>2021-03-12T22:04:46.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="equipment"><a class="markdownIt-Anchor" href="#equipment"></a> Equipment</h2><h3 id="flir-blackfly-s-rgb-camera"><a class="markdownIt-Anchor" href="#flir-blackfly-s-rgb-camera"></a> FLIR Blackfly S RGB Camera</h3><ol><li><p>Spectral Range:</p><ul><li>Blue: 460 nm</li><li>Green: 530 nm</li><li>Red: 625 nm</li></ul></li><li><p>Resolution: 720 × 540</p></li><li><p>FPS: 522</p><span id="more"></span></li><li><p>Dimensions [W x H x L]: 29 mm × 29 mm × 30 mm</p></li><li><p>Official Link: <a href="https://www.flir.com/products/blackfly-s-usb3/">Link</a></p></li></ol><h3 id="ximea-mq022hg-im-sm5x5-nir-multispectral-camera"><a class="markdownIt-Anchor" href="#ximea-mq022hg-im-sm5x5-nir-multispectral-camera"></a> XIMEA MQ022HG-IM-SM5X5-NIR Multispectral Camera</h3><ol><li>Spectral Range: 665~975nm</li><li>Resolution:<ul><li>Original: 2048 × 1088</li><li>Spatial: 409 × 217</li></ul></li><li>FPS: up to 170 cubes/sec</li><li>Sensor size: 2/3&quot;</li><li>Dimensions WxHxD: 26 x 26 x 31 mm</li><li>Pixel size: 5.5 µm</li><li>Python multispectral processing lib: <a href="http://www.spectralpython.net/#documentation">Link</a></li><li>Camera control official python lib: <a href="https://www.ximea.com/support/wiki/apis/Python">Link</a></li><li>Official brief specification: <a href="https://www.ximea.com/files/brochures/xiSpec-Hyperspectral-cameras-2015-brochure.pdf">Link</a></li><li>Official Page: <a href="https://www.ximea.com/en/products/hyperspectral-cameras-based-on-usb3-xispec/mq022hg-im-sm5x5-nir">Link</a></li></ol><table><thead><tr><th>Full Specifications:</th><th></th></tr></thead><tbody><tr><td><strong>Part Number</strong></td><td>MQ022HG-IM-SM5X5-NIR</td></tr><tr><td><strong>Resolution</strong></td><td>Original: 2048 × 1088 Spatial: 409 × 217</td></tr><tr><td><strong>Frame rates</strong></td><td>up to 170 cubes/sec</td></tr><tr><td><strong>Sensor type</strong></td><td>CMOS, Hyperspectral filters added at wafer-level</td></tr><tr><td><strong>Sensor model</strong></td><td>IMEC SNm5x5</td></tr><tr><td><strong>Sensor size</strong></td><td>2/3&quot;</td></tr><tr><td><strong>Sensor active area</strong></td><td>25 Bands</td></tr><tr><td><strong>Readout Method</strong></td><td>Snapshot Mosaic</td></tr><tr><td><strong>Pixel size</strong></td><td>5.5 µm</td></tr><tr><td><strong>ADC -Bits per pixel</strong></td><td>8, 10 bit RAW pixel data</td></tr><tr><td><strong>Data interface</strong></td><td>USB 3.1 Gen1 or PCI Express (xiX camera model)</td></tr><tr><td><strong>Data I/O</strong></td><td>GPIO IN, OUT</td></tr><tr><td><strong>Power consumption</strong></td><td>1.6 Watt</td></tr><tr><td><strong>Lens mount</strong></td><td>C or CS Mount</td></tr><tr><td><strong>Weight</strong></td><td>32 grams</td></tr><tr><td><strong>Dimensions WxHxD</strong></td><td>26 x 26 x 31 mm</td></tr><tr><td><strong>Operating temperature</strong></td><td>50 °C</td></tr><tr><td><strong>Spectral range</strong></td><td>665-975 nm</td></tr><tr><td><strong>Customs tariff code</strong></td><td>8525.80 30 (EU) / 8525.80 40 (USA)</td></tr><tr><td><strong>ECCN</strong></td><td>EAR99</td></tr></tbody></table><h3 id="seek-compact-pro-thermal-camera"><a class="markdownIt-Anchor" href="#seek-compact-pro-thermal-camera"></a> Seek Compact Pro Thermal Camera</h3><ol><li>Seek Compact Pro: 7500~14000 nm</li><li>Resolution:  <strong>320 x 240</strong></li><li>Field of view: <strong>32°</strong></li><li>Frame rate: <strong>&lt; 9 Hz</strong></li><li>Focusable lens</li><li>Platform: Android or iOS (Linux 3rd-party binary library)</li><li>Specification Sheet: <a href="https://www.thermal.com/uploads/1/0/1/3/101388544/compactpro-sellsheet-website.pdf">Link</a></li></ol><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><p>Full spectrum:</p><p><img data-src="2880px-EM_spectrum.svg.png" alt="img"></p><p><img data-src="2880px-EM_Spectrum_Properties_edit_zh.svg.png" alt="img"></p><p><img data-src="Atmospheric_electromagnetic_opacity.svg" alt="img"></p><p><img data-src="200px-Light_spectrum.svg.png" alt="img"></p><h3 id="different-infrared"><a class="markdownIt-Anchor" href="#different-infrared"></a> <strong>Different Infrared:</strong></h3><table><thead><tr><th style="text-align:center">Division name</th><th style="text-align:center">Abbreviation</th><th style="text-align:center">Wavelength</th><th style="text-align:center">Frequency</th><th style="text-align:center">Photon energy</th><th style="text-align:center">Temperature[<a href="https://en.wikipedia.org/wiki/Infrared#cite_note-%E2%80%A0-15">i]</a></th><th style="text-align:center">Characteristics</th></tr></thead><tbody><tr><td style="text-align:center">Near-infrared</td><td style="text-align:center">NIR, IR-A <em><a href="https://en.wikipedia.org/wiki/DIN">DIN</a></em></td><td style="text-align:center">0.75–1.4 <a href="https://en.wikipedia.org/wiki/%CE%9Cm">μm</a></td><td style="text-align:center">214–400 <a href="https://en.wikipedia.org/wiki/Terahertz_(unit)">THz</a></td><td style="text-align:center">886–1653 <a href="https://en.wikipedia.org/wiki/MeV">meV</a></td><td style="text-align:center">3,864–2,070 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (3,591–1,797 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align:center">Defined by water absorption,[<em><a href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify">clarification needed</a></em>] and commonly used in <a href="https://en.wikipedia.org/wiki/Fiber_optic">fiber optic</a> telecommunication because of low attenuation losses in the SiO2 glass (<a href="https://en.wikipedia.org/wiki/Silica">silica</a>) medium. <a href="https://en.wikipedia.org/wiki/Image_intensifier">Image intensifiers</a> are sensitive to this area of the spectrum; examples include <a href="https://en.wikipedia.org/wiki/Night_vision">night vision</a> devices such as night vision goggles. <a href="https://en.wikipedia.org/wiki/Near-infrared_spectroscopy">Near-infrared spectroscopy</a> is another common application.</td></tr><tr><td style="text-align:center">Short-wavelength infrared</td><td style="text-align:center">SWIR, IR-B <em>DIN</em></td><td style="text-align:center">1.4–3 μm</td><td style="text-align:center">100–214 THz</td><td style="text-align:center">413–886 meV</td><td style="text-align:center">2,070–966 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (1,797–693 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align:center">Water absorption increases significantly at 1450 nm. The 1530 to 1560 nm range is the dominant spectral region for long-distance telecommunications.</td></tr><tr><td style="text-align:center">Mid-wavelength infrared</td><td style="text-align:center">MWIR, IR-C <em>DIN</em>; MidIR.[<a href="https://en.wikipedia.org/wiki/Infrared#cite_note-rdmag20120908-16">15]</a> Also called intermediate infrared (IIR)</td><td style="text-align:center">3–8 μm</td><td style="text-align:center">37–100 THz</td><td style="text-align:center">155–413 meV</td><td style="text-align:center">966–362 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (693–89 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align:center">In guided missile technology the 3–5 μm portion of this band is the atmospheric window in which the homing heads of passive IR ‘heat seeking’ missiles are designed to work, homing on to the <a href="https://en.wikipedia.org/wiki/Infrared_signature">Infrared signature</a> of the target aircraft, typically the jet engine exhaust plume. This region is also known as thermal infrared.</td></tr><tr><td style="text-align:center">Long-wavelength infrared</td><td style="text-align:center">LWIR, IR-C <em>DIN</em></td><td style="text-align:center">8–15 μm</td><td style="text-align:center">20–37 THz</td><td style="text-align:center">83–155 meV</td><td style="text-align:center">362–193 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (89 – −80 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align:center">The “thermal imaging” region, in which sensors can obtain a completely passive image of objects only slightly higher in temperature than room temperature - for example, the human body - based on thermal emissions only and requiring no illumination such as the sun, moon, or infrared illuminator. This region is also called the “thermal infrared”.</td></tr><tr><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/Far_infrared">Far infrared</a></td><td style="text-align:center">FIR</td><td style="text-align:center">15–1000 μm</td><td style="text-align:center">0.3–20 THz</td><td style="text-align:center">1.2–83 meV</td><td style="text-align:center">193–3 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (−80.15 – −270.15 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align:center">(see also <a href="https://en.wikipedia.org/wiki/Far-infrared_laser">far-infrared laser</a> and <a href="https://en.wikipedia.org/wiki/Far_infrared">far infrared</a>)</td></tr></tbody></table><h2 id="thermal"><a class="markdownIt-Anchor" href="#thermal"></a> Thermal</h2><h3 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h3><ol><li><p><a href="https://www.flir.com/oem/adas/adas-dataset-form/">FREE FLIR Thermal Dataset for Algorithm Training</a></p><p><img data-src="image-20200716161001770.png" alt="image-20200716161001770"></p><p><img data-src="image-20200716161013251.png" alt="image-20200716161013251"></p></li><li><p><a href="https://soonminhwang.github.io/rgbt-ped-detection/">KAIST Multispectral Pedestrian Detection Benchmark</a> [2018] <a href="https://www-users.cs.umn.edu/~jsyoon/JaeShin_homepage/kaist_multispectral.pdf">Paper</a></p><p>Contain day and night scenarios. Human with bounding box. RGB-Thermal pair.</p><p>The KAIST Multispectral Pedestrian Dataset consists of 95k color-thermal pairs (640x480, 20Hz) taken from a vehicle. All the pairs are manually annotated (person, people, cyclist) for the total of 103,128 dense annotations and 1,182 unique pedestrians.</p><p><img data-src="teaser.png" alt="teaserImage"></p></li></ol><h2 id="real-multispectral"><a class="markdownIt-Anchor" href="#real-multispectral"></a> Real-Multispectral</h2><ol><li><p><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset">Hyperspectral Images Database</a> [2017]</p><p><strong>Visible Range MSI</strong></p><p>NUS hyperspectral images database: 52 Outdoor Scene, 35 Indoor Scene, 33 Individual Fruit Scene, 11 Group Fruit Scene, 13 Real vs Fake Fruit Scene, 44 color Charts &amp; Patches Scene.</p></li></ol><p>It consists of various indoor and outdoor scenes taken with a SPECIM hyperspectral camera and multiple consumer cameras. For consumer cameras, camera-specific RAW format that is free of any manipulation, is available. For easier classification, this hyperspectral camera dataset has been categorized into the following categories:</p><ul><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/general-scenes">General Scenes (Outdoor &amp; Indoor)</a></li><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/fruits">Fruits</a></li><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/color-patches">Color Charts and Patches</a></li></ul><p>Additionally, our spectral data can be visualized using the professional software by <a href="http://scyllarus.research.nicta.com.au/">Scyllarus Matlab/C++ toolbox</a>.</p><p>Relevant Code <a href="https://github.com/trangreyle/gene-color-mapping">GitHub</a></p>  <img data-src="image-20200729094343425.png" alt="image-20200729094343425" style="zoom: 50%;">  <img data-src="image-20200729094355733.png" alt="image-20200729094355733" style="zoom:50%;">  <img data-src="image-20200729094407105.png" alt="image-20200729094407105" style="zoom:50%;">  <img data-src="image-20200729094457881.png" alt="image-20200729094457881" style="zoom:50%;"><ol start="2"><li><p><a href="https://biic.wvu.edu/data-sets/multispectral-dataset">Multispectral Dataset from west virginia university</a></p><ol><li>SWIR Biometrics Dataset: <strong>SWIR</strong></li><li>WVU Multispectral Face Database: Three types of camera are used: <strong>RGB, Multi(RGB+NIR), SWIR</strong></li><li>Multispectral Imaging (Iris) Database:</li></ol></li><li><p><a href="https://www.mi.t.u-tokyo.ac.jp/static/projects/mil_multispectral/">Multispectral Image Recognition</a></p><ol><li>Multi-spectral Object Detection</li></ol><p><strong>RGB, Near-infrared (NIR), Mid-wavelength infrared (MIR), and Far infrared (FIR)</strong> from the left. Objects are labeled and bounding box predicted.</p><p><img data-src="det_result.png" alt="img"></p><ol start="2"><li>Multi-spectral Semantic Segmentation</li></ol><p>RGB-Thermal dataset with semantic segmentation</p><p><img data-src="predictionExamples_good.png" alt="img"></p></li><li><p><a href="https://projects.ics.forth.gr/cvrl/msi/">Multispectral Imaging (MSI) datasets</a>: Painting multispectral images. Not paired. Not ordinary objects.</p><p><img data-src="image-20200727175214182.png" alt="image-20200727175214182"></p></li><li><p><a href="https://www.cs.columbia.edu/CAVE/databases/multispectral/">CAVE Multispectral Image Database</a></p><p><strong>Visible Range MSI:</strong> <strong>400nm to 700nm</strong></p><p>It only has 32 multispectral &amp; RGB image pairs… Be careful to use it. Each image has 31 bands, and they are separated.</p><table><thead><tr><th>Camera</th><th><a href="http://www.ccd.com/alta_u260.html">Cooled CCD camera (Apogee Alta U260)</a></th></tr></thead><tbody><tr><td>Resolution</td><td>512 x 512 pixel</td></tr><tr><td>Filter</td><td><a href="http://www.cri-inc.com/products/varispec.asp">VariSpec liquid crystal tunable filter</a></td></tr><tr><td>Illuminant</td><td>CIE Standard Illuminant D65</td></tr><tr><td>Range of wevelength</td><td>400nm - 700nm</td></tr><tr><td>Steps</td><td>10nm</td></tr><tr><td>Number of band</td><td>31 band</td></tr><tr><td>Focal length</td><td>f/1.4</td></tr><tr><td>Focus</td><td>Fixed (focused using 550nm image)</td></tr><tr><td>Image format</td><td>PNG (16bit)</td></tr></tbody></table><p><img data-src="teaser-20200727174750119.png" alt="img"></p></li><li><p><a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/">Bristol Hyperspectral Images Database</a> [1995]</p><p><strong>Visible Range MSI</strong></p><p>The database consists of <strong>29 scenes</strong>, each composed by <strong>31 spectrally filtered images</strong> (256 x 256 x 256 grey levels). Each scene has been compressed (zipped) and can be downloaded separately by clicking on the corresponding picture. Please bear in mind that all individual images have a 32 bytes header. To download the whole database at once, just click <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/brelstaff.tar.gz">here</a>.</p><p>There is some code and miscellaneous files <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/src/Src.zip">here</a> (these need to be run in order to make use of the images as physical measurements). A more complete description on how the images were gathered and some issues on the camera’s technicalities can be found <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/2-TECH.pdf">here</a>.</p><img data-src="image-20200729095320320.png" alt="image-20200729095320320" style="zoom:50%;"></li><li><p><a href="http://vision.seas.harvard.edu/hyperspec/">Harvard Real-World Hyperspectral Images</a> [2011]</p><p><strong>Visible Range MSI:</strong> <strong>420nm to 720nm</strong></p><p>The camera uses an integrated liquid crystal tunable filter and is capable of acquiring a hyperspectral image by sequentially tuning the filter through a series of <strong>31 narrow wavelength bands</strong>, each with approximately 10nm bandwidth and centered at steps of 10nm from <strong>420nm to 720nm</strong>.</p><p>The captured dataset includes images of both indoor and outdoor scenes featuring a diversity of objects, materials and scale.</p><p>This is a database of <strong>50</strong> hyperspectral images of indoor and outdoor scenes under daylight illumination, and an additional <strong>25</strong> images under artificial and mixed illumination. The images were captured using a commercial hyperspectral camera (Nuance FX, CRI Inc) with an integrated liquid crystal tunable filter capable of acquiring a hyperspectral image by sequentially tuning the filter through a series of thirty-one narrow wavelength bands, each with approximately 10nm bandwidth and centered at steps of 10nm from 420nm to 720nm. The camera is equipped with an apo-chromatic lens and the images were captured with the smallest viable aperture setting, thus largely avoiding chromatic aberration. All the images are of static scenes, with labels to mask out regions with movement during exposure.</p><p>This database is available for non-commercial research use. The data is available as a series of MATLAB .mat files (one for each image) containing both the images data and masks. Since the size of the download is large (around 5.5 + 2.2 GB), we ask that you send an e-mail to the authors at <strong>ayanc[at]eecs[dot]harvard[dot]edu</strong> for the download link. If you use this data in an academic publication, kindly cite the following paper:</p><img data-src="image-20200729100019504.png" alt="image-20200729100019504" style="zoom:50%;"></li><li><p><a href="http://colour.cmp.uea.ac.uk/datasets/multispectral.html">UAE multispectral image database</a></p><p><strong>Visible Range MSI:</strong> <strong>400nm to 700nm</strong></p><p>Wavelength range from 400nm to 700nm at 10nm steps (31 samples). The image matrix for each object is 31xWIDTHxHEIGHT. The images have been captured in a VeriVide viewing booth with a black cloth background under CIE illuminant D75. Each image has been captured twice: once with a white tile and once without. The illuminant has been estimated from the white tile and the spectral data divided by this estimate, in order to arrive at reflectance measurements. The images below are displayed sRGB values rendered under a neutral daylight (D65).</p><img data-src="image-20200729100842353.png" alt="image-20200729100842353" style="zoom:33%;"></li><li><p><a href="http://personalpages.manchester.ac.uk/staff/david.foster/default.html">Manchester hyperspectral images Dataset<strong>s</strong></a></p><p><strong>Visible Range MSI:</strong>  400, 410, …, 720 nm</p><p>Multiple MSI datasets included:</p><img data-src="image-20200729101027937.png" alt="image-20200729101027937" style="zoom:50%;"><ul><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html">Time-Lapse Hyperspectral Radiance Images of Natural Scenes 2015</a></p><img data-src="image-20200729101135099.png" alt="image-20200729101135099" style="zoom:33%;"></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Local_Illumination_HSIs/Local_Illumination_HSIs_2015.html">Hyperspectral Images for Local Illumination in Natural Scenes 2015</a></p><img data-src="image-20200729101232518.png" alt="image-20200729101232518" style="zoom:50%;"></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_02.html">Hyperspectral Images of Natural Scenes 2002</a></p><img data-src="image-20200729101343922.png" alt="image-20200729101343922" style="zoom:50%;"></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_04.html">Hyperspectral Images of Natural Scenes 2004</a></p><img data-src="image-20200729101441578.png" alt="image-20200729101441578" style="zoom:50%;"></li></ul></li><li><p><a href="https://pythonhosted.org/bob.db.cbsr_nir_vis_2/">BOB NIR+VIS Face Database</a> [2013]</p><p>It consists of 725 subjects in total. There are [1-22] VIS and [5-50] NIR face images per subject. The eyes positions are also distributed with the images.</p><img data-src="database.png" alt="_images/database.png" style="zoom:33%;"></li><li><p><a href="http://icvl.cs.bgu.ac.il/hyperspectral/">ICVL hyperspectral database</a></p></li></ol><p><strong>RGB+NIR Range MSI:</strong>  Images were collected at 1392×1300 spatial resolution over 519 spectral bands (<strong>400-1,000nm</strong> at roughly 1.25nm increments)</p><p>The database images were acquired using a Specim PS Kappa DX4 hyperspectral camera and a rotary stage for spatial scanning. At this time it contains 201 images and will continue to grow progressively. For your convenience, <strong>.mat</strong> files are provided, downsampled to 31 spectral channels from 400nm to 700nm at 10nm increments.</p>   <img data-src="image-20200729102126434.png" alt="image-20200729102126434" style="zoom:50%;"><ol start="11"><li><p><a href="http://colorimaginglab.ugr.es/pages/Data">University of Granada hyperspectral image database</a></p><p><strong>RGB+NIR Range MSI:</strong> Most of the images have spatial resolution of 1000 × 900 pixels. The spectral range is from <strong>400 nm to 1000</strong> nm in 10 nm intervals, resulting in total 61 channels.</p><img data-src="image-20200729102332515.png" alt="image-20200729102332515" style="zoom:50%;"></li><li><p><a href="http://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/">SWIRPowder</a>: A 400-1700nm Multispectral Dataset with 100 Powders on Complex Backgrounds</p><p><strong>SWIR(Multi)+RGB+NIR</strong></p><p><img data-src="result.png" alt="img"></p><p><img data-src="illustration_1.png" alt="img"></p></li><li><p><a href="http://www.ok.sc.e.titech.ac.jp/res/MSI/MSIdata31.html">TokyoTech 31-band Hyperspectral Image Dataset</a> [2015]</p><p><strong>Visible Range MSI:</strong> <strong>420nm to 720nm</strong></p><p>Colorful objects with rich textures 30 scenes from 420nm to 720nm at 10nm intervals</p><p><img data-src="MSimage.png" alt="MSimage"></p><p><img data-src="image-20200729094232057.png" alt="image-20200729094232057"></p></li></ol><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ul><li><a href="https://jbthomas.org/TechReport/CIC-shortcourseSFA-2017.pdf">Spectral Filter Arrays Technology</a></li><li><a href="https://en.wikipedia.org/wiki/Infrared">Infrared WiKi</a></li></ul><h2 id="electromagnetic-wave-classification"><a class="markdownIt-Anchor" href="#electromagnetic-wave-classification"></a> Electromagnetic Wave Classification</h2><p>γ = <a href="https://zh.wikipedia.org/wiki/%E4%BC%BD%E9%A6%AC%E5%B0%84%E7%B7%9A">伽马射线</a><br><strong><a href="https://zh.wikipedia.org/wiki/X%E5%B0%84%E7%B7%9A">X射线</a>：</strong><br>HX = 硬<a href="https://zh.wikipedia.org/wiki/X%E5%B0%84%E7%B7%9A">X射线</a><br>SX = 软X射线<br><strong><a href="https://zh.wikipedia.org/wiki/%E7%B4%AB%E5%A4%96%E7%B7%9A">紫外线</a>：</strong><br>EUV = 极端<a href="https://zh.wikipedia.org/wiki/%E7%B4%AB%E5%A4%96%E7%B7%9A">紫外线</a><br>NUV = 近紫外线<br><strong><a href="https://zh.wikipedia.org/wiki/%E7%B4%85%E5%A4%96%E7%B7%9A">红外线</a>：</strong><br>NIR = 近<a href="https://zh.wikipedia.org/wiki/%E7%B4%85%E5%A4%96%E7%B7%9A">红外线</a><br>MIR =中红外线<br>FIR = <a href="https://zh.wikipedia.org/wiki/%E9%81%A0%E7%B4%85%E5%A4%96%E7%B7%9A">远红外线</a></p><p>Typically we define near infrared (<em>NIR</em>) from 780 nm to 1400 nm and shortwave infrared (<em>SWIR</em>) from 1400 nm to 3000 nm.</p><p><strong><a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E6%B3%A2">微波</a>：</strong><br>EHF = <a href="https://zh.wikipedia.org/wiki/%E6%A5%B5%E9%AB%98%E9%A0%BB">极高频</a><br>SHF = <a href="https://zh.wikipedia.org/wiki/%E8%B6%85%E9%AB%98%E9%A0%BB">超高频</a><br>UHF = <a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E9%AB%98%E9%A0%BB">特高频</a><br><strong><a href="https://zh.wikipedia.org/wiki/%E7%84%A1%E7%B7%9A%E9%9B%BB%E6%B3%A2">无线电波</a>：</strong><br>VHF = <a href="https://zh.wikipedia.org/wiki/%E7%94%9A%E9%AB%98%E9%A0%BB">甚高频</a><br>HF = <a href="https://zh.wikipedia.org/wiki/%E9%AB%98%E9%A0%BB">高频</a><br>MF = <a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E9%A0%BB">中频</a><br>LF = <a href="https://zh.wikipedia.org/wiki/%E4%BD%8E%E9%A0%BB">低频</a><br>VLF = <a href="https://zh.wikipedia.org/wiki/%E7%94%9A%E4%BD%8E%E9%A2%91">甚低频</a><br>ULF = <a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E4%BD%8E%E9%A0%BB">特低频</a><br>ELF = <a href="https://zh.wikipedia.org/wiki/%E6%A5%B5%E4%BD%8E%E9%A0%BB">极低频</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;equipment&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#equipment&quot;&gt;&lt;/a&gt; Equipment&lt;/h2&gt;
&lt;h3 id=&quot;flir-blackfly-s-rgb-camera&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#flir-blackfly-s-rgb-camera&quot;&gt;&lt;/a&gt; FLIR Blackfly S RGB Camera&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Spectral Range:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blue: 460 nm&lt;/li&gt;
&lt;li&gt;Green: 530 nm&lt;/li&gt;
&lt;li&gt;Red: 625 nm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Resolution: 720 × 540&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FPS: 522&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="hsi" scheme="https://www.miracleyoo.com/tags/hsi/"/>
    
    <category term="data" scheme="https://www.miracleyoo.com/tags/data/"/>
    
    <category term="camera" scheme="https://www.miracleyoo.com/tags/camera/"/>
    
    <category term="optics" scheme="https://www.miracleyoo.com/tags/optics/"/>
    
  </entry>
  
  <entry>
    <title>日系绘画构图</title>
    <link href="https://www.miracleyoo.com/2020/09/12/illustration-composition/"/>
    <id>https://www.miracleyoo.com/2020/09/12/illustration-composition/</id>
    <published>2020-09-13T00:48:14.000Z</published>
    <updated>2021-03-12T23:12:02.117Z</updated>
    
    <content type="html"><![CDATA[<p><strong>构图法+出镜比例+人物动态=好的构图</strong></p><h2 id="常见单人构图"><a class="markdownIt-Anchor" href="#常见单人构图"></a> 常见单人构图</h2><h3 id="对称构图"><a class="markdownIt-Anchor" href="#对称构图"></a> 对称构图</h3><p>主要包含：左右对称，对角线对称</p><p>相对比较中规中矩，但是同样的，限制会比较大，也需要更多细节上的<strong>不对称</strong>来中和构图上的对称。这些不对称元素往往来自于：人物动态、道具、背景元素。</p><img data-src="image-20200912154217731.png" alt="image-20200912154217731" style="zoom:50%;"><span id="more"></span><h3 id="九宫格构图"><a class="markdownIt-Anchor" href="#九宫格构图"></a> 九宫格构图</h3><p>将重要的点置于九宫格的焦点上，起到强化关键要素的作用，如脸部、关节等。</p><p><img data-src="image-20200912153443124.png" alt="image-20200912153443124"></p><h3 id="黄金比例构图"><a class="markdownIt-Anchor" href="#黄金比例构图"></a> 黄金比例构图</h3><p>B格较高，人物的人体曲线和黄金分割线契合，螺旋正中央缩于人物脸部。该构图较为有特色，有着不对称的美感。</p><img data-src="image-20200912153516507.png" alt="image-20200912153516507" style="zoom:50%;"><h3 id="好用技巧"><a class="markdownIt-Anchor" href="#好用技巧"></a> 好用技巧</h3><ol><li><p>人物动态和道具都可以被用来表达动态。如果人体动态比较复杂灵动，衣物可以相对简单点；而如果人物动态想画的简单保守一点，可以使用道具（长而飘逸的头发、大而舞动的裙摆、长长的布料…）</p><img data-src="image-20200912172118092.png" alt="image-20200912172118092" style="zoom:50%;"></li><li><p>利用人物动作带来动态感（风）-&gt; 头发和裙摆随风飘动，人物跳起来或浮于空中。这样可以让画面看起来更有活力和生命力。</p></li></ol><img data-src="image-20200912172201727.png" alt="image-20200912172201727" style="zoom:50%;"><ol><li></li></ol><h2 id="出镜比例"><a class="markdownIt-Anchor" href="#出镜比例"></a> 出镜比例</h2><p>一般的人物在插画中的出镜比例为：</p><ol><li><p>腰部以上：刻画部分更少，但是相对的，对头部和上身衣物的刻画就会要求更高。</p><img data-src="image-20200912153749506.png" alt="image-20200912153749506" style="zoom:50%;"></li><li><p>大腿以上：可以刻画几乎全部的人物动态。</p><p>特点：方便表现躯干动态，方便表达透视。</p><img data-src="image-20200912153853701.png" alt="image-20200912153853701" style="zoom:50%;"></li><li><p>全身：由于人体是一个细长的物体，如果要展现全身并主要由人物填满画面的话，需要作出很大的动作（弯曲）和大角度的透视。</p><img data-src="image-20200912153932298.png" alt="image-20200912153932298" style="zoom:50%;"></li></ol><p>竖版插图（A4）三种比例都很多，而横版插图（1080p）则偏全身。</p><h2 id="常见双人构图"><a class="markdownIt-Anchor" href="#常见双人构图"></a> 常见双人构图</h2><p>无论使用什么构图，两个人之间一定是有某种较为亲近的关系的。由于有不止一个人，此时的人物动态自由度会相对降低，而对构图法的依赖更高，同时也需要更加灵活准确的透视。相比单人，画面会更加丰富，也会侧重边线人物之间的联系、互动。</p><h3 id="平分型"><a class="markdownIt-Anchor" href="#平分型"></a> 平分型</h3><p>偏实用型。</p><p>往往会有某种形式的肢体接触，这样让全图显得更加自然。如果两个人站在一起又没有任何互动，画面会比价奇怪。</p><p>如果两个人的身高体型都近似，那自然是最好；如果身高体型有别，那么则需要通过某些部分（探出的头部，飘逸的头发、突出的膝盖、背包等道具）来平衡画面。</p><p>特点：画面易于平衡、容易理解好上手。对动态要求不高。</p><img data-src="image-20200912155936519.png" alt="image-20200912155936519" style="zoom:50%;"><img data-src="image-20200912161613370.png" alt="image-20200912161613370" style="zoom:50%;"><h3 id="八卦型"><a class="markdownIt-Anchor" href="#八卦型"></a> 八卦型</h3><p>偏静谧美型，强调双人关系。</p><p>特点：两人之间往往会有交叉关系。构图有特征，画面不容易单调。</p><p>要求：对人物动态要求比较高。</p><img data-src="image-20200912161749570.png" alt="image-20200912161749570" style="zoom:50%;"><img data-src="image-20200912161941253.png" alt="image-20200912161941253" style="zoom:50%;"><h3 id="空间构图"><a class="markdownIt-Anchor" href="#空间构图"></a> 空间构图</h3><p>偏氛围。</p><p>特点：两个人物由与摄像机镜头的远近产生了远近、大小的区别。让画面的空间感更足，从而对动态要求不高。</p><img data-src="image-20200912162117364.png" alt="image-20200912162117364" style="zoom:50%;"><h2 id="常见多人构图"><a class="markdownIt-Anchor" href="#常见多人构图"></a> 常见多人构图</h2><p>普通插画中使用相对较少，多用于动画海报、游戏登录界面。</p><h3 id="没有绝对主角的时候"><a class="markdownIt-Anchor" href="#没有绝对主角的时候"></a> 没有绝对主角的时候</h3><h4 id="并排构图及其变形"><a class="markdownIt-Anchor" href="#并排构图及其变形"></a> 并排构图及其变形</h4><p>人物基本分布在同一距离，处于同一直/曲线上。</p><p>特点：</p><ol><li>有着明显走势（为了让画面有序）</li><li>集中</li></ol><p><img data-src="image-20200912162754564.png" alt="image-20200912162754564"></p><img data-src="image-20200912162706717.png" alt="image-20200912162706717" style="zoom:80%;"><h3 id="有绝对主角"><a class="markdownIt-Anchor" href="#有绝对主角"></a> 有绝对主角</h3><p>以主角为中心扩散。</p><img data-src="日系绘画构图/image-20200912163421619.png" alt="image-20200912163421619" style="zoom:50%;">]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;构图法+出镜比例+人物动态=好的构图&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;常见单人构图&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#常见单人构图&quot;&gt;&lt;/a&gt; 常见单人构图&lt;/h2&gt;
&lt;h3 id=&quot;对称构图&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#对称构图&quot;&gt;&lt;/a&gt; 对称构图&lt;/h3&gt;
&lt;p&gt;主要包含：左右对称，对角线对称&lt;/p&gt;
&lt;p&gt;相对比较中规中矩，但是同样的，限制会比较大，也需要更多细节上的&lt;strong&gt;不对称&lt;/strong&gt;来中和构图上的对称。这些不对称元素往往来自于：人物动态、道具、背景元素。&lt;/p&gt;
&lt;img data-src=&quot;image-20200912154217731.png&quot; alt=&quot;image-20200912154217731&quot; style=&quot;zoom:50%;&quot;&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
  </entry>
  
  <entry>
    <title>日系人体绘画总结</title>
    <link href="https://www.miracleyoo.com/2020/08/21/illustration-human-body/"/>
    <id>https://www.miracleyoo.com/2020/08/21/illustration-human-body/</id>
    <published>2020-08-22T00:50:24.000Z</published>
    <updated>2021-03-12T22:51:22.670Z</updated>
    
    <content type="html"><![CDATA[<h2 id="日系画风与现实的区别"><a class="markdownIt-Anchor" href="#日系画风与现实的区别"></a> 日系画风与现实的区别</h2><ul><li>真实的“<strong>三庭五眼</strong>”：<ul><li>三庭：<strong>d(发际线, 眉骨) = d(眉骨, 鼻底) = d(鼻底, 下巴)</strong></li><li>五眼：<strong>眼宽=眼间距=眼睛到面部轮廓的距离</strong></li></ul></li><li>眼睛的长度相对真实比例更长的，并不符合三庭五眼中的五眼比例。而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。</li></ul><img data-src="Screenshot%2520-%25202020-06-12%252022.46.34.png" alt="Screenshot - 2020-06-12 22.46.34" style="zoom:50%;"><span id="more"></span><h2 id="日系男生与女生刻画的特点与区别"><a class="markdownIt-Anchor" href="#日系男生与女生刻画的特点与区别"></a> 日系男生与女生刻画的特点与区别</h2><h3 id="女生"><a class="markdownIt-Anchor" href="#女生"></a> 女生</h3><ul><li>头部的高宽比偏小，脸部偏短。</li><li>面部相对较圆，使用更多的曲线。</li><li>五官相对下移，留出更高的额头放刘海。</li><li>眼睛是刻画的重点，它很大。</li><li>眼型偏圆的居多。</li><li>弱化鼻子、嘴巴、<strong>眉毛</strong>等的刻画。</li><li>眉毛和眼睛间距较大。</li><li>表情方面更多的表现出萌、可爱、甜美，总体偏美型。</li></ul><h3 id="男生"><a class="markdownIt-Anchor" href="#男生"></a> 男生</h3><ul><li>头部的高宽比更大，脸部较长，或是更偏向真实比例。</li><li>面部外轮廓使用更多的直线、锐利的线条。</li><li>脸型更方正/直。</li><li>眼睛相对女生来说较小，或说较细，高度较低，但长度还是相对真实比例更长的，并不符合三庭五眼中的五眼比例，而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。</li><li>眼型偏平行四边形的居多。</li><li>同样眉毛也被着重刻画，意在表现男生的英气、帅气等。</li><li>眉毛和眼睛间距小。</li><li>弱化鼻子和嘴巴的刻画，但又比女生强一点，女生往往可以归结为一个点，而男生还是要刻画的。中老年男人鼻子甚至非常明显。</li><li>表情方面更偏向于酷、帅、冷、不耐烦的感觉。</li></ul><h2 id="日系年龄对面部画法的影响"><a class="markdownIt-Anchor" href="#日系年龄对面部画法的影响"></a> 日系年龄对面部画法的影响</h2><h3 id="男生-2"><a class="markdownIt-Anchor" href="#男生-2"></a> 男生</h3><ul><li>年轻人的用线会相对圆润，越年长线条越锋利。</li><li>年长者的面部转折点会更向上部靠拢，五官也相对向上。</li><li>年长者的眼睛更窄，同样是更加锐利的感觉；而年轻人可以稍微眼睛大一点，高一点，圆一点。</li><li>年长者的眉毛会更尖锐有力。</li><li>年长者的下巴会更宽一些。</li></ul><img data-src="image-20200613182651960.png" alt="image-20200613182651960" style="zoom:50%;"><h3 id="女生-2"><a class="markdownIt-Anchor" href="#女生-2"></a> 女生</h3><ul><li>同样，年长者最重要的是脸部转折点上移。</li><li>年长者五官上移。</li><li>年长者眼睛变窄。</li><li>曲线更加有力。</li></ul><img data-src="image-20200613184807549.png" alt="image-20200613184807549" style="zoom:50%;"><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><ul><li>在年龄变化的刻画上，更加着重地去刻画性别的特点，卡点和转折更重。</li></ul><img data-src="image-20200613185226577.png" alt="image-20200613185226577" style="zoom:50%;"><ul><li>角色设定是一个把文字设定转为标签化的预设人体特征的过程。比如不同的眼型、不同的睫毛表现、不同的眉毛表现、不同的面部外轮廓等。</li></ul><h2 id="眉眼结构"><a class="markdownIt-Anchor" href="#眉眼结构"></a> 眉眼结构</h2> <img data-src="image-20200614093652461.png" alt="image-20200614093652461" style="zoom:50%;"><ul><li>男生眉毛粗，离眼睛近。画的时候要压低、往下放。</li><li>女生眉毛细，离眼睛远。画的时候要提高、往上走。女生眉毛很多时候更像是一条线。</li><li>眉眼的侧视图大致是正视图宽度的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。</li></ul><img data-src="image-20200614094501368.png" alt="image-20200614094501368" style="zoom:50%;"><h2 id="口鼻的画法"><a class="markdownIt-Anchor" href="#口鼻的画法"></a> 口鼻的画法</h2><ul><li>嘴角处要卡点（加重），无论嘴多么小，改卡点的都要卡。嘴的中间要相对画的较虚，若隐若现感觉。</li><li>与真实的人不同，日系画法中往往不刻画上嘴唇，但有时还是会把下嘴唇用一根线带过。</li><li>张开的嘴最好画出牙齿和舌头。</li><li>微笑最好画出左右斜向下箭头的形状。同理，微微噘嘴要画出左右斜向上箭头的形状。</li><li>侧面张开的嘴是梯形的。</li><li>鼻尖也需要卡点。</li><li>男性的鼻子长，嘴巴宽；女性的鼻子小、短，嘴巴窄。</li><li>虽然女生鼻子很多都是一个点，但这个点是有方向和轻重的。方向是面部朝向，而轻重则是点的起笔重（上方），落笔轻（下方）。</li></ul><h3 id="男性口鼻示例"><a class="markdownIt-Anchor" href="#男性口鼻示例"></a> 男性口鼻示例</h3><img data-src="image-20200614150816122.png" alt="image-20200614150816122" style="zoom:50%;"><h3 id="女性口鼻示例"><a class="markdownIt-Anchor" href="#女性口鼻示例"></a> 女性口鼻示例</h3><img data-src="image-20200614150929840.png" alt="image-20200614150929840" style="zoom:50%;"><h2 id="面部示例"><a class="markdownIt-Anchor" href="#面部示例"></a> 面部示例</h2><h3 id="女生-3"><a class="markdownIt-Anchor" href="#女生-3"></a> 女生</h3><img data-src="image-20200614204900408.png" alt="image-20200614204900408" style="zoom:50%;"><h3 id="男生-3"><a class="markdownIt-Anchor" href="#男生-3"></a> 男生</h3><img data-src="image-20200615124628061.png" alt="image-20200615124628061" style="zoom:50%;"><h2 id="整体注意事项"><a class="markdownIt-Anchor" href="#整体注意事项"></a> 整体注意事项</h2><ol><li>绘制步骤：找参考-&gt;打型-&gt;细化-&gt;调整</li><li>画的过程中要不断重复一下几个检查：<ul><li>头部的角度</li><li>角色性格相对应的五官形状</li><li>不断放大缩小去看位置和透视</li></ul></li></ol><h2 id="头发的绘制"><a class="markdownIt-Anchor" href="#头发的绘制"></a> 头发的绘制</h2><ol><li><p>绘制的顺序：</p><ol><li>观察参考</li><li>确定发心（刘海中心点）或发中线位置</li><li>分块画草图</li><li>细化</li></ol></li><li><p>头发多样性的体现：</p><ol><li>粗细变换</li><li>不能过于对称（粗细、位置高低、头发的根数）</li><li>头发的方向（发旋、贴头弯曲、呆毛和不规则发）</li><li>动态变化</li></ol><img data-src="Screenshot%2520-%25202020-07-08%252023.03.30.png" alt="Screenshot - 2020-07-08 23.03.30" style="zoom:50%;"></li><li><p>头发的最大分块可分为：前发（刘海）、中发（一般到脸或肩的侧面头发）和后发。前中都可以没有，但后发一定有。</p></li><li><p>各种发型可以看做各种前中后发的组合。</p></li><li><p>后发不能紧贴头皮，要预留一定距离、体现蓬松感。</p></li><li><p>好的发型的绘制其剪影也是很好看的。</p></li><li><p>注意发梢在脖子周围行程一个圆，而不是直线。这点在短发情况下尤其明显。</p></li><li><p>颈部后面会有短小的头发。</p><img data-src="image-20200709105324243.png" alt="image-20200709105324243" style="zoom:33%;"></li><li><p>发梢是头发的灵魂。不同种类的头发其发梢差距由其之大，比如直线型、尖锐型、折叠型等。</p></li><li><p>发梢部分，尤其是后发，尽量往里收，除非发型就是翘的。</p><img data-src="image-20200709095740444.png" alt="image-20200709095740444" style="zoom:50%;"></li><li><p>长卷发会偏向成熟魅力的气质、长直发偏向冷酷高贵、短直发更多偏向清纯感和学生感、短卷发偏可爱活力。</p></li><li><p>卷发的基础是一撮头发，每一撮头发的基础是S型线，分清楚里外。先画大的S，最后画装饰线。</p><img data-src="image-20200709102107872.png" alt="image-20200709102107872" style="zoom:50%;"></li><li><p>画长发，尤其是长卷发，最重要的是即使线条乱，也不能被线条带节奏，要搞清楚每一片头发的从属关系。</p><img data-src="image-20200709102245876.png" alt="image-20200709102245876" style="zoom:33%;"></li></ol><h3 id="辫子"><a class="markdownIt-Anchor" href="#辫子"></a> 辫子</h3><ol><li><p>绘制辫子的时候要尤其注意线条的虚实，营造穿插感。加细节的时候一定要注意碎发也要顺着辫子的大体走势绘制。</p><img data-src="image-20200709104609283.png" alt="image-20200709104609283" style="zoom:50%;"></li><li><p>马尾辫：年龄越大、马尾越低。马尾朝向斜向上的辫子要先上升后下降。</p></li><li><p>相较于披散发，辫子的翻转更多。</p></li><li><p>辫子中，总会有一些短的、碎的头发丝无法被扎到辫子主流中，他们构成了有效的细节。</p><p><img data-src="image-20200709105414092.png" alt="image-20200709105414092"></p></li></ol><h3 id="男性头发"><a class="markdownIt-Anchor" href="#男性头发"></a> 男性头发</h3><ol><li><p>男性头发如果是向外翘，显得有攻气；向内卷，显得温柔和受气。</p><img data-src="image-20200709121121242.png" alt="image-20200709121121242" style="zoom:50%;"></li><li><p>长发的男性角色，可以直接参照女性头发画法。</p></li></ol><h3 id="发型设计"><a class="markdownIt-Anchor" href="#发型设计"></a> 发型设计</h3><ol><li><p>设计流程：</p><ol><li>分块</li><li>分组（使用不同的元素）</li><li>组合设计</li><li>不同的发型出来上がり～</li></ol><img data-src="image-20200709113650623.png" alt="image-20200709113650623" style="zoom:50%;"></li></ol><h2 id="发饰与其他装饰道具的绘制"><a class="markdownIt-Anchor" href="#发饰与其他装饰道具的绘制"></a> 发饰与其他装饰道具的绘制</h2><h3 id="蝴蝶结和蝴蝶结类似物分类"><a class="markdownIt-Anchor" href="#蝴蝶结和蝴蝶结类似物分类"></a> 蝴蝶结和蝴蝶结类似物分类</h3><ol><li>固定形状的，即卖的时候就是打好的那种蝴蝶结。实心、较大，常用作头饰和领结。</li><li>手打的空心蝴蝶结。同样较大，除了头饰和领结，还可以用作围裙后面等。</li><li>飘带。由一根细长扁平的带子打出来的蝴蝶结。易随风飘动，且由于体积较小，需仔细控制。</li><li>小型点缀式的蝴蝶结。用作小型装饰物，出现在头上（发卡）、衣服上、口袋上等等。</li><li>以上的组合和变形。</li></ol><img data-src="image-20200719014529861.png" alt="image-20200719014529861" style="zoom: 60%;"><img data-src="image-20200719014444973.png" alt="image-20200719014444973" style="zoom:50%;"><img data-src="image-20200719014416132.png" alt="image-20200719014416132" style="zoom:50%;"><img data-src="image-20200719014348797.png" alt="image-20200719014348797" style="zoom:50%;"><img data-src="image-20200719014322885.png" alt="image-20200719014322885" style="zoom:68%;"><h3 id="发库"><a class="markdownIt-Anchor" href="#发库"></a> 发库：</h3><ol><li>注意不能沿着头部轮廓画。</li><li>应该先找出来面部竖直轮廓线，然后保证发库与它垂直。</li><li>尤其注意透视现象，发库的末端结束于耳朵后面。</li></ol><p><img data-src="image-20200719103005141.png" alt="image-20200719103005141"></p><h3 id="帽子"><a class="markdownIt-Anchor" href="#帽子"></a> 帽子</h3><ol><li><p>偏头顶（正）：棒球帽、礼帽、田园风</p></li><li><p>后脑勺：贝雷帽、太阳帽</p></li><li><p>棒球帽的画法：</p><ul><li>帽子的中线与头部中线平齐</li><li>注意帽子的外形</li><li>适当的给帽檐增加厚度</li></ul><img data-src="image-20200719104600298.png" alt="image-20200719104600298" style="zoom:33%;"></li><li><p>太阳帽的画法：</p><ul><li>位置偏脑后</li><li>*帽檐的形状</li><li>适当的装饰：缎带、蝴蝶结、鲜花等</li><li>画的时候即使后面的帽檐部分被脑袋遮挡，还是要先画出来、确保左右是连着的线。</li></ul></li></ol><h3 id="眼镜"><a class="markdownIt-Anchor" href="#眼镜"></a> 眼镜</h3><ol><li><p>可爱风：镜框偏大、偏圆、可以抹除上眼镜框。这样的好处是可以避免眼镜挡住眼睛。</p><p><img data-src="image-20200719110203120.png" alt="image-20200719110203120"></p></li><li><p>成熟风：眼型本来就偏小，眼镜也就偏窄。</p><p><img data-src="image-20200719110140108.png" alt="image-20200719110140108"></p></li><li><p>性感风：无镜框、偏窄。</p><p><img data-src="image-20200719110242408.png" alt="image-20200719110242408"></p></li><li><p>搞事派：镜片反光。。。如柯南开始推理、坂本大佬等。</p><p><img data-src="image-20200719110419553.png" alt="image-20200719110419553"></p></li><li><p>画法要点：</p><ul><li>两个关键支点：鼻梁、耳朵</li><li>眼镜中线呈直线型：镜片透视保持一致</li><li>在画眼睛之前先画出来一个能包裹住两个镜片的矩形外框</li></ul></li></ol><h2 id="表情"><a class="markdownIt-Anchor" href="#表情"></a> 表情</h2><h3 id="分类"><a class="markdownIt-Anchor" href="#分类"></a> 分类</h3><ol><li><p>微笑（尴尬而不失礼貌的）</p><img data-src="image-20200724113228668.png" alt="image-20200724113228668" style="zoom:50%;"><ol><li>口型就外八字点两点就好，注意卡点</li></ol></li><li><p>开口笑</p><img data-src="image-20200724113253860.png" alt="image-20200724113253860" style="zoom:50%;"><ol><li>开口是梯形类似物</li><li>开口的大小要多尝试，因为不同人物适合的大小并不相同</li></ol></li><li><p>狂笑</p><img data-src="image-20200724113443615.png" alt="image-20200724113443615" style="zoom:50%;"></li><li><p>闭眼笑</p><ol><li>眼睛即使闭上了也不是一条单调的线，而是有形状有细节的</li><li>那条线在之前上下眼睑中间</li></ol></li><li><p>怒（攻气、傲慢、盛气凌人）</p><img data-src="image-20200724113521388.png" alt="image-20200724113521388" style="zoom:50%;"><ol><li>眼角上扬</li><li>眉毛外八字，也是上扬</li><li>嘴中间向上弯曲（即曲线向上凸）</li></ol></li><li><p>哀（受气、可怜）</p><img data-src="image-20200724113548199.png" alt="image-20200724113548199" style="zoom:50%;"><ol><li>眼角向下撇</li><li>眉毛正八字</li><li>嘴会很小</li></ol></li><li><p>三无少女</p><img data-src="image-20200724113639102.png" alt="image-20200724113639102" style="zoom:50%;"><ol><li>无口无心无表情</li><li>嘴部很小很小，一般就是一个点</li><li>由于本来角色就不怎么有表情，所以眼睛和眉毛移一直都是放松状态</li></ol></li><li><p>邪魅一笑（色气）</p><img data-src="image-20200724111805983.png" alt="image-20200724111805983" style="zoom:42%;"><ol><li>面部红晕</li><li>特制笑容</li></ol></li><li><p>微微哭泣</p><img data-src="image-20200724112336854.png" alt="image-20200724112336854" style="zoom:33%;"><ol><li>委屈+抿嘴</li><li>少量眼角泪花</li><li>面颊红晕</li></ol></li><li><p>大哭</p><img data-src="image-20200724110336117.png" alt="image-20200724110336117" style="zoom:33%;"><ol><li>委屈加强版，眉毛继续八字</li><li>眼睛高度减小，下眼睑向上挤压</li><li>有眼泪元素</li><li>嘴张开，露牙齿</li><li>面颊红晕</li></ol></li><li><p>害羞</p><ol><li>大片的面部红晕，可以覆盖眼睛下面一长条包括鼻子位置</li></ol></li><li><p>傲娇</p><img data-src="image-20200724112224823.png" alt="image-20200724112224823" style="zoom:33%;"><ol><li>害羞+怒</li><li>可以用微汗点缀</li><li>小虎牙</li></ol></li><li><p>病娇（诡异）</p><img data-src="image-20200724110015171.png" alt="image-20200724110015171" style="zoom:33%;"><ol><li>眼球和上眼睑不是连着的，之间隔着一点眼白</li><li>嘴部有不对称元素</li><li>视角可偏仰视</li></ol></li></ol><h3 id="复杂表情构成"><a class="markdownIt-Anchor" href="#复杂表情构成"></a> 复杂表情构成</h3><ol><li>基础表情（不同程度的笑、哭、哀）</li><li>不同角度、身体转向（比如表现委屈、惹人怜爱、哭泣可以采用俯视，而表现惊悚和恶役则可使用仰视，元气可爱灵动可用各种侧视图等）</li><li>肢体动作（如手部动作）</li></ol><h3 id="漫画与插画的区别"><a class="markdownIt-Anchor" href="#漫画与插画的区别"></a> 漫画与插画的区别</h3><ul><li>相较于漫画，插画更偏向于整体人物的唯美性，表情一般不会很夸张，笑脸会占据大半。</li><li>漫画则相对束缚较小，可以画各种各样奇奇怪怪的夸张表情来凸显人物心情和推动剧情。</li></ul><img data-src="image-20200724120144027.png" alt="image-20200724120144027" style="zoom:50%;"><h2 id="衣物的画法"><a class="markdownIt-Anchor" href="#衣物的画法"></a> 衣物的画法</h2><h3 id="衣服的分类"><a class="markdownIt-Anchor" href="#衣服的分类"></a> 衣服的分类</h3><p><strong>女生：</strong> 制服（JK）、女仆装、泳装、和风服饰（和服、浴衣）、LOLITA、idol（舞台装）</p><p><strong>男生：</strong> 衬衫、西服、卫衣、T恤</p><h3 id="画法要点"><a class="markdownIt-Anchor" href="#画法要点"></a> 画法要点</h3><ol><li>注意与人物形体的关系，不要直接画衣服</li><li>受力情况</li><li>搭配、元素组合</li><li>设计（饰品）</li><li>参与构图</li></ol><h3 id="衣褶"><a class="markdownIt-Anchor" href="#衣褶"></a> 衣褶</h3><h4 id="衣褶的成因"><a class="markdownIt-Anchor" href="#衣褶的成因"></a> 衣褶的成因</h4><ul><li>衣褶是受支撑点和地心引力的拉力影响而形成的褶皱。</li><li>如果没有内部结构就没有布纹衣褶的存在。</li><li>拉伸的力抹平衣褶，收缩的力形成衣褶。</li></ul><img data-src="image-20200726104737094.png" alt="image-20200726104737094" style="zoom:33%;"><h4 id="衣褶多的地方"><a class="markdownIt-Anchor" href="#衣褶多的地方"></a> 衣褶多的地方</h4><ul><li>和内部支撑物之间空间较大的地方（宽松领口、古人衣服、宽松袖子、卫衣腰部）（相反：紧身衣、内衣、丝袜）</li><li>受到收缩外力明显的地方（腰带处、袖口、撸起来的袖子、关节内缩侧）（相反：关节绷紧侧）</li></ul><img data-src="Screenshot%2520-%25202020-07-26%252010.48.59.png" alt="Screenshot - 2020-07-26 10.48.59" style="zoom:50%;"><ul><li><p>人体中衣褶多的位置列举：</p><ul><li>领口</li><li>腋下</li><li>弯曲的胳膊腿的内侧</li><li>腰部</li><li>裆部（裤褶、裙褶）</li><li>裤脚</li><li>其他所有的有转折的地方</li><li>妹子胸部下部</li><li>腰带、绳带处</li><li>扣子处</li></ul><img data-src="image-20200726110314572.png" alt="image-20200726110314572" style="zoom:33%;"></li></ul><h3 id="褶皱理论"><a class="markdownIt-Anchor" href="#褶皱理论"></a> 褶皱理论</h3><h4 id="一点支撑"><a class="markdownIt-Anchor" href="#一点支撑"></a> 一点支撑</h4><p>褶皱形状：放射状</p><p><img data-src="image-20200726111529235.png" alt="image-20200726111529235"></p><h5 id="常见位置"><a class="markdownIt-Anchor" href="#常见位置"></a> 常见位置</h5><ul><li>肩部</li><li>胸部中下侧</li><li>绳带</li><li>膝盖</li><li>手肘</li></ul><h4 id="两点支撑"><a class="markdownIt-Anchor" href="#两点支撑"></a> 两点支撑</h4><p>两点支撑可以看做两个一点支撑去掉内侧的线然后连起来。</p><img data-src="image-20200726112319409.png" alt="image-20200726112319409" style="zoom:40%;"><h5 id="常见位置-2"><a class="markdownIt-Anchor" href="#常见位置-2"></a> 常见位置：</h5><ul><li>领口</li><li>腰部（胯）</li><li>宽松的XX（袖子、裤子、腰）</li></ul><img data-src="image-20200726112701126.png" alt="image-20200726112701126" style="zoom:33%;"><img data-src="image-20200726112803185.png" alt="image-20200726112803185" style="zoom:33%;"><h3 id="褶皱画法"><a class="markdownIt-Anchor" href="#褶皱画法"></a> 褶皱画法</h3><h4 id="穿插"><a class="markdownIt-Anchor" href="#穿插"></a> 穿插</h4><p>简单的说，穿插就是A插到了B里面。由于褶皱弯曲，有时候会鼓起来突出一块，其效果就是下游的布料插进了上游的布料所形成的褶子里面。</p><img data-src="image-20200726120758174.png" alt="image-20200726120758174" style="zoom:50%;"><img data-src="image-20200726120727471.png" alt="image-20200726120727471" style="zoom:50%;"><p>由于视角的不同，让穿插呈现出2中绘制结构。</p><p>运用范围：衣袖、腰部、手肘（弯曲）、腿部（弯曲）。。。</p><h4 id="堆叠"><a class="markdownIt-Anchor" href="#堆叠"></a> 堆叠</h4><p>堆叠，即把过长的部分堆起来。其形如阶梯，但各阶形异。论其走线，莫有完全平行者，目之所及，皆为交错向下；间有内陷，若小水洼。</p><img data-src="image-20200726141313049.png" alt="image-20200726141313049" style="zoom:50%;"><h4 id="裁缝线"><a class="markdownIt-Anchor" href="#裁缝线"></a> 裁缝线</h4><ul><li><p>作用：分摊拉力。假设如果没有裁缝线时只有几个大的衣褶纹路，此时加上一根裁缝线，则在原来的褶皱纹路基础上会出现许多细小的褶子，此即为所谓“分摊拉力”。</p><img data-src="image-20200730133455227.png" alt="image-20200730133455227" style="zoom:33%;"></li></ul><h4 id="雷区"><a class="markdownIt-Anchor" href="#雷区"></a> 雷区</h4><ol><li>褶皱太少，状似铁皮（布的质感）</li><li>衣褶过多，如着破布（不美观）</li><li>衣褶位置不对</li><li>线条太实或太虚</li></ol><h4 id="服装的质感软与硬"><a class="markdownIt-Anchor" href="#服装的质感软与硬"></a> 服装的质感：软与硬</h4><ul><li>软的衣服（如水手服、T恤）：<ul><li>褶皱相对偏多</li><li>褶皱规律性弱</li><li>褶皱线条柔软</li><li>布纹偏多</li></ul></li><li>硬的衣服（如西服、制服）：<ul><li>褶皱相对较少</li><li>褶皱规律相对较强</li><li>褶皱线条偏硬</li><li>布纹偏少</li></ul></li><li>实际区分方法：线条</li></ul><h3 id="衣物绘制注意事项"><a class="markdownIt-Anchor" href="#衣物绘制注意事项"></a> 衣物绘制注意事项</h3><h4 id="厚度"><a class="markdownIt-Anchor" href="#厚度"></a> 厚度</h4><p>布是有厚度的，我们画布的时候要注意在垂下的边角处线与线的交汇处不要接死，留出一定的空隙以体现布料的厚度。</p><img data-src="image-20200730134510170.png" alt="image-20200730134510170" style="zoom:33%;"><h4 id="虚实"><a class="markdownIt-Anchor" href="#虚实"></a> 虚实</h4><p>我们在画衣褶的时候要注意线的虚实变化。交代清楚线的虚实，衣纹会更有活力，更生动。</p><p>外粗里细，主要褶皱粗、次要衣纹细。</p><h4 id="大小对比"><a class="markdownIt-Anchor" href="#大小对比"></a> 大小对比</h4><p>我们画衣纹的时候也要注意有大小对比，不要把每一个纹路画的一样大，这样会显得呆板无趣。</p><img data-src="image-20200730135011004.png" alt="image-20200730135011004" style="zoom:33%;"><h3 id="jk制服的画法"><a class="markdownIt-Anchor" href="#jk制服的画法"></a> JK制服的画法</h3><h4 id="分类-2"><a class="markdownIt-Anchor" href="#分类-2"></a> 分类</h4><p><strong>上衣</strong>：水手服、衬衫+（∅、针织衫、西服、马甲、背心）</p><p><strong>裙子</strong>：格子裙、净色裙、净色+条纹</p><p><strong>领饰</strong>：领绳、领带、领结、领巾</p><h4 id="变化"><a class="markdownIt-Anchor" href="#变化"></a> 变化</h4><p><strong>领口</strong>：圆领、尖领；长领口、短领口（向下延伸长度）；领子上带条纹、不带条纹</p><img data-src="image-20200811011515263.png" alt="image-20200811011515263" style="zoom:50%;"><img data-src="image-20200811011529584.png" alt="image-20200811011529584" style="zoom: 33%;"><p><strong>百褶裙堆叠方式</strong>：上下上下式、阶梯式</p><img data-src="image-20200811011403507.png" alt="image-20200811011403507" style="zoom: 33%;"><h4 id="特点"><a class="markdownIt-Anchor" href="#特点"></a> 特点</h4><ol><li>水手服并不是收腰紧身的，而是使用了相对较硬的材质、在胸部以下更偏向线性延伸的。</li></ol><h4 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h4><img data-src="image-20200811013940137.png" alt="image-20200811013940137" style="zoom:33%;"><h3 id="花边的画法"><a class="markdownIt-Anchor" href="#花边的画法"></a> 花边的画法</h3><img data-src="image-20200811112029242.png" alt="image-20200811112029242" style="zoom:33%;"><h3 id="服装的设计"><a class="markdownIt-Anchor" href="#服装的设计"></a> 服装的设计</h3><ol><li>元素的组合——主元素、辅元素。如以和风为主，但辅以花边和宽松裙摆等Lolita的设计。</li><li>变形：打破传统，求异。如传统的和服袖子都很长，领结都在领子下面，而求异的设计就可以把和服的袖子缩短、领结直接绑在颈部。</li><li>切割：衣服的虚与实。这里的实指的是有布料覆盖的地方，而虚则是指切开露出、没有布料覆盖的位置。虚的位置往往可以出现在肩部、腰部、腿部、胳膊等。</li><li>对比：<ol><li>体积的对比（大、小）</li><li>长度的对比（长、短）</li><li>娇小的角色穿宽松较大的服装（如埃罗芒阿老师、点兔、小埋）。宽大的衣服更显得角色本身的娇小。</li><li>性感的角色产布料较少的衣服，突出其身材</li></ol></li><li>剪影：整体上突出主元素</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;日系画风与现实的区别&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#日系画风与现实的区别&quot;&gt;&lt;/a&gt; 日系画风与现实的区别&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;真实的“&lt;strong&gt;三庭五眼&lt;/strong&gt;”：
&lt;ul&gt;
&lt;li&gt;三庭：&lt;strong&gt;d(发际线, 眉骨) = d(眉骨, 鼻底) = d(鼻底, 下巴)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;五眼：&lt;strong&gt;眼宽=眼间距=眼睛到面部轮廓的距离&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;眼睛的长度相对真实比例更长的，并不符合三庭五眼中的五眼比例。而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。&lt;/li&gt;
&lt;/ul&gt;
&lt;img data-src=&quot;Screenshot%2520-%25202020-06-12%252022.46.34.png&quot; alt=&quot;Screenshot - 2020-06-12 22.46.34&quot; style=&quot;zoom:50%;&quot;&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
    <category term="human-body" scheme="https://www.miracleyoo.com/tags/human-body/"/>
    
  </entry>
  
  <entry>
    <title>Cascade, Recursive, Residual and Dense 辨析</title>
    <link href="https://www.miracleyoo.com/2020/08/05/cascade-recursive/"/>
    <id>https://www.miracleyoo.com/2020/08/05/cascade-recursive/</id>
    <published>2020-08-06T00:00:45.000Z</published>
    <updated>2021-03-12T22:03:25.619Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cascade"><a class="markdownIt-Anchor" href="#cascade"></a> Cascade</h2><p>相当于Progressive Optimization，每个阶段都会输出一个和最终结果形状相同的Matrix，如目标分布的图像、BBox等，然后下一个block的作用则是输入这个Matrix和前面提取的Features，输出Refined后的Matrix，该步骤不断重复。核心是逐步优化。</p><span id="more"></span><h2 id="recursive"><a class="markdownIt-Anchor" href="#recursive"></a> Recursive</h2><p>相当于把一块Conv block重复了好多次，每次的权重是共享的。核心作用是节省内存和参数量、节省运算时间。同时它也含有时域特征。</p><h2 id="residual"><a class="markdownIt-Anchor" href="#residual"></a> Residual</h2><p>保留一条“信息高速公路”，使得前一轮的输出可以直接点加到经过了新一轮的Block卷积过后的结果上。核心作用是解决梯度消失问题，同时在网络的各层保留了不同层级的信息。变形有如Residual in Residual。</p><p><img data-src="v2-862e1c2dcb24f10d264544190ad38142_1440w.jpg" alt="img"></p><blockquote><p>ResNet网络的短路连接机制（其中+代表的是元素级相加操作）</p></blockquote><h2 id="dense"><a class="markdownIt-Anchor" href="#dense"></a> Dense</h2><p>每个Conv Block的输出会在Channel维上和后面所有Conv Block的输出Concate到一起。注意和Residual结构的区别，前者是直接逐点相加，而Dense则是并到Channel维度上。</p><p><img data-src="v2-2cb01c1c9a217e56c72f4c24096fe3fe_1440w.jpg" alt="img"></p><blockquote><p>DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）</p></blockquote><p><img data-src="v2-0a9db078f505b469973974aee9c27605_1440w.jpg" alt="img"></p><blockquote><p>DenseNet的前向过程</p></blockquote><p><img data-src="v2-c81da515c8fa9796601fde82e4d36f61_1440w.jpg" alt="img"></p><blockquote><p>原图</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;cascade&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#cascade&quot;&gt;&lt;/a&gt; Cascade&lt;/h2&gt;
&lt;p&gt;相当于Progressive Optimization，每个阶段都会输出一个和最终结果形状相同的Matrix，如目标分布的图像、BBox等，然后下一个block的作用则是输入这个Matrix和前面提取的Features，输出Refined后的Matrix，该步骤不断重复。核心是逐步优化。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>IoU, AP, mAP等对比</title>
    <link href="https://www.miracleyoo.com/2020/07/14/iou-ap-map/"/>
    <id>https://www.miracleyoo.com/2020/07/14/iou-ap-map/</id>
    <published>2020-07-15T01:01:42.000Z</published>
    <updated>2021-03-12T22:15:03.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="iou-ap-map-map05-map05-095-average-map"><a class="markdownIt-Anchor" href="#iou-ap-map-map05-map05-095-average-map"></a> IoU, AP, mAP, mAP@0.5, mAP@[0.5: 0.95],  Average mAP</h1><h2 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> TL;DR</h2><ol><li>IoU：两个框框重叠部分面积/两个框框合并后的总面积​</li><li>AP：绘制Recall-Precision图，经过平滑后曲线的下面全面积。这个图的绘制方法是：按照每个预测结果的Confidence从上往下排列，先只取一个画出图上左上角第一个点，然后是只取前两个，直到取完。</li><li>mAP：AP是针对某一个类的，而mAP是把各个类的AP做一个平均。</li><li>mAP@0.5：当IoU阈值为0.5时的mAP。</li><li>mAP@[0.5:0.95]：COCO要求IoU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均。</li></ol><span id="more"></span><h2 id="查准率precision和查全率recall"><a class="markdownIt-Anchor" href="#查准率precision和查全率recall"></a> <strong>查准率（Precision）和查全率（recall）</strong></h2><p>查准率（Precision）是指在所有预测为正例中真正例的比率，也即预测的准确性。</p><p>查全率（Recall）是指在所有正例中被正确预测的比率，也即预测正确的覆盖率。</p><p>一个样本模型预测按正确与否分类如下：</p><p>真正例： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>P</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">TP=True\space Positive</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span></span></span></span></p><p>真反例： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>N</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mtext> </mtext><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">TN=True\space Negative</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span></span></span></span></p><p>假正例：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>P</mi><mo>=</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">FP=False\space Positive</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span></span></span></span></p><p>假反例：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>N</mi><mo>=</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mtext> </mtext><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">FN=False\space Negative</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span></span></span></span></p><p><strong>则，查准率和查全率计算公式：</strong></p><p><strong>查准率</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Precision=\frac{TP}{TP+FP}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.275662em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><p><strong>查全率</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Recall=\frac{TP}{TP+FN}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.275662em;vertical-align:-0.403331em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><h2 id="交并比iouintersection-over-union"><a class="markdownIt-Anchor" href="#交并比iouintersection-over-union"></a> <strong>交并比IoU(Intersection over union)</strong></h2><p>交并比IoU衡量的是两个区域的重叠程度，是两个区域重叠部分面积占二者总面积（重叠部分只计算一次）的比例。如下图，两个矩形框的IoU是交叉面积（中间图片红色部分）与合并面积（右图红色部分）面积之比。</p><p><img data-src="v2-11ed1bf4a882ee38f9ea1f73a2593472_1440w.jpg" alt="img">IoU计算重叠度</p><p>这里需要注意的是IoU=0.5，并不意味着每个框刚好有50%与另外一个框交叉部分，而是每个框大约有2/3被交叉。有点反直觉。</p><p>我当初看到IoU，非常疑惑为啥不按交叉面积占每个框的比例（IoA 也即Intersection over Area）取大值计算重叠度，更符合直觉。其实这种算法只反应小图片的被遮盖度，并不能反映互相之间的重叠度，一般情况下不可取。如下图，橙色部分较小，IoA很大，但对于蓝色部分，IoA就很小，只按橙色取IoA显然有失偏驳。</p><p><img data-src="v2-284022eaa7bbb8dd7b4f8488e0495fcd_1440w.jpg" alt="img">IoA计算重叠度</p><h2 id="单类别apaverage-precision的计算"><a class="markdownIt-Anchor" href="#单类别apaverage-precision的计算"></a> <strong>单类别AP(Average Precision)的计算</strong></h2><p>物体检测中的每一个预测结果包含两部分，预测框（bounding box）和置信概率（Pc）。bounding box通常以矩形预测框的左上角和右下角的坐标表示，即x_min, y_min, x_max, y_max，如下图。置信概率Pc有两层意思，一是所预测bounding box的类别，二是这个类别的置信概率，如下图中的P_dog=0.88，代表预测绿色框为dog，并且置信概率为88%。</p><p><img data-src="v2-8e1e070d1a59043a349eb1f921ea1e1c_1440w.jpg" alt="img"></p><p>那么，怎么才叫预测正确呢？显而易见的，必须满足两个条件：</p><ol><li>类别正确且置信度大于一定阀值（P_threshold）</li><li>预测框与真实框（ground truth）的IoU大于一定阀值（IoU_threshold）</li></ol><p>如下图，假如P_threshold=0.6，IoU_threshold=0.5，则绿色框预测正确，记为True Positive。</p><p><img data-src="v2-fa34f541cee564e83435562297e768ab_1440w.jpg" alt="img"></p><p>而在衡量模型性能时，IoU_threshold先取一个定值，然后综合考虑各种P_threshold取值时的性能，进而得到一个与P_threshold选定无关的模型性能衡量标准。</p><p><strong>AP是计算单类别的模型平均准确度。</strong></p><p>假如目标类别为Dog，有5张照片，共包含7只Dog，也即GT（Ground Truth）数量为7，经模型预测，得到了Dog的10个预测结果，选定IoU_threshold=0.5，然后按confidence从高到低排序，如下图。其中，BB表示Bounding Box序号，GT=1表示有GT与所预测的Bounding Box的IoU&gt;=IoU_threshold，Bounding Box序号相同代表所对应的GT是同一个。</p><table><thead><tr><th>Rank</th><th>BB</th><th>confidence</th><th>GT</th></tr></thead><tbody><tr><td>1</td><td>BB1</td><td>0.9</td><td>1</td></tr><tr><td>2</td><td>BB2</td><td>0.8</td><td>1</td></tr><tr><td>3</td><td>BB1</td><td>0.8</td><td>1</td></tr><tr><td>4</td><td>BB3</td><td>0.5</td><td>0</td></tr><tr><td>5</td><td>BB4</td><td>0.4</td><td>0</td></tr><tr><td>6</td><td>BB5</td><td>0.4</td><td>1</td></tr><tr><td>7</td><td>BB6</td><td>0.3</td><td>0</td></tr><tr><td>8</td><td>BB7</td><td>0.2</td><td>0</td></tr><tr><td>9</td><td>BB8</td><td>0.1</td><td>1</td></tr><tr><td>10</td><td>BB9</td><td>0.1</td><td>1</td></tr></tbody></table><p>因此，如果设置P_threshold=0，则有 TP=5 (BB1, BB2, BB5, BB8, BB9)，FP=5 (重复检测到的BB1也算FP)。除了表里检测到的5个GT以外，我们还有2个GT没被检测到，因此: FN = 2.</p><p>然后依次从上到下设定对应的rank为正反分界线，此rank之前（包含此rank）的预测为正，此rank之后的预测为反，然后计算对应的Precision和Recall：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">rank=1  precision=1.00 and recall=0.14</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=2  precision=1.00 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=3  precision=0.66 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=4  precision=0.50 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=5  precision=0.40 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=6  precision=0.50 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=7  precision=0.43 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=8  precision=0.38 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=9  precision=0.44 and recall=0.57</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=10 precision=0.50 and recall=0.71</span><br><span class="line">--------------------------------------</span><br></pre></td></tr></table></figure><p>比如rank=4时，TP=2 (BB1, BB2)，则</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Precision=2/4=0.5，Recall=TP/GT=2/7=0.29</span><br></pre></td></tr></table></figure><p>可以看出，随着预测正反分割线的向下移动，Recall稳步变大，Precision整体减小，局部上下跳动，PR曲线如下图：</p><p><img data-src="v2-0a899369aeab8824dc3dd3e4fe572cd3_1440w.jpg" alt="img"></p><p>AP(Average Precision)的计算基本等同于计算PR曲线下的面积，但略有不同。需要先将PR曲线平滑化。</p><p>方法是，查全率r对应的查准率p，取查全率大于等于r时最大的查准率p。即，</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mi>e</mi><mi>x</mi><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo><mi>max</mi><mo>⁡</mo></mo><mrow><mover accent="true"><mi>r</mi><mo>~</mo></mover><mo>≥</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi>r</mi><mo>~</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">tex=p(r)=\max_{\tilde{r}\geq r}{p(\tilde{r})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.578681em;vertical-align:-0.828681em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43055999999999983em;"><span style="top:-2.366498em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span><span style="top:-3.0500000000000003em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord mtight">~</span></span></span></span></span></span></span><span class="mrel mtight">≥</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.828681em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></p><p>平滑后的曲线如下图中的绿色曲线：</p><p><img data-src="v2-666e46a022e32981aeb07b85958803cc_1440w.jpg" alt="img"></p><p>对于AP(Average Precision)的计算有两种方法：</p><p><strong>1. VOC2010之前的方法</strong></p><p>AP =（平滑后PR曲线上，Recall分别等于0，0.1，0.2，… , 1.0等11处Precision的平均值）。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mn>1</mn><mn>11</mn></mfrac><munder><mo>∑</mo><mrow><mi>r</mi><mo>⊆</mo><mrow><mo fence="true">{</mo><mn>0</mn><mo separator="true">,</mo><mn>0.1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>1.0</mn><mo fence="true">}</mo></mrow></mrow></munder><mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>r</mi><mo fence="true">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">AP=\frac{1}{11}\sum_{r\subseteq\left\{0,0.1,..,1.0\right\}}{p\left(r\right)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.8374449999999998em;vertical-align:-1.516005em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">1</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.808995em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mrel mtight">⊆</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">{</span></span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span><span class="mord mtight">.</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">.</span><span class="mord mtight">.</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mord mtight">.</span><span class="mord mtight">0</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">}</span></span></span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516005em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span></span></p><p>这里则有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AP = (1 + 1 + 1 + 0.5 + 0.5 + 0.5 + 0.5 + 0.5 + 0 + 0 + 0) / 11 = 0.5</span><br></pre></td></tr></table></figure><p><strong>2. VOC2010及以后的方法</strong></p><p>AP=平滑后PR曲线下包围的面积</p><p>这里则有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AP = (0.14-0) * 1 + (0.29-0.14) * 1 + (0.43-0.29) * 0.5 + (0.57-0.43) * 0.5 + (0.71-0.57) * 0.5 + (1-0.71) * 0 = 0.5</span><br></pre></td></tr></table></figure><p>这里两种方案得出的AP值相同，但通常是不同的。</p><p>需要注意的是上述AP的计算并没有显式设定<code>P_threshold</code>，而是通过从上到下依次指定每一个rank为正反分界线来变相的反映<code>P_threshold</code>不同取值。</p><h2 id="map的计算"><a class="markdownIt-Anchor" href="#map的计算"></a> <strong>mAP的计算</strong></h2><p>上述计算的AP只是针对dog这个类别，物体检测通常有多个类别，模型性能肯定是多个类别准度的综合度量。</p><p><strong>1. VOC数据集中的mAP</strong></p><p>VOC数据集中的mAP计算的是<code>IoU_threshold=0.5</code>时各个类别AP的均值。</p><p><strong>2. COCO数据集中的mAP</strong></p><p>检测是否正确有两个超参数，<code>P_threshold</code>和<code>IoU_threshold</code>。AP是固定了<code>IoU_threshold</code>，再综合考虑各个<code>P_threshold</code>下的模型平均准确度。</p><p>VOC认为<code>IoU_threshold</code>固定一个单值0.5即可，COCO则认为固定了<code>IoU_threshold</code>的取值，无法衡量<code>IoU_threshold</code>对模型性能的影响。</p><p>比如，</p><p>A模型在<code>IoU_threshold=0.5</code>时，<code>mAP=0.4</code>。</p><p>B模型在<code>IoU_threshold=0.7</code>时，<code>mAP</code>同样为0.4。</p><p>依据VOC的标准，AB模型的性能一样，但显然B模型的框更准，性能更优。</p><p>COCO在VOC标准的基础上，取<code>IoU_threshold=0.5，0.55， 0.6，… , 0.95</code>时各个mAP的均值。</p><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ol><li><a href="https://arleyzhang.github.io/articles/c521a01c/">目标检测评价标准-AP mAP</a></li><li><a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html">多标签图像分类任务的评价方法-mAP</a></li><li><a href="https://zhuanlan.zhihu.com/p/56961620">详解object detection中的mAP</a></li><li><a href="https://blog.csdn.net/luke_sanjayzzzhong/article/details/89851944">对于目标检测中mAP@0.5的理解</a></li><li><a href="https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean">What does the notation mAP@[.5:.95] mean?</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;iou-ap-map-map05-map05-095-average-map&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#iou-ap-map-map05-map05-095-average-map&quot;&gt;&lt;/a&gt; IoU, AP, mAP, mAP@0.5, mAP@[0.5: 0.95],  Average mAP&lt;/h1&gt;
&lt;h2 id=&quot;tldr&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#tldr&quot;&gt;&lt;/a&gt; TL;DR&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;IoU：两个框框重叠部分面积/两个框框合并后的总面积​&lt;/li&gt;
&lt;li&gt;AP：绘制Recall-Precision图，经过平滑后曲线的下面全面积。这个图的绘制方法是：按照每个预测结果的Confidence从上往下排列，先只取一个画出图上左上角第一个点，然后是只取前两个，直到取完。&lt;/li&gt;
&lt;li&gt;mAP：AP是针对某一个类的，而mAP是把各个类的AP做一个平均。&lt;/li&gt;
&lt;li&gt;mAP@0.5：当IoU阈值为0.5时的mAP。&lt;/li&gt;
&lt;li&gt;mAP@[0.5:0.95]：COCO要求IoU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch官方模型实现分析</title>
    <link href="https://www.miracleyoo.com/2020/07/11/pytorch-official-network-analysis/"/>
    <id>https://www.miracleyoo.com/2020/07/11/pytorch-official-network-analysis/</id>
    <published>2020-07-11T20:08:17.000Z</published>
    <updated>2021-03-12T22:08:45.249Z</updated>
    
    <content type="html"><![CDATA[<h2 id="resnet"><a class="markdownIt-Anchor" href="#resnet"></a> Resnet</h2><h3 id="如何应对不同尺寸输入"><a class="markdownIt-Anchor" href="#如何应对不同尺寸输入"></a> 如何应对不同尺寸输入</h3><p>在网络最后添加一个<code>AdaptiveAvgPool2d(output_size)</code>函数，它的作用是无论输入图片形状如何，最终都会转换为给定输出尺寸。而Resnet中，这个<code>output_size</code>被设置为了<code>(1,1)</code>，即无论输入的图片尺寸为多少，只要其大小足以扛得住网络前面的一系列pooling layers，到最后的输出尺寸大于等于<code>(1,1)</code>，其每个Channel就会被这一层压缩成一个点，即最后只会得到一个与Channel数目相等的向量。这个向量被送到了FC层。</p><span id="more"></span><h3 id="如何应对channel数目不合适问题"><a class="markdownIt-Anchor" href="#如何应对channel数目不合适问题"></a> 如何应对Channel数目不合适问题</h3><p>由于兼容了各种大小和尺寸的模型，所以有时难免会出现如Channel数目无法被4整除（BottleNeck Layer要求）的情况。这里，它使用了<code>1x1 conv</code>的方法。由于每个block的输出都要加到输入上，如果这个整除不了，结果就是Channel数目不匹配无法做Residual。这里的操作是，如果输出<code>Channel数*4 != 输入Channel数</code>，那么就直接让输入先用<code>1x1 conv</code>改变维度到<code>输出Channel数*4</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">    downsample = nn.Sequential(</span><br><span class="line">        conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">        norm_layer(planes * block.expansion),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="如何应对不同总layer数的resnet使用不同的layer-block的问题"><a class="markdownIt-Anchor" href="#如何应对不同总layer数的resnet使用不同的layer-block的问题"></a> 如何应对不同总Layer数的Resnet使用不同的Layer Block的问题</h3><p>在Resnet中，resnet18, 34使用的是双层<code>3x3 conv</code>的<code>Basic Block</code>，而resnet50, 101, 152则使用的是<code>1x1conv -&gt; 3x3 conv -&gt; 1x1 conv</code>的结构。为了获得最大的兼容性，这里官方模型将<code>Basic Block</code>和<code>Bottleneck Block</code>分别定义为两个class，即子模块，然后对于不同尺寸的resnet分别输入不同的模块。</p><h3 id="为什么定义了conv3x3和conv1x1两个函数"><a class="markdownIt-Anchor" href="#为什么定义了conv3x3和conv1x1两个函数"></a> 为什么定义了conv3x3和conv1x1两个函数</h3><p>这两个函数看似画蛇添足多此一举，但实际上在我的理解中，他们避免了一些重复变量的输入，更直观地反映了该层的功能：<code>1x1</code>或<code>3x3</code>，也潜在地避免了一些错误，并优化了理解。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;resnet&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#resnet&quot;&gt;&lt;/a&gt; Resnet&lt;/h2&gt;
&lt;h3 id=&quot;如何应对不同尺寸输入&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#如何应对不同尺寸输入&quot;&gt;&lt;/a&gt; 如何应对不同尺寸输入&lt;/h3&gt;
&lt;p&gt;在网络最后添加一个&lt;code&gt;AdaptiveAvgPool2d(output_size)&lt;/code&gt;函数，它的作用是无论输入图片形状如何，最终都会转换为给定输出尺寸。而Resnet中，这个&lt;code&gt;output_size&lt;/code&gt;被设置为了&lt;code&gt;(1,1)&lt;/code&gt;，即无论输入的图片尺寸为多少，只要其大小足以扛得住网络前面的一系列pooling layers，到最后的输出尺寸大于等于&lt;code&gt;(1,1)&lt;/code&gt;，其每个Channel就会被这一层压缩成一个点，即最后只会得到一个与Channel数目相等的向量。这个向量被送到了FC层。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Attention 从分类到实现细节</title>
    <link href="https://www.miracleyoo.com/2020/07/06/attention-wiki/"/>
    <id>https://www.miracleyoo.com/2020/07/06/attention-wiki/</id>
    <published>2020-07-07T00:13:30.000Z</published>
    <updated>2021-03-12T22:16:17.487Z</updated>
    
    <content type="html"><![CDATA[<p>Attention的本质可以看做加权求和。</p><h2 id="attention-的-n-种类型"><a class="markdownIt-Anchor" href="#attention-的-n-种类型"></a> Attention 的 N 种类型</h2><p><img data-src="image-20200705230846450.png" alt="image-20200705230846450"></p><p>Attention 有很多种不同的类型：Soft Attention、Hard Attention、静态 Attention、动态 Attention、Self Attention 等等。上图为各种Attention的分类，下面是这些不同的 Attention 的解释。</p><p>由于这篇文章《<a href="https://zhuanlan.zhihu.com/p/35739040">Attention 用于 NLP 的一些小结</a>》已经总结的很好的，下面就直接引用了：</p><p>本节从计算区域、所用信息、结构层次和模型等方面对 Attention 的形式进行归类。</p><span id="more"></span><h3 id="1-计算区域"><a class="markdownIt-Anchor" href="#1-计算区域"></a> <strong>1. 计算区域</strong></h3><p>根据 Attention 的计算区域，可以分成以下几种：</p><p>1）<strong>Soft</strong> Attention，这是比较常见的 Attention 方式，对所有 key 求权重概率，每个 key 都有一个对应的权重，是一种全局的计算方式（也可以叫 Global Attention）。这种方式比较理性，参考了所有 key 的内容，再进行加权。但是计算量可能会比较大一些。</p><p>2）<strong>Hard</strong> Attention，这种方式是直接精准定位到某个 key，其余 key 就都不管了，相当于这个 key 的概率是 1，其余 key 的概率全部是 0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用 gumbel softmax 之类的）</p><p>3）<strong>Local</strong> Attention，这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用 Hard 方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用 Soft 方式来算 Attention。</p><h3 id="2-所用信息"><a class="markdownIt-Anchor" href="#2-所用信息"></a> <strong>2. 所用信息</strong></h3><p>假设我们要对一段原文计算 Attention，这里原文指的是我们要做 attention 的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息。</p><p>1）<strong>General</strong> Attention，这种方式利用到了外部信息，常用于需要构建两段文本关系的任务，query 一般包含了额外信息，根据外部 query 对原文进行对齐。</p><p>简单判定依据：<strong>计算Attention时，有没有用到除了被Attention向量以外的向量。</strong></p><p>比如在阅读理解任务中，需要构建问题和文章的关联，假设现在 baseline 是，对问题计算出一个问题向量 q，把这个 q 和所有的文章词向量拼接起来，输入到 LSTM中进行建模。那么在这个模型中，文章所有词向量共享同一个问题向量，现在我们想让文章每一步的词向量都有一个不同的问题向量，也就是，在每一步使用文章在该步下的词向量对问题来算 attention，这里问题属于原文，文章词向量就属于外部信息。</p><p><img data-src="v2-1e08348b226f4e2f08c89ae6e5d7fcda_1440w.jpg" alt="img"></p><p>2）<strong>Self</strong> Attention，这种方式只使用内部信息，key 和 value 以及 query 只和输入原文有关，在 self attention 中，key=value=query。既然没有外部信息，那么在原文中的每个词可以跟该句子中的所有词进行 Attention 计算，相当于寻找原文内部的关系。</p><p>还是举阅读理解任务的例子，上面的 baseline 中提到，对问题计算出一个向量 q，那么这里也可以用上 attention，只用问题自身的信息去做 attention，而不引入文章信息。</p><p>同样是在Encoder-Decoder模型中，它的实现方法是Encoder部分堆叠了两层。</p><img data-src="v2-2c7e48868e98202b6c3af3a7aa4ea987_1440w.jpg" alt="img" style="zoom:50%;"><h3 id="3-结构层次"><a class="markdownIt-Anchor" href="#3-结构层次"></a> <strong>3. 结构层次</strong></h3><p>结构方面根据是否划分层次关系，分为单层 attention，多层 attention 和多头 attention：</p><p>1）单层 Attention，这是比较普遍的做法，用一个 query 对一段原文进行一次 attention。</p><p>2）多层 Attention，一般用于文本具有层次关系的模型，假设我们把一个 document 划分成多个句子，在第一层，我们分别对每个句子使用 attention 计算出一个句向量（也就是单层 attention）；在第二层，我们对所有句向量再做 attention 计算出一个文档向量（也是一个单层 attention），最后再用这个文档向量去做任务。</p><p>3）多头 Attention，这是 Attention is All You Need 中提到的 multi-head attention，用到了多个 query 对一段原文进行了多次 attention，每个 query 都关注到原文的不同部分，相当于重复做多次单层 attention：$$head_i=Attention(q_i,K,V)$$</p><p>最后再把这些结果拼接起来：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>H</mi><mi>e</mi><mi>a</mi><mi>d</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>n</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">MultiHead(Q, K, V)=Concat(head_1,head_2,head_3,...,head_n)W^O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></p><p><img data-src="v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_1440w.jpg" alt="img"></p><h3 id="4-模型方面"><a class="markdownIt-Anchor" href="#4-模型方面"></a> <strong>4. 模型方面</strong></h3><p>从模型上看，Attention 一般用在 CNN 和 LSTM 上，也可以直接进行纯 Attention 计算。</p><h4 id="1cnnattention"><a class="markdownIt-Anchor" href="#1cnnattention"></a> <strong>1）CNN+Attention</strong></h4><p>CNN 的卷积操作可以提取重要特征，我觉得这也算是 Attention 的思想，但是 CNN 的卷积感受视野是局部的，需要通过叠加多层卷积区去扩大视野。另外，Max Pooling 直接提取数值最大的特征，也像是 hard attention 的思想，直接选中某个特征。</p><p>CNN 上加 Attention 可以加在这几方面：</p><p>a. 在卷积操作前做 attention，比如 Attention-Based BCNN-1，这个任务是文本蕴含任务需要处理两段文本，同时对两段输入的序列向量进行 attention，计算出特征向量，再拼接到原始向量中，作为卷积层的输入。</p><p>b. 在卷积操作后做 attention，比如 Attention-Based BCNN-2，对两段文本的卷积层的输出做 attention，作为 pooling 层的输入。</p><p>c. 在 pooling 层做 attention，代替 max pooling。比如 Attention pooling，首先我们用 LSTM 学到一个比较好的句向量，作为 query，然后用 CNN 先学习到一个特征矩阵作为 key，再用 query 对 key 产生权重，进行 attention，得到最后的句向量。</p><h4 id="2lstmattention"><a class="markdownIt-Anchor" href="#2lstmattention"></a> <strong>2）LSTM+Attention</strong></h4><p>LSTM 内部有 Gate 机制，其中 input gate 选择哪些当前信息进行输入，forget gate 选择遗忘哪些过去信息，我觉得这算是一定程度的 Attention 了，而且号称可以解决长期依赖问题，实际上 LSTM 需要一步一步去捕捉序列信息，在长文本上的表现是会随着 step 增加而慢慢衰减，难以保留全部的有用信息。</p><p>LSTM 通常需要得到一个向量，再去做任务，常用方式有：</p><p>a. 直接使用最后的 hidden state（可能会损失一定的前文信息，难以表达全文）</p><p>b. 对所有 step 下的 hidden state 进行等权平均（对所有 step 一视同仁）。</p><p>c. Attention 机制，对所有 step 的 hidden state 进行加权，把注意力集中到整段文本中比较重要的 hidden state 信息。性能比前面两种要好一点，而方便可视化观察哪些 step 是重要的，但是要小心过拟合，而且也增加了计算量。</p><h4 id="3纯-attention"><a class="markdownIt-Anchor" href="#3纯-attention"></a> <strong>3）纯 Attention</strong></h4><p>Attention is all you need，没有用到 CNN/RNN，乍一听也是一股清流了，但是仔细一看，本质上还是一堆向量去计算 attention。本文提出了Transformer。Transformer 也可以视为一种自带Attention机制的RNN。它使用了问题、键、值三个向量，让权重的计算变得更加细致。</p><p>Transformer可以说是集近些年的研究之于大成。里面涉及到很多很多技术点，包括：</p><ul><li>Feed Forward Network</li><li>ResNet的思想</li><li>Positional Embedding 解决输入时序问题</li><li>Layer Normalization</li><li>Decoder中的Masked Self-Attention</li></ul><p><img data-src="transformer_resideual_layer_norm_3.png" alt="img"></p><img data-src="self-attention-output.png" alt="img" style="zoom:50%;"><img data-src="self-attention-matrix-calculation.png" alt="img" style="zoom:50%;"><img data-src="self-attention-matrix-calculation-2.png" alt="img" style="zoom:50%;"><h3 id="5-相似度计算方式"><a class="markdownIt-Anchor" href="#5-相似度计算方式"></a> <strong>5. 相似度计算方式</strong></h3><p>在做 attention 的时候，我们需要计算 query 和某个 key 的分数（相似度），常用方法有：</p><p>1）点乘：最简单的方法， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>q</mi><mi>T</mi></msup><mi>k</mi></mrow><annotation encoding="application/x-tex">s(q,k)=q^Tk</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></p><p>2）矩阵相乘： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>q</mi><mi>T</mi></msup><mi>W</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">s(q,k)=q^TWk</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.035771em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></p><p>3）cos 相似度： $$s(q,k)=\frac{q^T}{||q||·||k||}$$</p><p>4）串联方式：把 q 和 k 拼接起来， $$s(q,k)=W[q;k]$$</p><p>5）用多层感知机也可以： $$s(q,k)=v^T_atanh(Wq+Uk)$$</p><h2 id="encoder-decoder模型结构"><a class="markdownIt-Anchor" href="#encoder-decoder模型结构"></a> Encoder-Decoder模型结构</h2><p>以机器翻译模型为例：<a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">原文</a></p><p><img data-src="seq2seq.png" alt="img"></p><p><img data-src="image-20200705230609732.png" alt="image-20200705230609732"></p><ul><li>Encoder的输入是句子中每个词对应的数字编号的序列，输出的是RNN每一个Cell的相应Output Vector（或是一次只输入一个词的编号，然后每次得到一个Output，最后拼成一个List）。其RNN的初始Hidden Layer是随机初始化（Random、全0）的。</li><li>Decoder的第一个输入是句子起始符<SOS>，数字编码可以为0，然后Hidden Layer使用Encoder的最后一个Hidden Layer Value初始化。每次只输入一个Input数字编号，然后下一次的Input或使用上次的预测结果，或是使用Output Label的相应值。前者被称作<em>Teacher Forcing</em>，后者是<em>Without Teacher Forcing</em>。</SOS></li><li>Attention是加在Decoder上的。Decoder 的input会先做embedding，之后么embedding和Hidden Layer参数Concate到一起，再过一个FC（<strong>过FC就相当于乘上了一个变换矩阵了</strong>）就得到了Attention。这个算出来的Attention再和Encoder的output做一个<code>torch.bmm</code>矩阵乘法（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>∗</mo><mn>1</mn><mo>∗</mo><mi>L</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">]</mo><mo>∗</mo><mo stretchy="false">[</mo><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>∗</mo><mi>L</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>∗</mo><mi>H</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[Batch\_Size*1*Length]*[Batch\_Size*Length*Hidden\_Size]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mclose">]</span></span></span></span>），得到一个向量([<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>∗</mo><mn>1</mn><mo>∗</mo><mi>H</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Batch\_Size*1*Hidden\_Size</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span></span></span></span>])，这个数值即为该input过了Attention后的值。该值再和embedding后的值([<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>∗</mo><mn>1</mn><mo>∗</mo><mi>H</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Batch\_Size*1*Hidden\_Size</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span></span></span></span>]) 使用Concate拼到一起，过一个FC，输出一个([<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>∗</mo><mn>1</mn><mo>∗</mo><mi>H</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>S</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Batch\_Size*1*Hidden\_Size</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span></span></span></span>])的向量，它便是施加了Attention后的Input。这里本质上使用了前面提到的权值计算方法中的 <strong>串联方式</strong>。</li><li>简单说，decoder的input和hidden layer只是用来计算encoder各个cell的output的权重的。每个output有着hidden_size维度，他们最终按照attention作为加权求和。或是说：<strong>Decoder中每一个Cell，去Encoder中寻找最相关的记忆。</strong></li></ul><h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2><h3 id="embedding"><a class="markdownIt-Anchor" href="#embedding"></a> <code>Embedding</code></h3><p><code>torch.nn.Embedding(*num_embeddings: int*, *embedding_dim: int*)</code></p><blockquote><p>To summarize <code>num_embeddings</code> is total number of unique elements in the vocabulary, and <code>embedding_dim</code> is the size of each embedded vector once passed through the embedding layer. Therefore, you can have a tensor of 10+ elements, as long as each element in the tensor is in the range <code>[0, 9]</code>, because you defined a vocabulary size of 10 elements.</p></blockquote><p>即第一个数<code>num_embedding</code>指的是你的输入中有多少可能的值，或者说语料库的大小；第二个数<code>embedding_dim</code>指的是给定一个input（一个digit），输出几个digit。</p><h3 id="bmm"><a class="markdownIt-Anchor" href="#bmm"></a> <code>BMM</code></h3><p><code>torch.bmm(*input*, *mat2*, *deterministic=False*, *out=None*) → Tensor</code></p><blockquote><p>Performs a batch matrix-matrix product of matrices stored in <code>input</code> and <code>mat2</code>.</p><p><code>input</code> and <code>mat2</code> must be 3-D tensors each containing the same number of matrices.</p><p>If <code>input</code> is a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(b \times n \times m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mclose">)</span></span></span></span> tensor, <code>mat2</code> is a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mo>×</mo><mi>m</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(b \times m \times p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span> tensor, <code>out</code> will be a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>b</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(b \times n \times p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span>tensor.</p></blockquote><p>简单说，这个函数的作用是在不动batch维度的情况下对其他维度执行矩阵乘法。</p><h2 id="self-attention-与-general-attention在实现上的区别"><a class="markdownIt-Anchor" href="#self-attention-与-general-attention在实现上的区别"></a> Self-Attention 与 General Attention在实现上的区别</h2><p>Self Attention 本质上是乘上一个和输入向量需要加Attention的维度等长的向量（<code>nn.Parameter(torch.Tensor(1, D), requires_grad=True)</code>），并做矩阵相乘。如输入是一个长为L的句子，句子中每个词的Embedding长度是D， batch为B，即(B, L, D)， 那么若是要对句子的长度维度做Attention，则需要乘一个shape为(B, D, 1)的向量，得到的Output shape为(B, L, 1)。之后还需做softmax，句子长度mask，结果除以单词个数保证所有weight加起来等于一，然后再使用这个output点乘input，即可得到和input shape相同，但被加了Attention后的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Pytorch self-attention layer code inspired from:</span></span><br><span class="line"><span class="string">    Link:</span></span><br><span class="line"><span class="string">        https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4</span></span><br><span class="line"><span class="string">    Original Web Page:</span></span><br><span class="line"><span class="string">        https://www.kaggle.com/dannykliu/lstm-with-attention-clr-in-pytorch</span></span><br><span class="line"><span class="string">    Usage:</span></span><br><span class="line"><span class="string">        In __init__():</span></span><br><span class="line"><span class="string">            self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional</span></span><br><span class="line"><span class="string">        In forward():</span></span><br><span class="line"><span class="string">            x, _ = self.atten1(x, lengths)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, batch_first=<span class="literal">True</span>, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.device = device</span><br><span class="line"></span><br><span class="line">        self.att_weights = nn.Parameter(</span><br><span class="line">            torch.Tensor(<span class="number">1</span>, hidden_size), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        stdv = <span class="number">1.0</span> / np.sqrt(self.hidden_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.att_weights:</span><br><span class="line">            nn.init.uniform_(weight, -stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_mask</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, lengths</span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            batch_size, max_len = inputs.size()[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            max_len, batch_size = inputs.size()[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply attention layer</span></span><br><span class="line">        weights = torch.bmm(inputs,</span><br><span class="line">                            self.att_weights  <span class="comment"># (1, hidden_size)</span></span><br><span class="line">                            .permute(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># (hidden_size, 1)</span></span><br><span class="line">                            .unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, hidden_size, 1)</span></span><br><span class="line">                            <span class="comment"># (batch_size, hidden_size, 1)</span></span><br><span class="line">                            .repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create mask based on the sentence lengths</span></span><br><span class="line">        mask = torch.ones(attentions.size(), requires_grad=<span class="literal">True</span>).to(self.device)</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(lengths):  <span class="comment"># skip the first sentence</span></span><br><span class="line">            <span class="keyword">if</span> l &lt; max_len:</span><br><span class="line">                mask[i, l:] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply mask and renormalize attention scores (weights)</span></span><br><span class="line">        masked = attentions * mask</span><br><span class="line">        _sums = masked.<span class="built_in">sum</span>(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)  <span class="comment"># sums per row</span></span><br><span class="line"></span><br><span class="line">        attentions = masked.div(_sums)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply attention weights</span></span><br><span class="line">        weighted = torch.mul(</span><br><span class="line">            inputs, attentions.unsqueeze(-<span class="number">1</span>).expand_as(inputs))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the final fixed vector representations of the sentences</span></span><br><span class="line">        representations = weighted.<span class="built_in">sum</span>(<span class="number">1</span>).squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> representations, attentions</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>而General Attention的本质则是一个FC（即一个映射矩阵）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span><br><span class="line">        embedded = self.embedding(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttnDecoderRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input: (1)</span></span><br><span class="line">    <span class="comment"># hidden: (1, D)</span></span><br><span class="line">    <span class="comment"># encoder_outputs: (L, D)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden, encoder_outputs</span>):</span><br><span class="line">        embedded = self.embedding(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># (1, 1, D)</span></span><br><span class="line">        embedded = self.dropout(embedded) <span class="comment"># (1, 1, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (1, 2D) -&gt; (1, L)</span></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (1, 1, L) x (1, L, D) -&gt; (1, 1, D)</span></span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>)) </span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>) <span class="comment"># (1, 2D)</span></span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>) <span class="comment"># (1, 1, D)</span></span><br><span class="line"></span><br><span class="line">        output = F.relu(output) <span class="comment"># (1, 1, D)</span></span><br><span class="line">        output, hidden = self.gru(output, hidden) <span class="comment"># (1, D)</span></span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>) <span class="comment"># (1, output_size)</span></span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH</span>):</span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> <span class="built_in">range</span>(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ol><li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION</a></li><li><a href="https://easyai.tech/ai-definition/attention/">Attention 机制 – EasyAI</a></li><li><a href="https://zhuanlan.zhihu.com/p/35739040">Attention用于NLP的一些小结</a></li><li><a href="https://github.com/EvilPsyCHo/Attention-PyTorch">Attention-PyTorch</a></li><li><a href="https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq/blob/master/attentionRNN.py">Pytorch Batch Attention Seq-2-Seq</a></li><li><a href="https://zhuanlan.zhihu.com/p/47282410">Attention机制详解（二）——Self-Attention与Transformer</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li><li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li><li><a href="https://www.zhihu.com/question/68482809">知乎：目前主流的attention方法都有哪些？</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Attention的本质可以看做加权求和。&lt;/p&gt;
&lt;h2 id=&quot;attention-的-n-种类型&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#attention-的-n-种类型&quot;&gt;&lt;/a&gt; Attention 的 N 种类型&lt;/h2&gt;
&lt;p&gt;&lt;img data-src=&quot;image-20200705230846450.png&quot; alt=&quot;image-20200705230846450&quot;&gt;&lt;/p&gt;
&lt;p&gt;Attention 有很多种不同的类型：Soft Attention、Hard Attention、静态 Attention、动态 Attention、Self Attention 等等。上图为各种Attention的分类，下面是这些不同的 Attention 的解释。&lt;/p&gt;
&lt;p&gt;由于这篇文章《&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35739040&quot;&gt;Attention 用于 NLP 的一些小结&lt;/a&gt;》已经总结的很好的，下面就直接引用了：&lt;/p&gt;
&lt;p&gt;本节从计算区域、所用信息、结构层次和模型等方面对 Attention 的形式进行归类。&lt;/p&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="attention" scheme="https://www.miracleyoo.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>ACG相关AI项目-论文-代码-数据-资源</title>
    <link href="https://www.miracleyoo.com/2020/07/06/acg-dl/"/>
    <id>https://www.miracleyoo.com/2020/07/06/acg-dl/</id>
    <published>2020-07-06T18:19:16.000Z</published>
    <updated>2021-03-12T22:19:50.299Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文"><a class="markdownIt-Anchor" href="#论文"></a> 论文</h2><p>[<a href>Paper</a>] [<a href>Code</a>] [20XX]</p><h3 id="自动勾线-线稿"><a class="markdownIt-Anchor" href="#自动勾线-线稿"></a> 自动勾线-线稿</h3><ol><li><p>Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup [<a href="http://www.f.waseda.jp/hfs/SimoSerraSIGGRAPH2016.pdf">Paper</a>] [<a href="https://github.com/bobbens/sketch_simplification">Code</a>] [<a href="https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9">Blog</a>] [2015]</p><ul><li><p>早稻田大学15年经典论文。</p></li><li><p>粗糙手稿映射到精描线稿。</p></li><li><p>使用的是自建数据集，给定精致线稿让画师去照着描粗稿，这样避免了从线稿描到精稿时候添加和大改了很多线条。数据集没有开源…不过似乎作者给了训练好的权重。</p><span id="more"></span></li><li><p>We found that the standard approach, which we denote as direct dataset construction, of asking artists to draw a rough sketch and then produce a clean version of the sketch ended up with a lot of changes in the figure, i.e., output lines are greatly changed with respect to their input lines, or new lines are added in the output. This results in very noisy training data that does not perform well. In order to avoid this issue, we found that the best approach is the inverse dataset construction approach, that is, given a clean simplified sketch drawing, the artist is asked to make a rough version of that sketch.</p></li></ul><img data-src="image-20200705025359787.png" alt="image-20200705025359787" style="zoom:50%;"><img data-src="image-20200705025440435.png" alt="image-20200705025440435" style="zoom:50%;"></li></ol><h3 id="自动线稿上色"><a class="markdownIt-Anchor" href="#自动线稿上色"></a> 自动线稿上色</h3><ol><li><p>Scribbler: Controlling Deep Image Synthesis with Sketch and Color [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.pdf">Paper</a>] [<a href>Code</a>] [2017]</p></li><li><p>User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks [<a href="https://arxiv.org/pdf/1808.03240.pdf">Paper</a>] [<a href="https://github.com/orashi/AlacGAN">Code</a>] [2018]</p><ul><li>既支持直接从线稿转换为色稿，也支持用户交互画点生成色稿。</li><li>生成器和检测器的输入进行了创新。生成器的输入是线稿、用户色点图、还有一个线稿的特征Map。检测器的输入是两对：（真实色稿，线稿特征）与（生成色稿，线稿特征）。这样做的好处是避免了直接让判别器看到原始线稿，从而一定程度上避免了过拟合。</li><li>线稿是通过XDoG转换色稿得来。</li><li>Comment from <a href="https://www.reddit.com/r/AnimeResearch/comments/962ziu/userguided_deep_anime_line_art_colorization_with/">reddit</a>: I don’t have it locally but it should be easy to make an ‘illustration dataset’ simply by using a tag like <a href="https://danbooru.donmai.us/posts?utf8=%E2%9C%93&amp;tags=-monochrome+&amp;ms=1"><code>-monochrome</code></a> to filter out any line-art. It’s easy to make a line-art dataset because that’s also a tag: <code>lineart</code>. Finally, you don’t need to half-ass the pairs dataset with fake pairs using XDoG because you can just use <a href="https://danbooru.donmai.us/wiki_pages/21859">parent-child relationships</a> to find all sets of related images, and then extract all pairs of monochrome vs non-monochrome, which will usually give you sketch to completed image. Or there are tags just for this, like <a href="https://danbooru.donmai.us/posts?utf8=%E2%9C%93&amp;tags=colored+&amp;ms=1"><code>colored</code></a>, for human colorization of BW images (and you can again use the parent/child relationships to filter more).</li></ul><img data-src="image-20200705104853930.png" alt="image-20200705104853930" style="zoom:33%;"><img data-src="image-20200705104922846.png" alt="image-20200705104922846" style="zoom:33%;"><img data-src="image-20200705105004118.png" alt="image-20200705105004118" style="zoom:33%;"></li><li><p>Line Art Correlation Matching Network for Automatic Animation Colorization</p></li></ol><h3 id="照片转动漫"><a class="markdownIt-Anchor" href="#照片转动漫"></a> 照片转动漫</h3><ol><li>CartoonGAN: Generative Adversarial Networks for Photo Cartoonization [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf">Paper</a>] [<a href>Code</a>] [2018]<ul><li>Unpaired image dataset training.</li><li>Two loss: In generator, a semantic loss defined as an ℓ1 sparse regularization in the <strong>high-level feature maps</strong> of the VGG network. In discriminator, an edge-promoting adversarial loss for preserving <strong>clear edges</strong></li><li>Built a new set of data which is animation images with edge blurred. Traditional GAN discriminator only discriminate photo from carton(or real from false), but this discriminator has three classes: Normal Photo, Real Carton Image, and Blurred edge carton image.</li></ul></li></ol><h3 id="动漫图片embedding"><a class="markdownIt-Anchor" href="#动漫图片embedding"></a> 动漫图片Embedding</h3><ol><li>illustration2vec (i2v)。[<a href="https://www.gwern.net/docs/anime/2015-saito.pdf">Paper</a>] [<a href="https://github.com/rezoo/illustration2vec">Code</a>] [2015]<ul><li>使用了VGG网络将任意动漫图片压缩为一个长度为4096的Vector。</li><li>同样，由于训练时Y为标签，这里可以使用该网络为动漫图片打标。</li><li>训练图片来自于Danbooru和Safebooru两个网站。共计使用了1,287,596张图片和它们的metadata。</li><li>标签分为四类：<code>General tags</code>, <code>copyright tags</code>,<code>character tags</code>和<code>rating tags</code>。第一个是图片本身的特征，如“武器”，“微笑”，第二个是版权方，如“VOCALOID”；第三个是角色名字，如“hatsune miku”最后一个类别表示是18禁、擦边球还是全年龄图片。</li></ul></li></ol><h3 id="简笔画转照片"><a class="markdownIt-Anchor" href="#简笔画转照片"></a> 简笔画转照片</h3><ol><li><p>Deep Learning for Free-Hand Sketch: A Survey and A Toolbox [<a href="https://arxiv.org/pdf/2001.02600.pdf">Paper</a>] [<a href="https://github.com/PengBoXiangShang/torchsketch/">Code</a>] [2020]</p><p><img data-src="image-20200705105458250.png" alt="image-20200705105458250"></p><p><img data-src="image-20200705105522568.png" alt="image-20200705105522568"></p></li></ol><h3 id="其他gan"><a class="markdownIt-Anchor" href="#其他gan"></a> 其他GAN</h3><ol><li><p>Style GAN [<a href="https://arxiv.org/pdf/1812.04948.pdf">Paper</a>] [<a href="https://github.com/NVlabs/stylegan">Code</a>] [2019]</p><ul><li>NVIDIA出品</li><li>它可以连续控制生成图片的各种独立参数！</li><li>之前的GAN都是Input使用一个随机噪声，而NVIDIA这篇这是在许多层中间都添加一波噪声。层数越靠后，这些噪声控制的特征就越细节。</li><li>gwern训练好的二次元StyleGAN模型：<a href="https://drive.google.com/file/d/1z8N_-xZW9AU45rHYGj1_tDHkIkbnMW-R/view">Link</a></li></ul><img data-src="image-20200705105910579.png" alt="image-20200705105910579" style="zoom:33%;"><img data-src="v2-3fffd4e8d8e16d7da045de536f6d0e95_1440w.jpg" alt="img" style="zoom:33%;"><img data-src="image-20200705105951584.png" alt="image-20200705105951584" style="zoom:33%;"></li><li><p><a href="https://github.com/zhangqianhui/AdversarialNetsPapers">AdversarialNetsPapers 各种GAN的Papers</a></p></li><li><p><a href="https://github.com/tjwei/GANotebooks">GANotebooks 各种GAN的Jupyter Notebook教程</a></p></li><li><p><a href="https://github.com/jayleicn/animeGAN">animeGAN A simple PyTorch Implementation of GAN, focusing on anime face drawing.</a></p></li></ol><h2 id="数据集"><a class="markdownIt-Anchor" href="#数据集"></a> 数据集</h2><h3 id="纯动漫图片"><a class="markdownIt-Anchor" href="#纯动漫图片"></a> 纯动漫图片</h3><ol><li><p>Danbooru2019. [<a href="https://www.gwern.net/Danbooru2019">Release</a>] [<a href="https://github.com/fire-eggs/Danbooru2019">Code</a>] [2019]</p><ul><li>Original or 3x512x512</li><li>~3TB or 295GB</li><li>3.69M or 2828400 images</li><li>108M tag instances (of 392k defined tags, ~29/image).</li><li>Covering Danbooru from 24 May 2005 through 31 December 2019 (final ID: #3,734,659).</li><li>Image files &amp; a JSON export of the metadata.</li></ul></li><li><p>Anime Face Dataset [<a href="https://www.kaggle.com/splcher/animefacedataset">Link</a>][2019]</p><ul><li>数据来源：<a href="www.getchu.com">Getchu</a></li><li>包含图片数目： 63,632</li><li>只包含脸部截取图片</li><li>大小：395.95MB</li><li>每张图片分辨率：90 * 90 ~ 120 * 120</li></ul><img data-src="test.jpg" alt="anime girls" style="zoom:50%;"></li></ol><h3 id="线稿-色稿对"><a class="markdownIt-Anchor" href="#线稿-色稿对"></a> 线稿-色稿对</h3><ol><li>Danbooru Sketch Pair 128x [<a href="https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x">Link</a>] [2019]<ul><li>9.58GB</li><li>647K images. 323K sketch-color pairs</li><li>Image size: 3x128x128</li><li>有的是插画，有的是漫画。</li><li>博客：Sketch to Color Anime: An application of Conditional GAN. [<a href="https://medium.com/@raviranjankr165/sketch-to-color-anime-an-application-of-conditional-gan-e40f59c66281">Link</a>] [<a href="https://github.com/ravi-1654003/Sketch2Color-conditional-GAN">Code</a>] [2020]</li></ul></li></ol><h3 id="推荐与评价"><a class="markdownIt-Anchor" href="#推荐与评价"></a> 推荐与评价</h3><ol><li><p>Anime Recommendations Database [<a href="https://www.kaggle.com/CooperUnion/anime-recommendations-database">Link</a>] [2016]</p><ul><li>数据来源于：<a href="https://myanimelist.net/">Link</a></li><li>大小：107.14MB</li><li>This data set contains information on user preference data from 73,516 users on 12,294 anime.</li></ul><img data-src="image-20200705024244220.png" alt="image-20200705024244220" style="zoom: 33%;"></li></ol><h2 id="博客"><a class="markdownIt-Anchor" href="#博客"></a> 博客</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/24767059">GAN学习指南：从原理入门到制作生成Demo</a></li><li><a href="https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09">GAN — What is Generative Adversary Networks GAN?</a></li><li><a href="https://www.leiphone.com/news/201709/i9qlcvWrpitOacjf.html">可能是近期最好玩的深度学习模型：CycleGAN的原理与实验详解</a></li><li><a href="https://makegirlsmoe.github.io/main/2017/08/14/news-english.html">输入各种参数生成动漫人物头像官方博客</a></li><li><a href="https://www.jiqizhixin.com/articles/2017-08-20-4">宅男的福音：用GAN自动生成二次元萌妹子</a></li><li><a href="https://qiita.com/rezoolab/items/5cc96b6d31153e0c86bc">Chainerを使ってコンピュータにイラストを描かせる</a></li><li><a href="https://www.jqr.com/article/000215">旋转吧！换装少女：一种可生成高分辨率全身动画的GAN</a></li><li><a href="https://www.jianshu.com/p/f31d9fc1d677">不要怂，就是GAN</a></li><li><a href="https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900">GAN — Some cool applications of GANs</a></li><li><a href="https://zhuanlan.zhihu.com/p/30532830">眼见已不为实，迄今最真实的GAN：Progressive Growing of GANs</a></li><li><a href="https://zhuanlan.zhihu.com/p/33752313">通俗理解生成对抗网络GAN</a></li><li><a href="https://zhuanlan.zhihu.com/p/27012520">从头开始GAN</a></li><li><a href="https://junyanz.github.io/CycleGAN/">Cycle GAN 作者官网</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-04-17-5">如何从零开始构建深度学习项目？这里有一份详细的教程</a></li><li><a href="https://zhuanlan.zhihu.com/p/27145954">带你理解CycleGAN，并用TensorFlow轻松实现</a></li></ol><h2 id="其他资源"><a class="markdownIt-Anchor" href="#其他资源"></a> 其他资源</h2><h3 id="文字转语音"><a class="markdownIt-Anchor" href="#文字转语音"></a> 文字转语音</h3><ol><li><a href="http://sinsy.jp/">Sinsy</a>: アップロードされた楽譜(MusicXML)に基づいて任意の歌声を生成するHMM/DNN歌声合成システム，Sinsy（しぃんしぃ）です．<ul><li>支持日语、中文、英语。</li><li>输入特定格式的乐谱，输出相当不错的唱词音频文件。</li><li><a href="https://pypi.org/project/sinsy-cli/">sinsy-cli</a>: 使用命令行调用Sinsy进行合成。安装：<code>pip install sinsy-cli</code></li><li>介绍博客：<a href="http://blog.pcedev.com/2016/02/18/hands-sinsy-solution-song-vocal-synthesis-using-open-source-software/">Hands on Sinsy, a free software solution for song vocal synthesis</a></li></ul></li><li><a href="https://www.animenewsnetwork.com/interest/2013-10-22/vocaloducer-automatically-creates-songs-from-lyrics">Vocaloducer</a>。</li></ol><h3 id="动漫人脸检测切割"><a class="markdownIt-Anchor" href="#动漫人脸检测切割"></a> 动漫人脸检测切割</h3><ol><li><a href="https://github.com/nagadomi/lbpcascade_animeface">自动化动漫人物脸部切割保存</a><br><img data-src="43184241-ed3f1af8-9022-11e8-8800-468b002c73d9.png" alt="result"></li></ol><h3 id="头像生成"><a class="markdownIt-Anchor" href="#头像生成"></a> 头像生成</h3><ol><li><a href="https://make.girls.moe/">输入各种参数生成动漫人物头像</a></li><li><a href="https://github.com/Aixile/chainer-cyclegan">Chainer-CycleGAN 动漫人物头发转银色</a></li></ol><h3 id="图像超分辨率"><a class="markdownIt-Anchor" href="#图像超分辨率"></a> 图像超分辨率</h3><ol><li><a href="http://waifu2x.udp.jp">Waifu2x 动漫图片无损放大</a></li></ol><h3 id="自动线稿上色-2"><a class="markdownIt-Anchor" href="#自动线稿上色-2"></a> 自动线稿上色</h3><ol><li><p><a href="https://github.com/pfnet/PaintsChainer">PaintsChainer</a>: Paints Chainer is a line drawing colorizer using chainer. Using CNN, you can colorize your sketch semi-automatically .</p><ul><li>作者提供了直接搭建网站server 的代码。</li><li>这里是搭建好的<a href="http://paintschainer.preferred.tech/">站点</a>。</li><li>该网站也提供草图或照片提取线稿功能。</li></ul><p><img data-src="sample.png" alt="image"></p></li></ol><h3 id="照片画风迁移"><a class="markdownIt-Anchor" href="#照片画风迁移"></a> 照片画风迁移</h3><ol><li><p><a href="https://deepart.io/">Repaint your picture in the style of your favorite artist</a></p><p><img data-src="ACG%E7%9B%B8%E5%85%B3AI%E9%A1%B9%E7%9B%AE-%E8%AE%BA%E6%96%87-%E4%BB%A3%E7%A0%81-%E6%95%B0%E6%8D%AE-%E8%B5%84%E6%BA%90/image-20200705105719731.png" alt="image-20200705105719731"></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;论文&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#论文&quot;&gt;&lt;/a&gt; 论文&lt;/h2&gt;
&lt;p&gt;[&lt;a href&gt;Paper&lt;/a&gt;] [&lt;a href&gt;Code&lt;/a&gt;] [20XX]&lt;/p&gt;
&lt;h3 id=&quot;自动勾线-线稿&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#自动勾线-线稿&quot;&gt;&lt;/a&gt; 自动勾线-线稿&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup [&lt;a href=&quot;http://www.f.waseda.jp/hfs/SimoSerraSIGGRAPH2016.pdf&quot;&gt;Paper&lt;/a&gt;] [&lt;a href=&quot;https://github.com/bobbens/sketch_simplification&quot;&gt;Code&lt;/a&gt;] [&lt;a href=&quot;https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9&quot;&gt;Blog&lt;/a&gt;] [2015]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;早稻田大学15年经典论文。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;粗糙手稿映射到精描线稿。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用的是自建数据集，给定精致线稿让画师去照着描粗稿，这样避免了从线稿描到精稿时候添加和大改了很多线条。数据集没有开源…不过似乎作者给了训练好的权重。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="acg" scheme="https://www.miracleyoo.com/tags/acg/"/>
    
    <category term="anime" scheme="https://www.miracleyoo.com/tags/anime/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
  </entry>
  
</feed>
