---
title: How are optimizer.step() and loss.backward() related?
tags:
  - machine-learning
  - pytorch
date: 2018-04-11 00:45:42
---


ä»Šå¤©å’ŒåŒå­¦è®¨è®ºçš„æ—¶å€™å‘ç°è¿™ä¸ªåœ°æ–¹çŸ¥è¯†å­˜åœ¨æ¼æ´ï¼Œèµ¶ç´§è¡¥äº†ä¸€æ³¢ã€‚é—®é¢˜å°±æ˜¯optimizer.step() å’Œ loss.backward()è¿™ä¸¤ä¸ªæ€»æ˜¯å‡ºç°åœ¨ä¸€èµ·çš„ä¸¤ä¸ªpytorchå‡½æ•°å„è‡ªæ‰§è¡Œçš„åŠŸèƒ½ã€‚

## ç®€å•çš„è¯´ï¼Œ

* loss.backward()æ ¹æ®è¿™ä¸€è½®çš„lossè®¡ç®—å‡ºäº†ç½‘ç»œä¸­æ‰€æœ‰éœ€è¦è®¡ç®—çš„å¯¼æ•°
* optimizer.step()æ ¹æ®ä½ é€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¸Šé¢ğŸ‘†loss.backward()è®¡ç®—å‡ºçš„å„ä¸ªå¯¼æ•°æ›´æ–°äº†ç½‘ç»œä¸­çš„å„ä¸ªæƒå€¼

<!-- more -->

## ä»¥ä¸‹ä¸ºè®ºå›åŸæ–‡ï¼š

`loss.backward()` computes `dloss/dx` for every parameter `x` which has `requires_grad=True`. These are accumulated into `x.grad` for every parameter `x`. In pseudo-code:

```
x.grad += dloss/dx
```

`optimizer.step` updates the value of `x` using the gradient `x.grad`. For example, the SGD optimizer performs:

```
x += -lr * x.grad
```

`optimizer.zero_grad()` clears `x.grad` for every parameter `x` in the optimizer. Itâ€™s important to call this before `loss.backward()`, otherwise youâ€™ll accumulate the gradients from multiple passes.

If you have multiple losses (loss1, loss2) you can sum them and then call backwards once:

```
loss3 = loss1 + loss2
loss3.backward()
```