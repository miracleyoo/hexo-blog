<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.miracleyoo.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="写在前面 Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch Lighting 完全攻略">
<meta property="og:url" content="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/index.html">
<meta property="og:site_name" content="Miracleyoo">
<meta property="og:description" content="写在前面 Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/plres.png">
<meta property="article:published_time" content="2021-03-12T04:09:57.000Z">
<meta property="article:modified_time" content="2021-03-12T22:05:55.192Z">
<meta property="article:author" content="Miracle Yoo">
<meta property="article:tag" content="deep-learning">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="pytorch-lightning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/plres.png">


<link rel="canonical" href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.miracleyoo.com/2021/03/11/pytorch-lightning/","path":"2021/03/11/pytorch-lightning/","title":"Pytorch Lighting 完全攻略"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Pytorch Lighting 完全攻略 | Miracleyoo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Miracleyoo" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Miracleyoo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text"> 写在前面</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#crucial"><span class="nav-number">2.</span> <span class="nav-text"> Crucial</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text"> 推荐使用方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lightning-module"><span class="nav-number">4.</span> <span class="nav-text"> Lightning Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">4.1.</span> <span class="nav-text"> 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%84%E4%BB%B6%E4%B8%8E%E5%87%BD%E6%95%B0"><span class="nav-number">4.2.</span> <span class="nav-text"> 组件与函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A6%81%E7%82%B9"><span class="nav-number">4.3.</span> <span class="nav-text"> 要点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E6%9D%BF"><span class="nav-number">4.4.</span> <span class="nav-text"> 模板</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trainer"><span class="nav-number">5.</span> <span class="nav-text"> Trainer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="nav-number">5.1.</span> <span class="nav-text"> 基础使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81%E4%B8%8Ehooks"><span class="nav-number">5.2.</span> <span class="nav-text"> 伪代码与hooks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0"><span class="nav-number">5.3.</span> <span class="nav-text"> 推荐参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fit%E5%87%BD%E6%95%B0"><span class="nav-number">5.4.</span> <span class="nav-text"> .fit()函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E8%A6%81%E7%82%B9"><span class="nav-number">5.5.</span> <span class="nav-text"> 其他要点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%A0%B7%E4%BE%8B"><span class="nav-number">5.6.</span> <span class="nav-text"> 使用样例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0"><span class="nav-number">5.7.</span> <span class="nav-text"> 所有参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#log%E5%92%8Creturn-loss%E5%88%B0%E5%BA%95%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88"><span class="nav-number">5.8.</span> <span class="nav-text"> Log和return loss到底在做什么</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#training-epoch-level-metrics"><span class="nav-number">5.8.1.</span> <span class="nav-text"> Training epoch-level metrics</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#train-epoch-level-operations"><span class="nav-number">5.8.2.</span> <span class="nav-text"> Train epoch-level operations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#datamodule"><span class="nav-number">6.</span> <span class="nav-text"> DataModule</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">6.1.</span> <span class="nav-text"> 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">6.2.</span> <span class="nav-text"> 示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A6%81%E7%82%B9-2"><span class="nav-number">6.3.</span> <span class="nav-text"> 要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saving-and-loading"><span class="nav-number">7.</span> <span class="nav-text"> Saving and Loading</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#saving"><span class="nav-number">7.1.</span> <span class="nav-text"> Saving</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loading"><span class="nav-number">7.2.</span> <span class="nav-text"> Loading</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#callbacks"><span class="nav-number">8.</span> <span class="nav-text"> Callbacks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#callbacks%E6%8E%A8%E8%8D%90"><span class="nav-number">8.1.</span> <span class="nav-text"> Callbacks推荐</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logging"><span class="nav-number">9.</span> <span class="nav-text"> Logging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8tensorboard%E5%92%8Ccsv-logger"><span class="nav-number">9.1.</span> <span class="nav-text"> 同时使用TensorBoard和CSV Logger</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transfer-learning"><span class="nav-number">10.</span> <span class="nav-text"> Transfer Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8Edevice%E6%93%8D%E4%BD%9C"><span class="nav-number">11.</span> <span class="nav-text"> 关于device操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8Elimit_train_batches%E9%80%89%E9%A1%B9"><span class="nav-number">12.</span> <span class="nav-text"> 关于limit_train_batches选项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#points"><span class="nav-number">13.</span> <span class="nav-text"> Points</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Miracle Yoo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">146</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/miracleyoo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;miracleyoo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongyang.zhang.hust@gmail.com" title="E-Mail → mailto:zhongyang.zhang.hust@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Ogisomiracle" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Ogisomiracle" rel="noopener me" target="_blank"><i class="twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/mirakuruyoo" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;mirakuruyoo" rel="noopener me" target="_blank"><i class="facebook fa-fw"></i>FB Page</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Miracle Yoo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Miracleyoo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Pytorch Lighting 完全攻略 | Miracleyoo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch Lighting 完全攻略<a href="https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/_posts/pytorch-lightning.md" class="post-edit-link" title="Edit this post" rel="noopener" target="_blank"><i class="fa fa-pen-nib"></i></a>
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-03-11 20:09:57" itemprop="dateCreated datePublished" datetime="2021-03-11T20:09:57-08:00">2021-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-03-12 14:05:55" itemprop="dateModified" datetime="2021-03-12T14:05:55-08:00">2021-03-12</time>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>6.7k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>24 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h2>
<p>Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？</p>
<span id="more"></span>
<p>于是我第二次发现了Pytorch-Lightning。</p>
<p>真香。</p>
<p>但是问题还是来了。这个框架并没有因为香而变得更加易学。官网的教程很丰富，可以看出来开发者们在努力做了。但是很多相连的知识点都被分布在了不同的版块里，还有一些核心的理解要点并没有被强调出来，而是小字带过，这让我想做一个普惠的教程，包含所有我在学习过程中认为重要的概念，好用的参数，一些注意点、坑点，大量的示例代码段和一些核心问题的集中讲解。</p>
<p>最后，第三部分提供了一个我总结出来的易用于大型项目、容易迁移、易于复用的模板，有兴趣的可以去<a target="_blank" rel="noopener" href="https://github.com/miracleyoo/pytorch-lightning-template">GitHub</a>试用。</p>
<h2 id="crucial"><a class="markdownIt-Anchor" href="#crucial"></a> Crucial</h2>
<ul>
<li>
<p>Pytorch-Lighting 的一大特点是把模型和系统分开来看。模型是像Resnet18， RNN之类的纯模型， 而系统定义了一组模型如何相互交互，如GAN（生成器网络与判别器网络）、Seq2Seq（Encoder与Decoder网络）和Bert。同时，有时候问题只涉及一个模型，那么这个系统则可以是一个通用的系统，用于描述模型如何使用，并可以被复用到很多其他项目。</p>
</li>
<li>
<p>Pytorch-Lighting 的核心设计思想是“自给自足”。每个网络也同时包含了如何训练、如何测试、优化器定义等内容。</p>
</li>
</ul>
<p><img data-src="plres.png" alt="img"></p>
<h2 id="推荐使用方法"><a class="markdownIt-Anchor" href="#推荐使用方法"></a> 推荐使用方法</h2>
<p>这一部分放在最前面，因为全文内容太长，如果放后面容易忽略掉这部分精华。</p>
<p>Pytorch-Lightning 是一个很好的库，或者说是pytorch的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。缺点也很明显，这个包需要学习和理解的内容还是挺多的，或者换句话说，很重。如果直接按照官方的模板写代码，小型project还好，如果是大型项目，有复数个需要调试验证的模型和数据集，那就不太好办，甚至更加麻烦了。经过几天的摸索和调试，我总结出了下面这样一套好用的模板，也可以说是对Pytorch-Lightning的进一步抽象。</p>
<p>欢迎大家尝试这一套代码风格，如果用习惯的话还是相当方便复用的，也不容易半道退坑。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">	|-data</span><br><span class="line">		|-__init__.py</span><br><span class="line">		|-data_interface.py</span><br><span class="line">		|-xxxdataset1.py</span><br><span class="line">		|-xxxdataset2.py</span><br><span class="line">		|-...</span><br><span class="line">	|-model</span><br><span class="line">		|-__init__.py</span><br><span class="line">		|-model_interface.py</span><br><span class="line">		|-xxxmodel1.py</span><br><span class="line">		|-xxxmodel2.py</span><br><span class="line">		|-...</span><br><span class="line">	|-main.py	</span><br></pre></td></tr></table></figure>
<p>如果对每个模型直接上plmodule，对于已有项目、别人的代码等的转换将相当耗时。另外，这样的话，你需要给每个模型都加上一些相似的代码，如<code>training_step</code>，<code>validation_step</code>。显然，这并不是我们想要的，如果真的这样做，不但不易于维护，反而可能会更加杂乱。同理，如果把每个数据集类都直接转换成pl的DataModule，也会面临相似的问题。基于这样的考量，我建议使用上述架构：</p>
<ul>
<li>
<p>主目录下只放一个<code>main.py</code>文件。</p>
</li>
<li>
<p><code>data</code>和<code>modle</code>两个文件夹中放入<code>__init__.py</code>文件，做成包。这样方便导入。两个<code>init</code>文件分别是：</p>
<ul>
<li><code>from .data_interface import DInterface</code></li>
<li><code>from .model_interface import MInterface</code></li>
</ul>
</li>
<li>
<p>在<code>data_interface </code>中建立一个<code>class DInterface(pl.LightningDataModule):</code>用作所有数据集文件的接口。<code>__init__()</code>函数中import相应Dataset类，<code>setup()</code>进行实例化，并老老实实加入所需要的的<code>train_dataloader</code>, <code>val_dataloader</code>, <code>test_dataloader</code>函数。这些函数往往都是相似的，可以用几个输入args控制不同的部分。</p>
</li>
<li>
<p>同理，在<code>model_interface </code>中建立<code>class MInterface(pl.LightningModule):</code>类，作为模型的中间接口。<code>__init__()</code>函数中import相应模型类，然后老老实实加入<code>configure_optimizers</code>, <code>training_step</code>, <code>validation_step</code>等函数，用一个接口类控制所有模型。不同部分使用输入参数控制。</p>
</li>
<li>
<p><code>main.py</code>函数只负责：</p>
<ul>
<li>定义parser，添加parse项。</li>
<li>选好需要的<code>callback</code>函数们。</li>
<li>实例化<code>MInterface</code>, <code>DInterface</code>, <code>Trainer</code>。</li>
</ul>
<p>完事。</p>
</li>
</ul>
<h2 id="lightning-module"><a class="markdownIt-Anchor" href="#lightning-module"></a> Lightning Module</h2>
<h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">主页面</a></p>
<ul>
<li>
<p>三个核心组件：</p>
<ul>
<li>模型</li>
<li>优化器</li>
<li>Train/Val/Test步骤</li>
</ul>
</li>
<li>
<p>数据流伪代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">    out = training_step(batch)</span><br><span class="line">    outs.append(out)</span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure>
<p>等价Lightning代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    prediction = ...</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        <span class="comment"># do something with these</span></span><br></pre></td></tr></table></figure>
<p>我们需要做的，就是像填空一样，填这些函数。</p>
</li>
</ul>
<h3 id="组件与函数"><a class="markdownIt-Anchor" href="#组件与函数"></a> 组件与函数</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api">API页面</a></p>
<ul>
<li>
<p>一个Pytorch-Lighting 模型必须含有的部件是：</p>
<ul>
<li>
<p><code>init</code>: 初始化，包括模型和系统的定义。</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.training_step"><code>training_step(self, batch, batch_idx)</code></a>: 即每个batch的处理函数。</p>
<blockquote>
<p>参数：</p>
<ul>
<li><strong>batch</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> | (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …) | [<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …]) – The output of your <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. A tensor, tuple or list.</li>
<li><strong>batch_idx</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Integer displaying index of this batch</li>
<li><strong>optimizer_idx</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – When using multiple optimizers, this argument will also be present.</li>
<li><strong>hiddens</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>) – Passed in if <a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.params.truncated_bptt_steps"><code>truncated_bptt_steps</code></a> &gt; 0.</li>
</ul>
<p>返回值：Any of.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li>
<li><code>None</code> - Training will skip to the next batch</li>
</ul>
</blockquote>
<p>返回值无论如何也需要有一个loss量。如果是字典，要有这个key。没loss这个batch就被跳过了。例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y, z = batch</span><br><span class="line">    out = self.encoder(x)</span><br><span class="line">    loss = self.loss(out, x)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple optimizers (e.g.: GANs)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, optimizer_idx</span>):</span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># do training_step with encoder</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># do training_step with decoder</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Truncated back-propagation through time</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, hiddens</span>):</span><br><span class="line">    <span class="comment"># hiddens are the hidden states from the previous truncated backprop step</span></span><br><span class="line">    ...</span><br><span class="line">    out, hiddens = self.lstm(data, hiddens)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;hiddens&#x27;</span>: hiddens&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization"><code>configure_optimizers</code></a>: 优化器定义，返回一个优化器，或数个优化器，或两个List（优化器，Scheduler）。如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most cases</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    opt = Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">return</span> opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> generator_opt, disriminator_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [generator_opt, disriminator_opt], [discriminator_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sched = &#123;<span class="string">&#x27;scheduler&#x27;</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">                 <span class="string">&#x27;interval&#x27;</span>: <span class="string">&#x27;step&#x27;</span>&#125;  <span class="comment"># called after each training step</span></span><br><span class="line">    dis_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sched, dis_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: dis_opt, <span class="string">&#x27;frequency&#x27;</span>: n_critic&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: gen_opt, <span class="string">&#x27;frequency&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>可以指定的部件有：</p>
<ul>
<li><code>forward</code>: 和正常的<code>nn.Module</code>一样，用于inference。内部调用时：<code>y=self(batch)</code></li>
<li><code>training_step_end</code>: 只在使用多个node进行训练且结果涉及如softmax之类需要全部输出联合运算的步骤时使用该函数。同理，<code>validation_step_end</code>/<code>test_step_end</code>。</li>
<li><code>training_epoch_end</code>:
<ul>
<li>在一个训练epoch结尾处被调用。</li>
<li>输入参数：一个List，List的内容是前面<code>training_step()</code>所返回的每次的内容。</li>
<li>返回：None</li>
</ul>
</li>
<li><code>validation_step(self, batch, batch_idx)</code>/<code>test_step(self, batch, batch_idx)</code>:
<ul>
<li>没有返回值限制，不一定非要输出一个<code>val_loss</code>。</li>
</ul>
</li>
<li><code>validation_epoch_end</code>/<code>test_epoch_end</code>:</li>
</ul>
</li>
<li>
<p>工具函数有：</p>
<ul>
<li>
<p><code>freeze</code>：冻结所有权重以供预测时候使用。仅当已经训练完成且后面只测试时使用。</p>
</li>
<li>
<p><code>print</code>：尽管自带的<code>print</code>函数也可以使用，但如果程序运行在分布式系统时，会打印多次。而使用<code>self.print()</code>则只会打印一次。</p>
</li>
<li>
<p><code>log</code>：像是TensorBoard等log记录器，对于每个log的标量，都会有一个相对应的横坐标，它可能是batch number或epoch number。而<code>on_step</code>就表示把这个log出去的量的横坐标表示为当前batch，而<code>on_epoch</code>则表示将log的量在整个epoch上进行累积后log，横坐标为当前epoch。</p>
<table>
<thead>
<tr>
<th>LightningMoule Hook</th>
<th>on_step</th>
<th>on_epoch</th>
<th>prog_bar</th>
<th>logger</th>
</tr>
</thead>
<tbody>
<tr>
<td>training_step</td>
<td>T</td>
<td>F</td>
<td>F</td>
<td>T</td>
</tr>
<tr>
<td>training_step_end</td>
<td>T</td>
<td>F</td>
<td>F</td>
<td>T</td>
</tr>
<tr>
<td>training_epoch_end</td>
<td>F</td>
<td>T</td>
<td>F</td>
<td>T</td>
</tr>
<tr>
<td>validation_step*</td>
<td>F</td>
<td>T</td>
<td>F</td>
<td>T</td>
</tr>
<tr>
<td>validation_step_end*</td>
<td>F</td>
<td>T</td>
<td>F</td>
<td>T</td>
</tr>
<tr>
<td>validation_epoch_end*</td>
<td>F</td>
<td>T</td>
<td>F</td>
<td>T</td>
</tr>
</tbody>
</table>
<p><code>*</code> also applies to the test loop</p>
<blockquote>
<p>参数</p>
<ul>
<li><strong>name</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – key name</li>
<li><strong>value</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>) – value name</li>
<li><strong>prog_bar</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the progress bar</li>
<li><strong>logger</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the logger</li>
<li><strong>on_step</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs at this step. None auto-logs at the training_step but not validation/test_step</li>
<li><strong>on_epoch</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</li>
<li><strong>reduce_fx</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – reduction function over step values for end of epoch. Torch.mean by default</li>
<li><strong>tbptt_reduce_fx</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – function to reduce on truncated back prop</li>
<li><strong>tbptt_pad_token</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – token to use for padding</li>
<li><strong>enable_graph</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, will not auto detach the graph</li>
<li><strong>sync_dist</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, reduces the metric across GPUs/TPUs</li>
<li><strong>sync_dist_op</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>, <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>]) – the op to sync across GPUs/TPUs</li>
<li><strong>sync_dist_group</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>]) – the ddp group</li>
</ul>
</blockquote>
</li>
<li>
<p><code>log_dict</code>：和<code>log</code>函数唯一的区别就是，<code>name</code>和<code>value</code>变量由一个字典替换。表示同时log多个值。如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;acc&#x27;</span>: acc, ..., <span class="string">&#x27;metric_n&#x27;</span>: metric_n&#125;</span><br><span class="line">self.log_dict(values)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>save_hyperparameters</code>：储存<code>init</code>中输入的所有超参。后续访问可以由<code>self.hparams.argX</code>方式进行。同时，超参表也会被存到文件中。</p>
</li>
</ul>
</li>
<li>
<p>函数内建变量：</p>
<ul>
<li><code>device</code>：可以使用<code>self.device</code>来构建设备无关型tensor。如：<code>z = torch.rand(2, 3, device=self.device)</code>。</li>
<li><code>hparams</code>：含有所有前面存下来的输入超参。</li>
<li><code>precision</code>：精确度。常见32和16。</li>
</ul>
</li>
</ul>
<h3 id="要点"><a class="markdownIt-Anchor" href="#要点"></a> 要点</h3>
<ul>
<li>如果准备使用DataParallel，在写<code>training_step</code>的时候需要调用forward函数，<code>z=self(x)</code></li>
</ul>
<h3 id="模板"><a class="markdownIt-Anchor" href="#模板"></a> 模板</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">any_extra_hook</span>(<span class="params">...</span>)</span><br></pre></td></tr></table></figure>
<h2 id="trainer"><a class="markdownIt-Anchor" href="#trainer"></a> Trainer</h2>
<h3 id="基础使用"><a class="markdownIt-Anchor" href="#基础使用"></a> 基础使用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightningModule()</span><br><span class="line"></span><br><span class="line">trainer = Trainer()</span><br><span class="line">trainer.fit(model, train_dataloader, val_dataloader)</span><br></pre></td></tr></table></figure>
<p>如果连<code>validation_step</code>都没有，那<code>val_dataloader</code>也就算了。</p>
<h3 id="伪代码与hooks"><a class="markdownIt-Anchor" href="#伪代码与hooks"></a> 伪代码与hooks</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks">Hooks页面</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">...</span>):</span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> global_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># prepare data is called on GLOBAL_ZERO only</span></span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> gpu/tpu <span class="keyword">in</span> gpu/tpus:</span><br><span class="line">        train_on_device(model.copy())</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_on_device</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># setup is called PER DEVICE</span></span><br><span class="line">    setup()</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_pretrain_routine_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_loop()</span><br><span class="line"></span><br><span class="line">    teardown()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>():</span><br><span class="line">    on_train_epoch_start()</span><br><span class="line">    train_outs = []</span><br><span class="line">    <span class="keyword">for</span> train_batch <span class="keyword">in</span> train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----- train_step methods -------</span></span><br><span class="line">        out = training_step(batch)</span><br><span class="line">        train_outs.append(out)</span><br><span class="line"></span><br><span class="line">        loss = out.loss</span><br><span class="line"></span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line">        optimizer_step()</span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end training epoch</span></span><br><span class="line">    logs = training_epoch_end(outs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_loop</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line">    val_outs = []</span><br><span class="line">    <span class="keyword">for</span> val_batch <span class="keyword">in</span> val_dataloader():</span><br><span class="line">        on_validation_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------- val step methods -------</span></span><br><span class="line">        out = validation_step(val_batch)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up for train</span></span><br><span class="line">    model.train()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="推荐参数"><a class="markdownIt-Anchor" href="#推荐参数"></a> 推荐参数</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags">参数介绍（附视频）</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api">类定义与默认参数</a></p>
<ul>
<li>
<p><code>default_root_dir</code>：默认存储地址。所有的实验变量和权重全部会被存到这个文件夹里面。推荐是，每个模型有一个独立的文件夹。每次重新训练会产生一个新的<code>version_x</code>子文件夹。</p>
</li>
<li>
<p><code>max_epochs</code>：最大训练周期数。<code>trainer = Trainer(max_epochs=1000)</code></p>
</li>
<li>
<p><code>min_epochs</code>：至少训练周期数。当有Early Stop时使用。</p>
</li>
<li>
<p><code>auto_scale_batch_size</code>：在进行任何训练前自动选择合适的batch size。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (no scaling of batch size)</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run batch size scaling, result overrides hparams.batch_size</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="string">&#x27;binsearch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the batch size</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>auto_select_gpus</code>：自动选择合适的GPU。尤其是在有GPU处于独占模式时候，非常有用。</p>
</li>
<li>
<p><code>auto_lr_find</code>：自动找到合适的初始学习率。使用了该<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.01186">论文</a>的技术。当且仅当执行<code>trainer.tune(model)</code>代码时工作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run learning rate finder, results override hparams.learning_rate</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run learning rate finder, results override hparams.my_lr_arg</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="string">&#x27;my_lr_arg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the lr</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>precision</code>：精确度。正常是32，使用16可以减小内存消耗，增大batch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 16-bit precision</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">16</span>, gpus=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>val_check_interval</code>：进行Validation测试的周期。正常为1，训练1个epoch测试4次是0.25，每1000 batch测试一次是1000。</p>
<blockquote>
<ul>
<li>use (float) to check within a training epoch：此时这个值为一个epoch的百分比。每百分之多少测试一次。</li>
<li>use (int) to check every n steps (batches)：每多少个batch测试一次。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set 4 times during a training epoch</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set every 1000 training batches</span></span><br><span class="line"><span class="comment"># use this when using iterableDataset and your dataset has no length</span></span><br><span class="line"><span class="comment"># (ie: production cases with streaming data)</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#gpus"><code>gpus</code></a>：控制使用的GPU数。当设定为None时，使用cpu。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (ie: train on CPU)</span></span><br><span class="line">trainer = Trainer(gpus=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># int: train on 2 gpus</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list: train on GPUs 1, 4 (by bus ordering)</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;1, 4&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1: train on all gpus</span></span><br><span class="line">trainer = Trainer(gpus=-<span class="number">1</span>)</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;-1&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine with num_nodes to train on multiple GPUs across nodes</span></span><br><span class="line"><span class="comment"># uses 8 gpus in total</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>, num_nodes=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train only on GPUs 1 and 4 across nodes</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>], num_nodes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#limit-train-batches"><code>limit_train_batches</code></a>：使用训练数据的百分比。如果数据过多，或正在调试，可以使用这个。值的范围为0~1。同样，有<code>limit_test_batches</code>，<code>limit_val_batches</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 25% of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 10 batches of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run"><code>fast_dev_run</code></a>：bool量。如果设定为true，会只执行一个batch的train, val 和 test，然后结束。仅用于debug。</p>
<blockquote>
<p>Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like <code>LearningRateLogger</code> and runs for only 1 epoch</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 1 train, val, test batch and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 7 train, val, test batches and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="fit函数"><a class="markdownIt-Anchor" href="#fit函数"></a> .fit()函数</h3>
<p><code>Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None)</code>：输入第一个量一定是model，然后可以跟一个LigntningDataModule或一个普通的Train DataLoader。如果定义了Val step，也要有Val DataLoader。</p>
<blockquote>
<p>参数</p>
<ul>
<li><strong>datamodule</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule"><code>LightningDataModule</code></a>]) – A instance of <code>LightningDataModule</code>.</li>
<li><strong>model</strong> (<a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule"><code>LightningModule</code></a>) – Model to fit.</li>
<li><strong>train_dataloader</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>]) – A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.</li>
<li><strong>val_dataloaders</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>, <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/typing.html#typing.List"><code>List</code></a>[<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>], <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/constants.html#None"><code>None</code></a>]) – Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped</li>
</ul>
</blockquote>
<h3 id="其他要点"><a class="markdownIt-Anchor" href="#其他要点"></a> 其他要点</h3>
<ul>
<li><code>.test()</code>若非直接调用，不会运行。<code>trainer.test()</code></li>
<li><code>.test()</code>会自动load最优模型。</li>
<li><code>model.eval()</code> and <code>torch.no_grad()</code> 在进行测试时会被自动调用。</li>
<li>默认情况下，<code>Trainer()</code>运行于CPU上。</li>
</ul>
<h3 id="使用样例"><a class="markdownIt-Anchor" href="#使用样例"></a> 使用样例</h3>
<ol>
<li>手动添加命令行参数：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">hparams</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer(gpus=hparams.gpus)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gpus&#x27;</span>, default=<span class="literal">None</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>自动添加所有<code>Trainer</code>会用到的命令行参数：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>混合式，既使用<code>Trainer</code>相关参数，又使用一些自定义参数，如各种模型超参：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> LightningModule, Trainer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">32</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hidden_dim&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">128</span>)</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">	</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<h3 id="所有参数"><a class="markdownIt-Anchor" href="#所有参数"></a> 所有参数</h3>
<blockquote>
<p><code>Trainer.``__init__</code>(<em>logger=True</em>, <em>checkpoint_callback=True</em>, <em>callbacks=None</em>, <em>default_root_dir=None</em>, <em>gradient_clip_val=0</em>, <em>process_position=0</em>, <em>num_nodes=1</em>, <em>num_processes=1</em>, <em>gpus=None</em>, <em>auto_select_gpus=False</em>, <em>tpu_cores=None</em>, <em>log_gpu_memory=None</em>, <em>progress_bar_refresh_rate=None</em>, <em>overfit_batches=0.0</em>, <em>track_grad_norm=- 1</em>, <em>check_val_every_n_epoch=1</em>, <em>fast_dev_run=False</em>, <em>accumulate_grad_batches=1</em>, <em>max_epochs=None</em>, <em>min_epochs=None</em>, <em>max_steps=None</em>, <em>min_steps=None</em>, <em>limit_train_batches=1.0</em>, <em>limit_val_batches=1.0</em>, <em>limit_test_batches=1.0</em>, <em>limit_predict_batches=1.0</em>, <em>val_check_interval=1.0</em>, <em>flush_logs_every_n_steps=100</em>, <em>log_every_n_steps=50</em>, <em>accelerator=None</em>, <em>sync_batchnorm=False</em>, <em>precision=32</em>, <em>weights_summary=‘top’</em>, <em>weights_save_path=None</em>, <em>num_sanity_val_steps=2</em>, <em>truncated_bptt_steps=None</em>, <em>resume_from_checkpoint=None</em>, <em>profiler=None</em>, <em>benchmark=False</em>, <em>deterministic=False</em>, <em>reload_dataloaders_every_epoch=False</em>, <em>auto_lr_find=False</em>, <em>replace_sampler_ddp=True</em>, <em>terminate_on_nan=False</em>, <em>auto_scale_batch_size=False</em>, <em>prepare_data_per_node=True</em>, <em>plugins=None</em>, <em>amp_backend=‘native’</em>, <em>amp_level=‘O2’</em>, <em>distributed_backend=None</em>, <em>move_metrics_to_cpu=False</em>, <em>multiple_trainloader_mode=‘max_size_cycle’</em>, <em>stochastic_weight_avg=False</em>)</p>
</blockquote>
<h3 id="log和return-loss到底在做什么"><a class="markdownIt-Anchor" href="#log和return-loss到底在做什么"></a> Log和return loss到底在做什么</h3>
<p>To add a training loop use the training_step method</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitClassifier</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = model</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">         x, y = batch</span><br><span class="line">         y_hat = self.model(x)</span><br><span class="line">         loss = F.cross_entropy(y_hat, y)</span><br><span class="line">         <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<ul>
<li>无论是<code>training_step</code>，还是<code>validation_step</code>，<code>test_step</code>返回值都是<code>loss</code>。返回的loss会被用一个list收集起来。</li>
</ul>
<p>Under the hood, Lightning does the following (pseudocode):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put model in train mode</span></span><br><span class="line">model.train()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    loss = training_step(batch)</span><br><span class="line">    losses.append(loss.detach())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<h4 id="training-epoch-level-metrics"><a class="markdownIt-Anchor" href="#training-epoch-level-metrics"></a> Training epoch-level metrics</h4>
<p>If you want to calculate epoch-level metrics and log them, use the <code>.log</code> method</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logs metrics for each training_step,</span></span><br><span class="line">    <span class="comment"># and the average across the epoch, to the progress bar and logger</span></span><br><span class="line">    self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<ul>
<li>如果在<code>x_step</code>函数中使用了<code>.log()</code>函数，那么这个量将会被逐步记录下来。每一个<code>log</code>出去的变量都会被记录下来，每一个<code>step</code>会集中生成一个字典dict，而每个epoch都会把这些字典收集起来，形成一个字典的list。</li>
</ul>
<p>The .log object automatically reduces the requested metrics across the full epoch. Here’s the pseudocode of what it does under the hood:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">epoch_metric = torch.mean(torch.stack([x[<span class="string">&#x27;train_loss&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> outs]))</span><br></pre></td></tr></table></figure>
<h4 id="train-epoch-level-operations"><a class="markdownIt-Anchor" href="#train-epoch-level-operations"></a> Train epoch-level operations</h4>
<p>If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    preds = ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;other_stuff&#x27;</span>: preds&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">   <span class="keyword">for</span> pred <span class="keyword">in</span> training_step_outputs:</span><br><span class="line">       <span class="comment"># do something</span></span><br></pre></td></tr></table></figure>
<p>The matching pseudocode is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure>
<h2 id="datamodule"><a class="markdownIt-Anchor" href="#datamodule"></a> DataModule</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html">主页面</a></p>
<h3 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h3>
<ul>
<li>
<p>首先，这个<code>DataModule</code>和之前写的Dataset完全不冲突。前者是后者的一个包装，并且这个包装可以被用于多个torch Dataset 中。在我看来，其最大的作用就是把各种train/val/test划分、DataLoader初始化之类的重复代码通过包装类的方式得以被简单的复用。</p>
</li>
<li>
<p>具体作用项目：</p>
<ul>
<li>Download instructions：下载</li>
<li>Processing instructions：处理</li>
<li>Split instructions：分割</li>
<li>Train dataloader：训练集Dataloader</li>
<li>Val dataloader(s)：验证集Dataloader</li>
<li>Test dataloader(s)：测试集Dataloader</li>
</ul>
</li>
<li>
<p>其次，<code>pl.LightningDataModule</code>相当于一个功能加强版的torch Dataset，加强的功能包括：</p>
<ul>
<li><code>prepare_data(self)</code>：
<ul>
<li>最最开始的时候，进行一些无论GPU有多少只要执行一次的操作，如写入磁盘的下载操作、分词操作(tokenize)等。</li>
<li>这里是一劳永逸式准备数据的函数。</li>
<li>由于只在单线程中调用，不要在这个函数中进行<code>self.x=y</code>似的赋值操作。</li>
<li>但如果是自己用而不是给大众分发的话，这个函数可能并不需要调用，因为数据提前处理好就好了。</li>
</ul>
</li>
<li><code>setup(self, stage=None)</code>：
<ul>
<li>实例化数据集（Dataset），并进行相关操作，如：清点类数，划分train/val/test集合等。</li>
<li>参数<code>stage</code>用于指示是处于训练周期(<code>fit</code>)还是测试周期(<code>test</code>)，其中，<code>fit</code>周期需要构建train和val两者的数据集。</li>
<li>setup函数不需要返回值。初始化好的train/val/test set直接赋值给self即可。</li>
</ul>
</li>
<li><code>train_dataloader/val_dataloader/test_dataloader</code>：
<ul>
<li>初始化<code>DataLoader</code>。</li>
<li>返回一个DataLoader量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MNISTDataModule</span>(pl.LightningDataModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir: <span class="built_in">str</span> = <span class="string">&#x27;./&#x27;</span>, batch_size: <span class="built_in">int</span> = <span class="number">64</span>, num_workers: <span class="built_in">int</span> = <span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line"></span><br><span class="line">        self.transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.dims is returned when you call dm.size()</span></span><br><span class="line">        <span class="comment"># Setting default dims here because we know them.</span></span><br><span class="line">        <span class="comment"># Could optionally be assigned dynamically in dm.setup()</span></span><br><span class="line">        self.dims = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        self.num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># download</span></span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;fit&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">            self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assign test dataset for use in dataloader(s)</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;test&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mnist_test = MNIST(self.data_dir, train=<span class="literal">False</span>, transform=self.transform)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)</span><br></pre></td></tr></table></figure>
<h3 id="要点-2"><a class="markdownIt-Anchor" href="#要点-2"></a> 要点</h3>
<ul>
<li>若在DataModule中定义了一个<code>self.dims</code> 变量，后面可以调用<code>dm.size()</code>获取该变量。</li>
</ul>
<h2 id="saving-and-loading"><a class="markdownIt-Anchor" href="#saving-and-loading"></a> Saving and Loading</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html">主页面</a></p>
<h3 id="saving"><a class="markdownIt-Anchor" href="#saving"></a> Saving</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint">ModelCheckpoint</a>: 自动储存的callback module。默认情况下training过程中只会自动储存最新的模型与相关参数，而用户可以通过这个module自定义。如观测一个<code>val_loss</code>的量，并储存top 3好的模型，且同时储存最后一个epoch的模型，等等。例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span></span><br><span class="line">checkpoint_callback = ModelCheckpoint(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    filename=<span class="string">&#x27;sample-mnist-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">    save_top_k=<span class="number">3</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    save_last=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer(gpus=<span class="number">1</span>, max_epochs=<span class="number">3</span>, progress_bar_refresh_rate=<span class="number">20</span>, callbacks=[checkpoint_callback])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>另外，也可以手动存储checkpoint: <code>trainer.save_checkpoint(&quot;example.ckpt&quot;)</code></p>
</li>
<li>
<p><code>ModelCheckpoint</code> Callback中，如果<code>save_weights_only =True</code>，那么将会只储存模型的权重（相当于<code>model.save_weights(filepath)</code>），反之会储存整个模型（相当于<code>model.save(filepath)</code>）。</p>
</li>
</ul>
<h3 id="loading"><a class="markdownIt-Anchor" href="#loading"></a> Loading</h3>
<ul>
<li>
<p>load一个模型，包括它的weights、biases和超参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightingModule.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.learning_rate)</span><br><span class="line"><span class="comment"># prints the learning_rate you used in this checkpoint</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">y_hat = model(x)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>load模型时替换一些超参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.l1 = nn.Linear(self.hparams.in_dim, self.hparams.out_dim)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># if you train and save the model like this it will use these values when loading</span></span><br><span class="line"><span class="comment"># the weights. But you can overwrite this</span></span><br><span class="line">LitModel(in_dim=<span class="number">32</span>, out_dim=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=32, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=128, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH, in_dim=<span class="number">128</span>, out_dim=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>完全load训练状态：load包括模型的一切，以及和训练相关的一切参数，如<code>model, epoch, step, LR schedulers, apex</code>等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = LitModel()</span><br><span class="line">trainer = Trainer(resume_from_checkpoint=<span class="string">&#x27;some/path/to/my_checkpoint.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically restores model, epoch, step, LR schedulers, apex, etc...</span></span><br><span class="line">trainer.fit(model)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="callbacks"><a class="markdownIt-Anchor" href="#callbacks"></a> Callbacks</h2>
<ul>
<li>Callback 是一个自包含的程序，可以与训练流程交织在一起，而不会污染主要的研究逻辑。</li>
<li>Callback 并非只会在epoch结尾调用。pytorch-lightning 提供了数十个hook（接口，调用位置）可供选择，也可以自定义callback，实现任何想实现的模块。</li>
<li>推荐使用方式是，随问题和项目变化的操作，这些函数写到lightning module里面，而相对独立，相对辅助性的，需要复用的内容则可以定义单独的模块，供后续方便地插拔使用。</li>
</ul>
<h3 id="callbacks推荐"><a class="markdownIt-Anchor" href="#callbacks推荐"></a> Callbacks推荐</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks">内建Callbacks</a></p>
<ul>
<li>
<p><code>EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)</code>：根据某个值，在数个epoch没有提升的情况下提前停止训练。</p>
<blockquote>
<p>参数：</p>
<ul>
<li><strong>monitor</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – quantity to be monitored. Default: <code>'early_stop_on'</code>.</li>
<li><strong>min_delta</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#float"><code>float</code></a>) – minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: <code>0.0</code>.</li>
<li><strong>patience</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – number of validation epochs with no improvement after which training will be stopped. Default: <code>3</code>.</li>
<li><strong>verbose</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – verbosity mode. Default: <code>False</code>.</li>
<li><strong>mode</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – one of <code>'min'</code>, <code>'max'</code>. In <code>'min'</code> mode, training will stop when the quantity monitored has stopped decreasing and in <code>'max'</code> mode it will stop when the quantity monitored has stopped increasing.</li>
<li><strong>strict</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – whether to crash the training if monitor is not found in the validation metrics. Default: <code>True</code>.</li>
</ul>
</blockquote>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">trainer = Trainer(callbacks=[early_stopping])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>ModelCheckpoint</code>：见上文<strong>Saving and Loading</strong>.</p>
</li>
<li>
<p><code>PrintTableMetricsCallback</code>：在每个epoch结束后打印一份结果整理表格。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pl_bolts.callbacks <span class="keyword">import</span> PrintTableMetricsCallback</span><br><span class="line"></span><br><span class="line">callback = PrintTableMetricsCallback()</span><br><span class="line">trainer = pl.Trainer(callbacks=[callback])</span><br><span class="line">trainer.fit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># at the end of every epoch it will print</span></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss│train_loss│val_loss│epoch</span></span><br><span class="line"><span class="comment"># ──────────────────────────────</span></span><br><span class="line"><span class="comment"># 2.2541470527648926│2.2541470527648926│2.2158432006835938│0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="logging"><a class="markdownIt-Anchor" href="#logging"></a> Logging</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html">Logging</a>：Logger默认是TensorBoard，但可以指定各种主流Logger<a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#supported-loggers">框架</a>，<a target="_blank" rel="noopener" href="http://xn--Comet-gv5i.ml">如Comet.ml</a>，MLflow，Netpune，或直接CSV文件。可以同时使用复数个logger。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> loggers <span class="keyword">as</span> pl_loggers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(</span><br><span class="line">    save_dir=os.getcwd(),</span><br><span class="line">    version=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;lightning_logs&#x27;</span></span><br><span class="line">)</span><br><span class="line">trainer = Trainer(logger=tb_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or use the same format as others</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One Logger</span></span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=comet_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save code snapshot</span></span><br><span class="line">logger = pl_loggers.TestTubeLogger(<span class="string">&#x27;logs/&#x27;</span>, create_git_tag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Logger</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=[tb_logger, comet_logger])</span><br></pre></td></tr></table></figure>
<p>默认情况下，每50个batch log一次，可以通过调整参数</p>
</li>
<li>
<p>如果想要log输出非scalar（标量）的内容，如图片，文本，直方图等等，可以直接调用<code>self.logger.experiment.add_xxx()</code>来实现所需操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># the logger you used (in this case tensorboard)</span></span><br><span class="line">    tensorboard = self.logger.experiment</span><br><span class="line">    tensorboard.add_image()</span><br><span class="line">    tensorboard.add_histogram(...)</span><br><span class="line">    tensorboard.add_figure(...)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>使用log：如果是TensorBoard，那么：<code>tensorboard --logdir ./lightning_logs</code>。在Jupyter Notebook中，可以使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start tensorboard.</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir lightning_logs/</span><br></pre></td></tr></table></figure>
<p>在行内打开TensorBoard。</p>
</li>
<li>
<p>小技巧：如果在局域网内开启了TensorBoard，加上flag <code>--bind_all</code>即可使用主机名访问：</p>
<p><code>tensorboard --logdir lightning_logs --bind_all</code> -&gt; <code>http://SERVER-NAME:6006/</code></p>
</li>
</ul>
<h3 id="同时使用tensorboard和csv-logger"><a class="markdownIt-Anchor" href="#同时使用tensorboard和csv-logger"></a> 同时使用TensorBoard和CSV Logger</h3>
<p>如果同时使用两个Logger，PL会有睿智操作：如果保存根目录相同，他们会依次建立两个version文件夹，令人窒息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> TensorBoardLogger, CSVLogger</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_loggers</span>():</span><br><span class="line">    loggers = []</span><br><span class="line">    loggers.append(TensorBoardLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tb&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CSVLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;csv&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CometLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tt&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loggers</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_callbacks</span>(<span class="params">logger</span>):</span><br><span class="line">    callbacks = []</span><br><span class="line">    dirpath = <span class="string">f&#x27;lightning_logs/<span class="subst">&#123;logger.name&#125;</span>/version_<span class="subst">&#123;logger.version&#125;</span>/checkpoints&#x27;</span></span><br><span class="line">    callbacks.append(ModelCheckpoint(</span><br><span class="line">        dirpath=dirpath,</span><br><span class="line">        monitor=<span class="string">&#x27;loss_epoch&#x27;</span>,</span><br><span class="line">        filename=<span class="string">&#x27;&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">        save_top_k=<span class="number">3</span>,</span><br><span class="line">        mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">        save_last=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> callbacks</span><br><span class="line"></span><br><span class="line">loggers = load_loggers()</span><br><span class="line">callbacks = load_callbacks(loggers[<span class="number">0</span>])</span><br><span class="line">trainer = pl.Trainer(logger=loggers, callbacks=callbacks)</span><br></pre></td></tr></table></figure>
<h2 id="transfer-learning"><a class="markdownIt-Anchor" href="#transfer-learning"></a> Transfer Learning</h2>
<p><a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html#transfer-learning">主页面</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImagenetTransferLearning</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a pretrained resnet</span></span><br><span class="line">        backbone = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">        num_filters = backbone.fc.in_features</span><br><span class="line">        layers = <span class="built_in">list</span>(backbone.children())[:-<span class="number">1</span>]</span><br><span class="line">        self.feature_extractor = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use the pretrained model to classify cifar-10 (10 image classes)</span></span><br><span class="line">        num_target_classes = <span class="number">10</span></span><br><span class="line">        self.classifier = nn.Linear(num_filters, num_target_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            representations = self.feature_extractor(x).flatten(<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(representations)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h2 id="关于device操作"><a class="markdownIt-Anchor" href="#关于device操作"></a> 关于device操作</h2>
<p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># good (self is LightningModule)</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br></pre></td></tr></table></figure>
<p>For tensors that need to be model attributes, it is best practice to register them as buffers in the modules’s <code>__init__</code> method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">self.t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;t&quot;</span>, torch.rand(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>前面两段是教程中的文本。然而实际上有一个暗坑：</p>
<p>如果你使用了一个中继的<code>pl.LightningModule</code>，而这个module里面实例化了某个普通的<code>nn.Module</code>，而这个模型中又需要内部生成一些tensor，比如图片每个通道的mean，std之类，那么如果你从<code>pl.LightningModule</code>中pass一个<code>self.device</code>，实际上在一开始这个<code>self.device</code>永远是<code>cpu</code>。所以如果你在调用的<code>nn.Module</code>的<code>__init__()</code>中初始化，使用<code>to(device)</code>或干脆什么都不用，结果就是它永远都在<code>cpu</code>上。</p>
<p>但是，经过实验，虽然<code>pl.LightningModule</code>在<code>__init__()</code>阶段<code>self.device</code>还是<code>cpu</code>，当进入了<code>training_step()</code>之后，就迅速变为了<code>cuda</code>。所以，对于子模块，最佳方案是，使用一个<code>forward</code>中传入的量，如<code>x</code>，作为一个reference变量，用<code>type_as</code>函数将在模型中生成的tensor都放到和这个参考变量相同的device上即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RDNFuse</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_norm_func</span>(<span class="params">self, ref</span>):</span><br><span class="line">        self.mean = torch.tensor(np.array(self.mean_sen), dtype=torch.float32).type_as(ref)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mean&#x27;</span>):</span><br><span class="line">            self.init_norm_func(x)</span><br></pre></td></tr></table></figure>
<h2 id="关于limit_train_batches选项"><a class="markdownIt-Anchor" href="#关于limit_train_batches选项"></a> 关于<code>limit_train_batches</code>选项</h2>
<p>这里涉及到一个问题，就是每个epoch使用部分数据而非全部时，程序将会怎么工作。</p>
<blockquote>
<p>The shuffling happens when the iterator is created. In the case of the for loop, that happens just before the for loop starts. You can create the iterator manually with:</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator gets created, the data has been shuffled at this point.</span></span><br><span class="line">data_iterator = <span class="built_in">iter</span>(namesTrainLoader)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>By default the data loader uses <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler"><code>torch.utils.data.RandomSampler</code></a> if you set <code>shuffle=True</code> (without providing your own sampler). Its implementation is very straight forward and you can see where the data is shuffled when the iterator is created by looking at the <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/f3e620ee83f080283445aa1a7242d40e30eb6a7f/torch/utils/data/sampler.py#L103-L107"><code>RandomSampler.__iter__</code></a> method:</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">    <span class="keyword">if</span> self.replacement:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The return statement is the important part, where the shuffling takes place. It simply creates a random permutation of the indices.</p>
<p>That means you will see your entire dataset every time you fully consume the iterator, just in a different order every time. Therefore there is no data lost (not including cases with <code>drop_last=True</code>) and your model will see all data at every epoch.</p>
</blockquote>
<p>总结下来，如果使用了<code>shuffle=True</code>选项，那么即使每次都不跑完整个epoch，你还是有机会见到所有的数据的。数据集的shuffle发生在<code>iter</code>被创建的时候，在我们一般的代码中，也就是内层for循环开始时。但如果你没有选择<code>shuffle=True</code>，那你将永远只能看到你设定的前面N个数据。</p>
<h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2>
<ul>
<li>
<p><code>pl.seed_everything(1234)</code>：对所有相关的随机量固定种子。</p>
</li>
<li>
<p>使用LR Scheduler时候，不用自己<code>.step()</code>。它也被Trainer自动处理了。<a target="_blank" rel="noopener" href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html?highlight=scheduler#">Optimization 主页面</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single optimizer</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">        loss = model.training_step(batch, batch_idx, ...)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Multiple optimizers</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">     <span class="keyword">for</span> opt <span class="keyword">in</span> optimizers:</span><br><span class="line">        disable_grads_for_other_optimizers()</span><br><span class="line">        train_step(opt)</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">     scheduler.step()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>关于划分train和val集合的方法。与PL无关，但很常用，两个例子：</p>
<ol>
<li><code>random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</code></li>
<li>如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br></pre></td></tr></table></figure>
<p>Parameters：</p>
<ul>
<li><strong>dataset</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</li>
<li><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</li>
<li><strong>generator</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>Generator</em></a>) – Generator used for the random permutation.</li>
</ul>
</li>
<li>
<p>如果使用了<code>PrintTableMetricsCallback</code>，那么<code>validation_step</code>不要return内容，否则会炸。</p>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/miracleyoo">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/deep-learning/" rel="tag"># deep-learning</a>
              <a href="/tags/pytorch/" rel="tag"># pytorch</a>
              <a href="/tags/pytorch-lightning/" rel="tag"># pytorch-lightning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/02/03/satellite-basic/" rel="prev" title="Analyzing Geography Data (Beginners' Tutorial)">
                  <i class="fa fa-chevron-left"></i> Analyzing Geography Data (Beginners' Tutorial)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/27/blender-syn-data-gen-tutorial/" rel="next" title="blender-syn-data-gen-tutorial">
                  blender-syn-data-gen-tutorial <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Miracle Yoo</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">207k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">12:32</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/miracleyoo" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css" integrity="sha256-gMRN4/6qeELzO1wbFa8qQLU8kfuF2dnAPiUoI0ATjx8=" crossorigin="anonymous">


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"miracleyoo/utterances-repo","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
