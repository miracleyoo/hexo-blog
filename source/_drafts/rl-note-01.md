---
title: 强化学习学习笔记-01
tags: [machine learning, deep learning, reinforcement learning]
---

# 强化学习学习笔记

## Reinforcement Learning Basic

**状态State**和**动作Action**存在映射关系，也就是一个state可以对应一个action，或者对应不同动作的概率（常常用概率来表示，概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出的关系，而状态State到动作Action的过程就称之为一个**策略Policy，**一般用π表示，也就是需要找到以下关系(其中a是action，s是state)：

**一一对应表示**：a=π(s)

**概率表示**：π(a|s)

## DQN

**输入（1）**：状态（State）

**输出（1）**：该状态下可能采取的各种动作（Action）时对应的Q值

**输入（2）**：状态（State）和动作（Action）

**输出（2）**：该状态下采取该动作（Action）时对应的Q值

**目标**：找到一个最优的策略Policy从而使Reward最多

**特点**：

1. 不是直接输出当前状态下各动作的概率，每种动作都可能以其对应概率被选到，而是输出当前状态下选择各动作后带来的Q值，直接选择最大的，并随机以一定的可能性选择其他动作。
2. 是一种 off-policy 离线学习法，它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历。

**类比**：Q表。这里的神经网络的功能就好比一个Q表，输入要查找的内容，输出相应值。

**工程实现**：

1. 其训练过程中用到两个网络，其中一个目标网络用来表示target，拥有估计网络很久前的参数，参数过一段时间会和估计网络同步一次；而另一个估计网络则是每轮训练更新中被更新的网络。
2. 有一个记忆库用来储存之前的记忆，每次训练时随机抽一个batch扔到网络里训练。
3. Fixed Q-targets和记忆库都是打乱经历相关性的机理，也使得神经网络更新更有效率。

**更新算法**：![img](https://ws3.sinaimg.cn/large/006tNbRwgy1fxearwxiqnj30so0n2jxu.jpg)

## Policy Gradient

**输入**： 状态（State）

**输出**：直接是动作（Action）（而非Q值）

**公式**：a=π(s, θ) 或 a=π(a|s, θ)

**特点**：输出的直接是当前状态下采取各种可能动作的概率。每种动作都可能以其对应概率被选到。



## DDPG