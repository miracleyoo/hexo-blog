<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Miracleyoo</title>
  
  
  <link href="https://www.miracleyoo.com/atom.xml" rel="self"/>
  
  <link href="https://www.miracleyoo.com/"/>
  <updated>2023-04-23T02:43:20.893Z</updated>
  <id>https://www.miracleyoo.com/</id>
  
  <author>
    <name>Miracle Yoo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Curriculum Vitae -- Zhongyang Zhang</title>
    <link href="https://www.miracleyoo.com/2099/10/03/resume/"/>
    <id>https://www.miracleyoo.com/2099/10/03/resume/</id>
    <published>2099-10-03T22:44:20.000Z</published>
    <updated>2023-04-23T02:43:20.893Z</updated>
    
    <content type="html"><![CDATA[<p>The following document is my resume:</p><span id="more"></span><div class="pdf-container" data-target="cv.pdf" data-height="1200px"></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;The following document is my resume:&lt;/p&gt;</summary>
    
    
    
    
    <category term="Resume" scheme="https://www.miracleyoo.com/tags/Resume/"/>
    
  </entry>
  
  <entry>
    <title>Spiking Neural Network (SNN) 学习笔记</title>
    <link href="https://www.miracleyoo.com/2023/04/22/snn-notes/"/>
    <id>https://www.miracleyoo.com/2023/04/22/snn-notes/</id>
    <published>2023-04-23T01:01:06.000Z</published>
    <updated>2023-04-23T01:01:16.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ul><li><p>Input:</p><ul><li>One counter-intuitive fact is that almost all the mainstream SNN frameworks’ input shape is not a series of 1-D spike signals. Actually, the input shape is pretty much like other ANNs, but with an additional <code>T</code> time dimension. Here is the common input shape: <code>T x N x C x H x W</code>, where <code>T</code> means time, <code>N</code> means batch size, <code>C</code> means channel number, and <code>H, W</code> stands for the input image size. Except for the first two dimensions (<code>T, N</code>), the rest of the dimensions are quite flexible and can be modified as needed, and we can simplify the input shape as <code>T x N x X</code>.</li><li>For the Loihi SNN hardware platform, the input size is a bit different, but it’s a simple permutation: <code>N x X x T</code>.</li></ul></li></ul><span id="more"></span><ul><li><p>Another counter-intuitive side of SNN is, SNN is usually not used solely. Here, I mean people don’t solely use SNN neurons to build their entire model. The neuron-alone model can be potentially more efficient, but it is not good at capturing local and global patterns, and features. Instead, neurons here are only treated as a substitute for <strong>activation functions</strong> in ANNs.</p></li><li><p>Characteristics of SNN:</p><ul><li><p><strong>Generative Property of SNN</strong>: Generative, as its name shows, for a trained SNN, for an output neuron of a certain output class, if we scale the values of all the input neurons connected to this neuron and rearrange them properly, we can get a general pattern image of this class. This pattern could be clear and distinctive, or blurry and non-distinguishable, which reflects how well this class is trained. Like the image below, the last one is worse compared to the first three. This property will be used for demonstrating the results.</p><img data-src="image-20220707185619913.png" alt="image-20220707185619913" style="zoom:50%;"></li><li><p><strong>Variable Threshold</strong>: Each pattern (e.g., in MNIST, the different number) has a different number of activations. For example, the number 1 has less activation (white pixels in the 28x28 image) than the number 8, generally speaking. Therefore, those classes with more activation will overshadow all the other classes with less activation. To avoid this kind of inter-class imbalance, we need to set a different threshold for each class, which is calculated based on the number of activation each class contains.</p></li><li><p><strong>Lateral Inhibition</strong>: Many different neurons in the same layer could get excited at different time stamps, however, when one neuron gets excited, this mechanism (lateral inhibition) will reduce the activity and inhibit other neurons in the same layer to get excited. This property is also called <em>Winner-Takes-All(WTA)</em>. In biology, the neuron gets excited first and lowers down the membrane potential of other neurons in the same layer.</p></li></ul></li></ul><h2 id="how-to-rewrite-an-ann-to-snn"><a class="markdownIt-Anchor" href="#how-to-rewrite-an-ann-to-snn"></a> How to Rewrite an ANN to SNN</h2><ul><li><p>First, select a proper neuron type (let’s call it Nx), and replace all the activation functions in your original ANN model with Nx.</p><blockquote><p>The literature <code>[P1]</code> provides a theoretical basis for analyzing the conversion of ANN to SNN. The theory shows that the IF neuron in SNN is an unbiased estimator of the ReLU activation function over time.</p></blockquote></li><li><p>Then, remove all batch normalization layers.</p></li><li><p>If you plan to eventually deploy the model to the Loihi platform, you should set all the <code>bias</code> items in related layers to False, like <code>Conv</code> and <code>Linear</code>. This is because <code>bias</code> is not supported in Loihi.</p></li></ul><h2 id="stdp"><a class="markdownIt-Anchor" href="#stdp"></a> STDP</h2><ul><li><p>The algorithm that is commonly used in neuron training is called Spike Time Dependent Plasticity (STDP, 突触时间依赖可塑性)。</p></li><li><p>STDP can only be applied to the training of neurons (SNN layers), but not other ANN layers used in the same network.</p></li><li><p>STDP is actually a biological process used by the brain to modify its neural connections (synapses). Since the unmatched learning efficiency of the brain has been appreciated for decades, this rule was incorporated in ANNs to train a neural network. Modeling of weights is based on the following two rules -</p><ul><li>Any synapse that contributes to the firing of a post-synaptic neuron should be made strong i.e its value should be increased.</li><li>Synapses that don’t contribute to the firing of a post-synaptic neuron should be diminished i.e its value should be decreased.</li></ul><p>Here is an explanation of how this algorithm works:</p><p>Consider the scenario depicted in this figure</p><p><a href="https://github.com/Shikhargupta/Spiking-Neural-Network/blob/master/images/spikes.jpg"><img data-src="spikes.jpg" alt="img"></a></p><p>Four neurons connect to a single neuron by synapse. Each pre-synaptic neuron is firing at its own rate and the spikes are sent forward by the corresponding synapse. The intensity of the spike translated to the post-synaptic neuron depends upon the strength of the connecting synapse. Now, because of the input spikes membrane potential of the post-synaptic neuron increases and sends out a spike after crossing the threshold. At the time when the post-synaptic neuron spikes, we’ll monitor which all pre-synaptic neurons helped it to fire. This could be done by observing which pre-synaptic neurons sent out spikes before post-synaptic neurons spiked. This way they helped in the post-synaptic spike by increasing the membrane potential and hence the corresponding synapse is strengthened. The factor by which the weight of the synapse is increased is inversely proportional to the time difference between post-synaptic and pre-synaptic spikes given by this graph</p><p><a href="https://github.com/Shikhargupta/Spiking-Neural-Network/blob/master/images/stdp_curve.jpg"><img data-src="stdp_curve.jpg" alt="img" style="zoom:20%;"></a></p></li></ul><h2 id="parameters"><a class="markdownIt-Anchor" href="#parameters"></a> Parameters</h2><p>Building a Spiking Neural Network from scratch is not an easy job. There are several parameters that need to be tuned and taken care of. Combinations of so many parameters make it worse. Some of the major parameters that play an important role in the dynamics of a network are -</p><ul><li>Learning Rate</li><li>Threshold Potential</li><li>Weight Initialization</li><li>Number of Spikes Per Sample</li><li>Range of Weights</li></ul><h2 id="frequently-used-activation-layersneurons"><a class="markdownIt-Anchor" href="#frequently-used-activation-layersneurons"></a> Frequently-used Activation Layers/Neurons</h2><ul><li>IF stands for Integrate-and-Fire. It is a simple model in which a neuron integrates incoming inputs over time and generates a spike when the membrane potential reaches a threshold value.</li><li>LIF stands for Leaky Integrate-and-Fire. It is an extension of the IF model that takes into account the leakage of charge through the neuron membrane over time. This model is widely used in SNNs due to its simplicity and efficiency.</li><li>PLIF stands for Poisson Leaky Integrate-and-Fire. It is a model that takes into account the stochastic nature of synaptic inputs in biological neurons. In this model, synaptic inputs are modeled as a Poisson process, and the membrane potential of the neuron is determined by the integration of these inputs over time, taking into account the leakage of charge through the membrane.</li></ul><h2 id="transform-traditional-rgb-images-to-events"><a class="markdownIt-Anchor" href="#transform-traditional-rgb-images-to-events"></a> Transform Traditional RGB Images to Events</h2><ul><li>Methods:<ol><li>Show images on a paper/monitor, and then turn on/off the light.</li><li>Show images on a paper/monitor, and move the image/camera/sensor horizontally to keep the depth the same.</li><li>Show images on a paper/monitor, and rotate the camera.</li></ol></li><li><a href="https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/full">Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades</a>: The original paper proposed for generating N-MNIST (Event camera version MNIST). It is collected by using an actual DVS to capture the MNIST images shown on a monitor, which slightly rotates the DVS to generate brightness change.</li><li>For normal RGB images, we can also generate <em>spikes</em> for a certain period of time based on a certain distribution (e.g., Poisson Distribution) of pixel positions with a fixed frequency.</li></ul><h2 id="specialized-hardware"><a class="markdownIt-Anchor" href="#specialized-hardware"></a> Specialized Hardware</h2><ul><li><strong>Intel Loihi 2</strong>: A hardware device designed for neuromorphic computation.<ul><li>Asynchronous neurons applied.</li><li>Come with a new software platform, Lava.</li><li>Lava is <strong>NOT</strong> compatible with other platforms like PyTorch. Conversion needed.</li><li><a href="https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html">Official Site</a></li><li><a href="https://www.intel.com/content/www/us/en/research/neuromorphic-computing-loihi-2-technology-brief.html">Specification</a>.</li><li><a href="https://github.com/lava-nc/lava/issues/153">Pit Holes</a></li></ul></li></ul><h2 id="spikingjelly"><a class="markdownIt-Anchor" href="#spikingjelly"></a> spikingjelly</h2><ul><li><p><a href="https://spikingjelly.readthedocs.io/zh_CN/latest/activation_based/basic_concept.html">Link</a></p></li><li><p>Different from what I expected to be the input (a series of 1-D features along the time axis), the input shape of spikingjelly is very much like a normal input shape (<code>N x C x H x W</code>, or even <code>T x N x C x H x W</code>).</p></li><li><p>The training strategy is not any different from other Pytorch models.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">y = net(x)</span><br><span class="line">loss = criterion(y, label)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></li><li><p>To transform a CNN-based model to an SNN model, just remove all the activation layers and replace them with any neurons you like (e.g., LIF).</p></li><li><p>An SNN neuron layer, actually, can be seen as an array of neurons that has the same shape as your input. Let’s call it a sub-neuron. Each sub-neuron works separately, and neighboring neurons don’t share anything with them. In the entire training/prediction process, each sub-neuron can be seen as an RNN. The spatial understanding and local feature capture ability are brought by normal CNN layers like <code>Conv2d</code>.</p></li><li><p>You can select each neuron’s step mode from single-step to <code>multi-step</code>. The first one doesn’t take care of the time axis and you have to manually manage the behavior along the time axis, and the latter simply can be seen as a wrapper of standard loop-based stepping on a time series.</p></li><li><p>SpikingJelly supports both Gradient descent and STDP training scheme. Actually, you can even merge these two training strategies, train neuron layers using STDP and train other CNN layers using GD. <a href="https://spikingjelly.readthedocs.io/zh_CN/latest/activation_based_en/stdp.html">Link</a></p></li><li><p>It supports direct conversion to the Intel Loihi platform (<a href="https://spikingjelly.readthedocs.io/zh_CN/latest/activation_based_en/lava_exchange.html">Link</a>). You can even train an ANN at first and convert it to SNN with a bunch of patterns. (<a href="https://spikingjelly.readthedocs.io/zh_CN/latest/activation_based_en/ann2snn.html">Link</a>)</p></li></ul><h2 id="helpful-softwarepackages"><a class="markdownIt-Anchor" href="#helpful-softwarepackages"></a> Helpful Software/Packages</h2><ul><li><p><code>AWESOME</code>: <a href="https://github.com/realamirhe/awesome-computational-neuro-science">Computational Neuro Science</a></p></li><li><p><code>FRAMEWORK</code> <a href="https://spikingjelly.readthedocs.io/zh_CN/latest/index.html">惊蛰 Spiking Jelly</a>: A PyTorch-based SNN framework.</p></li><li><p><code>LIBRARY</code> <a href="https://github.com/neuromorphs/tonic">Tonic</a>: A tool to facilitate the download, manipulation and loading of event-based/spike-based data. It’s like PyTorch Vision but for neuromorphic data.</p></li><li><p><code>PACKAGE</code> <a href="https://github.com/BindsNET/bindsnet">BindsNET</a> is a Python package used for simulating spiking neural networks (SNNs) on CPUs or GPUs using PyTorch Tensor functionality. Similar to Spiking Jelly and need more comparison. This package is developed by Hava’s group, so potentially easier to deploy.</p></li></ul><h2 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h2><h3 id="papers"><a class="markdownIt-Anchor" href="#papers"></a> Papers</h3><ol><li>Rueckauer B, Lungu I-A, Hu Y, Pfeiffer M and Liu S-C (2017) Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification. Front. Neurosci. 11:682.</li></ol><h3 id="blogs"><a class="markdownIt-Anchor" href="#blogs"></a> Blogs</h3><ol><li><a href="https://github.com/realamirhe/awesome-computational-neuro-science/blob/master/tutorials.md">Realamirhe’s Tutorial on ANN&amp;SNN</a></li><li><a href="https://github.com/Shikhargupta/Spiking-Neural-Network">Spiking-Neural-Network</a>: A python implementation of hardware efficient spiking neural network. It contains a good introduction to SNN.</li><li><a href="https://spikingjelly.readthedocs.io/zh_CN/latest/index.html">惊蛰 Spiking Jelly</a>: General tutorial is also provided.</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;points&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#points&quot;&gt;&lt;/a&gt; Points&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Input:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One counter-intuitive fact is that almost all the mainstream SNN frameworks’ input shape is not a series of 1-D spike signals. Actually, the input shape is pretty much like other ANNs, but with an additional &lt;code&gt;T&lt;/code&gt; time dimension. Here is the common input shape: &lt;code&gt;T x N x C x H x W&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; means time, &lt;code&gt;N&lt;/code&gt; means batch size, &lt;code&gt;C&lt;/code&gt; means channel number, and &lt;code&gt;H, W&lt;/code&gt; stands for the input image size. Except for the first two dimensions (&lt;code&gt;T, N&lt;/code&gt;), the rest of the dimensions are quite flexible and can be modified as needed, and we can simplify the input shape as &lt;code&gt;T x N x X&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For the Loihi SNN hardware platform, the input size is a bit different, but it’s a simple permutation: &lt;code&gt;N x X x T&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="SNN" scheme="https://www.miracleyoo.com/tags/SNN/"/>
    
  </entry>
  
  <entry>
    <title>DVS Domain Adaption Note</title>
    <link href="https://www.miracleyoo.com/2023/04/19/dvs-da-paper/"/>
    <id>https://www.miracleyoo.com/2023/04/19/dvs-da-paper/</id>
    <published>2023-04-20T01:25:34.000Z</published>
    <updated>2023-04-23T01:25:57.689Z</updated>
    
    <content type="html"><![CDATA[<h2 id="unsupervised-domain-adaption-algorithms">Unsupervised Domain Adaption Algorithms</h2><ul><li>GRL: Gradient Reversal Layer。 目标是让两个domain的distribution在feature extractor眼中无法区分（即match两个domain使其分布趋同）。</li><li>MMD：Minimize the Maximum Mean Discrepancy between the target and source domain。最小化两个域之间的最大平均差异。作者提出了一个metric来衡量域间差异，通过加loss来抑制这个差异，最终达到让网络最终层输出与域无关的稳定feature。</li><li>AFN：比较玄学。他们说之所以在target domain上表现不好是因为目标向量的norm相比源域的更小。所以它们就逐渐提高深度embedding的L2 norms来解决这个问题。</li></ul><span id="more"></span><ul><li>Rotation：作者添加了一个额外的自监督任务，即预测图片的绝对旋转角度。因为它同时接受源域和目标域的输入，所以理论上会学的更倾向于用共通的参数。甚至还有人扩展了这个方法，让网络直接去预测paired多模态图片的相对旋转角度。</li><li>Entropy：Grandvalet提出了一种表征目标域不确定性的方法，通过加一个正则项来帮助减缓domain shift的影响。</li></ul><h2 id="n-rod-a-neuromorphic-dataset-for-synthetic-to-real-domain-adaptation">N-ROD: a Neuromorphic Dataset for Synthetic-to-Real Domain Adaptation</h2><ul><li><p>CVPRW, 2021, <a href="https://n-rod-dataset.github.io/home/">Site</a></p></li><li><p>核心思想就是用已有的上面提到的各种UDA方法来减轻Synthetic和Real Event Data的domain shift。</p></li><li><p>Pipeline也非常直接，就是给RGB和Event frame分别整一个Feature Extractor，然后让Synthetic的rgb和events feature concat到一起，real的concat到一起，送入一个通用的Domain Adaptation Block(DABlock)作为一个branch，另一个branch做预测。</p><p><img data-src="dvs-da-paper/image-20230422182441632.png" alt="image-20230422182441632" style="zoom:50%;"></p></li><li><p>训练的时候他们用了RGB和Event Frames，但测试的时候只用Event Frames。</p></li><li><p>值得一提的是，既然能用这种架构，说明上面那些UDA方法都可以被归纳为一个auxiliary head来进行plug-and-play。</p></li><li><p>另外，他们提供了一个<a href="https://polimi365-my.sharepoint.com/personal/10425666_polimi_it/_layouts/15/onedrive.aspx?ga=1">数据集</a>，包含有相同目标的real和syn版本。</p></li></ul><h2 id="bridging-the-gap-between-events-and-frames-through-unsupervised-domain-adaptation">Bridging the Gap Between Events and Frames Through Unsupervised Domain Adaptation</h2><ul><li><p>IEEE ROBOTICS AND AUTOMATION LETTERS, 2022</p></li><li><p>感性的认知上，这篇文章主要的手法是循环式Loss，类似cycle gan；主要的逻辑是style transfer。</p></li><li><p>核心优点有两个：</p><ol type="1"><li>不使用视频生成events，而是从单个image直接生成。</li><li>不需要paired data。</li></ol></li><li><p>缺点：</p><ol type="1"><li>高度依赖所参考的events frame的参数，如dvs相机的ego motion speed，events frame的accumulation方法和参数（如时间）</li><li>有前提假设：环境亮度恒定，且events frame的采样时间短。</li></ol></li><li><p>不被拒稿的创新点：</p><ol type="1"><li><p>首先是整体流程相对新颖，他们通过先生成optical flow，再结合frame图像的gradient和time difference生成events的。</p><p><img data-src="dvs-da-paper/image-20230419112731101.png" alt="image-20230419112731101" style="zoom:50%;"></p></li><li><p>把events frame和rgb image之间的domain adaption转化成了一个style transfer问题。events frame只需要贡献style feature就好，而rgb image则基于这个style，它自己的feature map，以及它自己的gradient输出相应的events frame。最后的loss计算也非常有style transfer的味道：</p><ul><li>计算原始rgb image和最终生成的events frame的feature map二者对应的content feature的Loss。</li><li>计算原始event frame和最终生成的events frame的feature map二者对应的style feature的Loss。</li></ul></li><li><p>通过上述手段，相当于作者强制让event feature extractor和rgb frame feature extractor输出的content feature达成一致，从而做到让RGB frame干的活（task，这里是目标检测）同样的events frame出来的feature也能干。</p></li><li><p>虽然作者说他的</p></li><li><p>作者大量使用了Loss，尤其是Adversarial Loss来enforce每个环节生成的内容都足够符合目标域的distribution。</p><p>以下是主Loss：</p><ol type="1"><li>原始rgb image的feature map做输入，在task（这里是object Detection）上面的loss。</li><li>最终生成的events frame的feature map做输入，在task上面的loss。</li></ol><p>循环（Cycle）Loss：</p><ol type="1"><li>Content部分：让生成的events frame的feature map与最初rgb image的feature map做L1 Loss</li><li>Style部分：让生成的events frame的style feature map和最初events frame的feature map做L1 Loss</li></ol><p>对抗Loss：</p><ol type="1"><li><p>首先是最开始rgb image和event frame生成的两个feature map，训练一个PatchGAN Discriminator来尽可能区分二者，同时加一个generator loss到主loss。</p><p><img data-src="dvs-da-paper/image-20230419135409950.png" alt="image-20230419135409950" style="zoom:33%;"></p></li><li><p>对生成的events frame和原始的events frame也做一个GAN，尽可能让Discriminator无法区分出来生成的和真实的。</p><p><img data-src="dvs-da-paper/image-20230419135721458.png" alt="image-20230419135721458" style="zoom:33%;"></p></li></ol></li><li><p>作者不但在深度学习pipeline上做的很好，还充分利用了events 生成的物理知识，通过结合image gradient、optical flow、event trigger mechanism等，在整体流程中把物理prior完美融入。</p></li></ol></li><li><p>核心idea是把event features拆分成两部分，一部分是内容，另一部分是运动特征。通过这种方式，他们可以很高效的让events和images的潜在空间（latent space）进行匹配。他们使用了一个生成式事件模型。</p></li><li><p>虽然说的是直接生成events，实际上生成的是accumulated events frame。</p></li><li><p>注意他的<span class="math inline">\(R_{ref}\)</span>并非一个深度学习block，而是一个使用了原图gradient和pseudo flow的内积操作！</p></li><li><p>同样值得一提的是，他们在pseudo flow generation部分并没有加额外的supervision！之所以即使这样还能生成挺好的pseudo flow，是因为如果想用image gradient和这里生成的东西内积后得到events，根据公式这个feature map必须是flow：</p><p><img data-src="dvs-da-paper/image-20230419163354478.png" alt="image-20230419163354478" style="zoom:33%;"></p></li><li><p>虽然左下角那个content feature没用到，但同样的events feature extractor，再最右边生成的feature map则被用来做了content reference。</p><p><img data-src="dvs-da-paper/image-20230419115118001.png" alt="image-20230419115118001" style="zoom:50%;"></p></li><li><p>作者怎么控制的events feature extractor出来的两个feature map分别代表content和style？</p><ul><li>通过他定义的各种GAN Loss和Cycle Loss教这两个feature map做人。</li></ul></li><li><p>他们做的augmentation也挺有趣，基本是这样的：先正常预测到pseudo flow为止，然后适当随机给预测的全局flow加一个bias，预测出events后再次生成pseudo flow，进行cycle loss。目的是确保events frame feature extractor输出的两个feature map分别表征内容和style（motion）。</p><p><img data-src="dvs-da-paper/image-20230419165600573.png" alt="image-20230419165600573" style="zoom:50%;"></p></li></ul><h2 id="object-tracking-by-jointly-exploiting-frame-and-event-domain">Object Tracking by Jointly Exploiting Frame and Event Domain</h2><ul><li><p>ICCV, 2021</p></li><li><p>目标问题：Object Tracking。</p></li><li><p>Modality：DVS+RGB。本文研究的不是domain shift而是domain fusion，或者说，如何用DVS信息来提高RGB表现。</p></li><li><p>Pipeline理解：</p><ul><li>首先，每个rgb frame会对应好几个events frame。</li><li>events frame们先每个单独进行feature extraction，self-attention，然后结果被element-wise add到一起。</li><li>rgb frame则直接过一系列的feature extraction blocks，直接得到feature map。</li><li>这两者的feature map被送到CDMS block中做cross-attention互相融合，然后过adaptive weighting module被分别赋予不同的权重，最后加到一起得到一个统一的feature map。</li><li>上述过程中，无论是rgb还是events frame，都有深浅两个branch，浅的那个就是early exit。最终会得到两个feature map，一个是<span class="math inline">\(K_l\)</span>，一个是<span class="math inline">\(K_h\)</span>。</li><li>后面其实就没什么好说的了，就是单纯把这两个feature map放到classifier和Bbox regressor里面就好了。</li></ul><p><img data-src="dvs-da-paper/image-20230419174703447.png" alt="image-20230419174703447" style="zoom:33%;"></p><p><img data-src="dvs-da-paper/image-20230419174720669.png" alt="image-20230419174720669" style="zoom: 33%;"></p></li></ul><h2 id="multi-domain-collaborative-feature-representation-for-robust-visual-object-tracking">Multi-domain collaborative feature representation for robust visual object tracking</h2><ul><li>The Visual Computer， 2021</li><li>目标问题：Object Tracking</li><li>Pipeline：<ul><li>整体思路是比较典中典的N路并行多模态运算啦。<ol type="1"><li>第一路：一个SNN feature extractor，输入是raw events（根据shape推算，其实应该是time voxel）。</li><li>第二路：accumulate events into event count frames, 然后和RGB图concat。之后过一个feature extractor。</li><li>第三路：RGB-only feature extractor。</li></ol></li><li>最后把三路结果concat起来，过一些block，得到tracking result。</li></ul></li><li>有点意思的是，他们的SNN branch是直接用的pretrain model然后给freeze起来用了。模型来自于“Eventbased angular velocity regression with spiking networks”。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;unsupervised-domain-adaption-algorithms&quot;&gt;Unsupervised Domain Adaption Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GRL: Gradient Reversal Layer。 目标是让两个domain的distribution在feature extractor眼中无法区分（即match两个domain使其分布趋同）。&lt;/li&gt;
&lt;li&gt;MMD：Minimize the Maximum Mean Discrepancy between the target and source domain。最小化两个域之间的最大平均差异。作者提出了一个metric来衡量域间差异，通过加loss来抑制这个差异，最终达到让网络最终层输出与域无关的稳定feature。&lt;/li&gt;
&lt;li&gt;AFN：比较玄学。他们说之所以在target domain上表现不好是因为目标向量的norm相比源域的更小。所以它们就逐渐提高深度embedding的L2 norms来解决这个问题。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="DVS" scheme="https://www.miracleyoo.com/tags/DVS/"/>
    
    <category term="domain-adaption" scheme="https://www.miracleyoo.com/tags/domain-adaption/"/>
    
  </entry>
  
  <entry>
    <title>3D Multi-Person Pose Estimation (HPE) Reviews</title>
    <link href="https://www.miracleyoo.com/2023/04/02/3d-multi-hpe-review/"/>
    <id>https://www.miracleyoo.com/2023/04/02/3d-multi-hpe-review/</id>
    <published>2023-04-03T00:53:13.000Z</published>
    <updated>2023-04-23T00:53:40.875Z</updated>
    
    <content type="html"><![CDATA[<h2 id="taxonomy"><a class="markdownIt-Anchor" href="#taxonomy"></a> Taxonomy</h2><ul><li>By method: Top-Down; Bottom-Up</li><li>By time dimension: Frame; Time Sequence</li><li>By input type: Monocular; Multi-View</li><li>By human number: Single; Multiple</li><li>By output type: Skeleton; Mesh (SMPL, SCAPE, DensePose)</li></ul><span id="more"></span><p><img data-src="image-20221212154341647.png" alt="image-20221212154341647"></p><h2 id="possible-self-supervision-methods"><a class="markdownIt-Anchor" href="#possible-self-supervision-methods"></a> Possible Self-Supervision Methods</h2><ol><li>Do an “event inpainting”, and try to predict the missing events.</li><li>Same as the point 1, but predict the number of events missing instead.</li><li>Set a certain area (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>:</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo>:</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_0:x_1,y_0:y_1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>) in <code>xy</code> plane all to 0, given a small piece <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>:</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo>:</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>0</mn></msub><mo>:</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_0:x_1,y_0:y_1, t_0:t_1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>, and try to predict the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">t_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li><li>Given a subgraph of events in the human body, try to predict the following events that are generated by the same part.</li><li>Randomly set some nodes’ <code>y</code>(which part in the human body, which human id, …) to 0, and let the network refill.</li><li>Predict the position of the future event for a certain part based on the “optical flow/motion vector” generated.</li></ol><h2 id="major-points-from-papers"><a class="markdownIt-Anchor" href="#major-points-from-papers"></a> Major Points from Papers</h2><ul><li>In order to be flexible enough, researchers choose to predict pixel-based camera-centered coordinates instead of calibrated absolute coordinates. In this way, they can bypass the camera’s intrinsic and extrinsic issues and not be bound to a single type of hardware. Also, they can work on any images without knowing the parameters of the camera. In most cases, we also don’t need the absolute coordinates.</li><li>To make the system support multi-person, one common way is to have a two-step structure and use a pretrained bounding box detection network in the first stage. This could be hard for us since our input feature (TORE) is far different from the RGB images and definitely need to fine-tune.</li><li>Instead of Single-Image-based methods, more papers focus on video-based 3D MHPE nowadays.</li></ul><h2 id="common-points-among-all-hpe-papers"><a class="markdownIt-Anchor" href="#common-points-among-all-hpe-papers"></a> Common Points Among All HPE Papers</h2><ul><li><p>For 2D pose estimation, the following are common tricks:</p><ul><li><p>Multi-tasking <code>※ Highly possible to be applied in our work, but the task need to be carefully designed based on the graph's characteristics.</code></p><ul><li>Jointly do 2D/3D pose estimation. <code>※ Possible</code></li><li>Human activity recognition. <code>※ Not applicable</code></li><li>Human part segmentation. <code>※ Maybe, but need a pipeline to generate human part label</code></li></ul></li><li><p>Teacher-student/Distill/Quantification -&gt; small nets <code>※ This may not work here, as GNN itself is small and efficient enough.</code></p></li><li><p>Refinement blocks <code>※ We can definitely use the though of refinement</code></p></li><li><p>Hourglass scheme <code>※ We can try to build a similar structure in GNN</code></p></li><li><p>Multi-stage network</p></li></ul><img data-src="image-20221212162944138.png" alt="image-20221212162944138" style="zoom:50%;"></li><li><p>For 3D HPE, the following frameworks are frequently used:</p><ul><li>Image -&gt; Heatmap -&gt; Joints Coordinates</li><li>Image -&gt; 2D Pose -&gt; 3D</li><li>Image -&gt; 2D Pose, + Image -&gt; 3D</li></ul><img data-src="image-20221212163200109.png" alt="image-20221212163200109" style="zoom:50%;"></li><li><p>For SMPL, here is the common pipeline:</p><img data-src="image-20221212163427300.png" alt="image-20221212163427300" style="zoom:50%;"></li></ul><h2 id="camera-distance-aware-top-down-approach-for-3d-multi-person-pose-estimation-from-a-single-rgb-image"><a class="markdownIt-Anchor" href="#camera-distance-aware-top-down-approach-for-3d-multi-person-pose-estimation-from-a-single-rgb-image"></a> Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</h2><p><img data-src="image-20220927122328357.png" alt="image-20220927122328357"></p><h3 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h3><ul><li><p>They construct the system based on the top-down approach that consists of DetectNet, RootNet, and PoseNet.</p></li><li><p>They include some intrinsic matrix parameters in their input (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><msqrt><mrow><msub><mi>α</mi><mi>x</mi></msub><msub><mi>α</mi><mi>y</mi></msub><mfrac><msub><mi>A</mi><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow></msub><msub><mi>A</mi><mrow><mi>i</mi><mi>m</mi><mi>g</mi></mrow></msub></mfrac></mrow></msqrt></mrow><annotation encoding="application/x-tex">k=\sqrt{\alpha_x\alpha_y\frac{A_{real}}{A_{img}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.84em;vertical-align:-0.6790645em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1609355000000001em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.894191em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.41586em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5423199999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.1209355em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width="400em" height="1.8800000000000001em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M983 90l0 -0c4,-6.7,10,-10,18,-10 H400000v40H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5c53.7,-170.3,84.5,-266.8,92.5,-289.5zM1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6790645em;"><span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">\alpha s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal">s</span></span></span></span> are from camera intrinsic). <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{real}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is an estimated fixed number (2000mm x 2000mm) to indicate an approximate “human bounding box” size in <strong>real life</strong>. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mi>m</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{img}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> is the predicted bounding box size on the image in pixels.</p></li><li><p>What they do is not directly predicted the actual depth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span>, instead, they predict a correction factor based on the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> mentioned above. In this way, they take the information from the image and intrinsic directly into the pipeline, which eases the model’s job and makes the pipeline more flexible.</p><img data-src="image-20220927131536292.png" alt="image-20220927131536292" style="zoom:50%;"></li><li><p>The PostNet is a standard one, just <code>RestNet+TransConvs-&gt;Heat Maps-&gt;Argmax-&gt;Coordinates</code> pipeline.</p></li><li><p>Even if you input a random image from an unknown hardware (whose intrinsic is unknown), their system can still provide an overlay-able prediction with a given (random) <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>.</p></li><li><p>For both predicted root coordinates by RootNet or the pose coordinates by PoseNet, the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> coordinates are directly in the image plane, with a unit of the pixel.</p></li><li><p>The z coordinates predicted in the PoseNet are relative coordinates to the human center. And the final prediction is this relative z value + predicted <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>r</mi><mi>o</mi><mi>o</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z_{root}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. That’s the reason that even <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is wrong, the predicted 3D coordinates could still align.</p></li></ul><h2 id="single-shot-multi-person-3d-pose-estimation-from-monocular-rgb"><a class="markdownIt-Anchor" href="#single-shot-multi-person-3d-pose-estimation-from-monocular-rgb"></a> Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB</h2><h3 id="points-2"><a class="markdownIt-Anchor" href="#points-2"></a> Points</h3><ul><li>It proposed a new dataset MuPoTs-3D, which is the first large-scale multi-person 3D HPE dataset with occlusion.</li><li>They didn’t use a marker-based MoCap system as that don’t works. So they merely employ purely multi-view marker-less motion capture to create the 20 sequences of MuPoTs3D. Here is the <a href="https://captury.com/">system</a> they used.</li><li>They did data augmentation with background and clothing replacement, as well as rotation and scaling.</li><li>Bottom-Up method. Predicting all the joints at first, then assemble them.</li></ul><h3 id="questions"><a class="markdownIt-Anchor" href="#questions"></a> Questions</h3><ul><li>Not very clear how they do the assembling of joints.</li></ul><h2 id="monocular-3d-multi-person-pose-estimation-by-integrating-top-down-and-bottom-up-networks"><a class="markdownIt-Anchor" href="#monocular-3d-multi-person-pose-estimation-by-integrating-top-down-and-bottom-up-networks"></a> Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks</h2><h3 id="points-3"><a class="markdownIt-Anchor" href="#points-3"></a> Points</h3><ul><li><p>This paper integrates both top-down and bottom-up methods to bypass the inherent disadvantages for both schemes. They do the bbox detection at first, and then predict the relative 3D poses and IDs of joints for each joints in the cropped image, then combine these image pieces to a complete channel. This heat map channel is concatenated to original input to serve as the input of the bottom-up methods. Then the bottom-up branch predict 4 maps:</p><ol><li>A 2D pose map.</li><li>A relative joint depth map.</li><li>A human root depth map.</li><li>A human ID map.</li></ol><p><img data-src="image-20220928124207020.png" alt="image-20220928124207020"></p></li><li><p>Graph Neural Network (GNN) is applied to do pose refinement in the top-down branch. This could help fix the incomplete pose caused by occlusion or partially out-of bbox body parts.</p></li><li><p>Two TCNs are used to estimate both person-centric 3D pose and camera-centric root depth based on a given sequence of 2D poses similar to [6].</p></li></ul><h2 id="graph-based-3d-multi-person-pose-estimation-using-multi-view-images"><a class="markdownIt-Anchor" href="#graph-based-3d-multi-person-pose-estimation-using-multi-view-images"></a> Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images</h2><h3 id="general"><a class="markdownIt-Anchor" href="#general"></a> General</h3><ul><li>Single-view 3D pose estimation approaches:<ol><li>2D pose -&gt; 3D pose.</li><li>Jointly learning 2D and 3D poses.</li><li>Directly regressing 3D poses.</li></ol></li><li>Multi-view 3D pose estimation approaches:<ol><li>2D -&gt; 3D: estimate 2D joints of the same person in each view through monocular pose estimator, then lift the matched 2D single view poses to 3D locations. This 2D to 3D process could be done by triangulation, single-person 3D PSM, or 1x1 Conv</li><li>Direct 3D pose estimation.</li></ol></li></ul><h3 id="points-4"><a class="markdownIt-Anchor" href="#points-4"></a> Points</h3><ul><li>They used the graphs for two parts: estimate the human center coordinates, and the whole body pose.</li><li>They take human structural prior to achieve better performance.</li></ul><h2 id="recent-advances-of-monocular-2d-and-3d-human-pose-estimation-a-deep-learning-perspective"><a class="markdownIt-Anchor" href="#recent-advances-of-monocular-2d-and-3d-human-pose-estimation-a-deep-learning-perspective"></a> Recent Advances of Monocular 2D and 3D Human Pose Estimation: A Deep Learning Perspective</h2><img data-src="image-20220929103125994.png" alt="image-20220929103125994" style="zoom:30%;"><img data-src="image-20220929103235556.png" alt="image-20220929103235556" style="zoom:50%;"><p><img data-src="image-20220929103344466.png" alt="image-20220929103344466"></p><h2 id="openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields"><a class="markdownIt-Anchor" href="#openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields"></a> OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</h2><img data-src="image-20220930122642807.png" alt="image-20220930122642807" style="zoom:50%;"><h3 id="questions-2"><a class="markdownIt-Anchor" href="#questions-2"></a> Questions</h3><ul><li><p>如何把预测的某个关节点的confidence map中含有的多个Local Maxima代表的多个不同人的同一个关节点区分开？</p><ul><li>使用了NMS（Non-Maximum Suppression）算法。直接得到discreet的关节点candidates。</li></ul></li><li><p>得到的这些不同人的关节点如何重新match到一起，形成一副副骨架？</p><ul><li><p>首先，对于已知会有连接的两个joints（如左手手腕和左手手肘），假设它们对应的confidence map中分别得到了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><msub><mi>J</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">N_{J_1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><msub><mi>J</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">N_{J_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>个候补点，那么对于每组可能的组合<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo separator="true">,</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A,B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>（共<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><msub><mi>J</mi><mn>1</mn></msub></msub><mo>∗</mo><msub><mi>N</mi><msub><mi>J</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">N_{J_1}*N_{J_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>个），分别计算一下沿着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mrow><mi>A</mi><mi>B</mi></mrow><mo stretchy="true">→</mo></mover></mrow><annotation encoding="application/x-tex">\overrightarrow{AB}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.20533em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.20533em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span><span class="svg-align" style="top:-3.6833299999999998em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="height:0.522em;min-width:0.888em;"><svg width="400em" height="0.522em" viewbox="0 0 400000 522" preserveaspectratio="xMaxYMin slice"><path d="M0 241v40h399891c-47.3 35.3-84 78-110 128-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67 151.7 139 205zm0 0v40h399900v-40z"/></svg></span></span></span></span></span></span></span></span></span>方向PAF的积分。实际操作上，用的是AB点之间等距取样的几个点的PAF值的和。</p></li><li><p>然后对这<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><msub><mi>J</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">N_{J_1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">J_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>类型点和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><msub><mi>J</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">N_{J_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.93343em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.09618em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">J_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>类型点做一个bipartite matching，简单说就是对于两组点，尝试在这两组点中建立尽可能多的匹配，每个匹配对应一条图上的边，edge的value就是由上一步定义的PAF积分计算得到。任意一个点最多匹配另一组中的一个点。目标是让这些匹配得到的边的值的和最大。这是一个NP-Hard问题，具体匹配由匈牙利算法执行。</p><img data-src="image-20221213233302649.png" alt="image-20221213233302649" style="zoom:80%;"><p>这里的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">E_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>就是所有边值的和，m和n分别是两组关节点组<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>j</mi><mn>1</mn></msub></msub></mrow><annotation encoding="application/x-tex">D_{j_1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.05724em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>j</mi><mn>2</mn></msub></msub></mrow><annotation encoding="application/x-tex">D_{j_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.05724em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>中的点，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">E_{mn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是上面提到的mn两点间的PAF积分值。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mrow><msub><mi>j</mi><mn>1</mn></msub><msub><mi>j</mi><mn>2</mn></msub></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">z_{j_1j_2}^{mn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.059164em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.05724em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.05724em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span>代表分别属于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">J_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">J_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>两组点中的m和n点是否相连。如果是，值为1；反之为0。尝试最大化这个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">E_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p></li></ul></li><li><p>PAF的GT怎么得到的？</p><ul><li><p>给定两个相连的关节点，连线。以这个连线为等分线沿线的垂直方向做一个矩形。</p></li><li><p>在这个矩形内的就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mrow><mi>A</mi><mi>B</mi></mrow><mo stretchy="true">→</mo></mover></mrow><annotation encoding="application/x-tex">\overrightarrow{AB}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.20533em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.20533em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span><span class="svg-align" style="top:-3.6833299999999998em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="height:0.522em;min-width:0.888em;"><svg width="400em" height="0.522em" viewbox="0 0 400000 522" preserveaspectratio="xMaxYMin slice"><path d="M0 241v40h399891c-47.3 35.3-84 78-110 128-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67 151.7 139 205zm0 0v40h399900v-40z"/></svg></span></span></span></span></span></span></span></span></span>单位向量，反之为0。</p></li><li><p>如果图中有多个人，且多个人的同一个部位出现在了同一个点上，则其对应多个单位向量，最终的值是这些单位向量向量和的均值:</p><img data-src="image-20221214000402136.png" alt="image-20221214000402136" style="zoom:50%;"><p>值得注意的是，对于每种肢体（不是关节点），对所有同种肢体只生成一张PAF map。由于某个2D点可能同时在多个人的这个肢体上（Occlusion），所以对于map上某个特定点的PAF值，是在这个点上所有可能在的人的该肢体的单位向量的向量和的均值。</p></li></ul></li><li><p>如果某个部位缺失了怎么办？</p><ul><li>缺失了就缺失了。本文会依次匹配所有PAF中含有的端点对，如果某个部分没了，那就单纯把这部分移除就好了。</li></ul></li></ul><h3 id="points-5"><a class="markdownIt-Anchor" href="#points-5"></a> Points</h3><ul><li><p>PAF是一系列单位向量，它们分布在每个肢体上（两个有联系的joints），方向是肢体的指向。</p></li><li><p>整个模型是顺序的，先predict-refine PAF（前一半蓝色区域的网络），然后再predict-refine关键点（后一半橙色区域的网络）。前半和后半的第一个block是预测，后面全部都是refine阶段。之所以先PAF再KP是因为如果先预测KP可以用它来帮助预测PAF（知道了线端自然可以得到一些关于端点的信息），而如果反过来先预测端点，因为端点只是一个个独立的点，并不知道相互间的连接方法，所以对预测PAF结果并无帮助。</p></li><li><p>由于有的数据集并不会标出来所有的人，所以有时候即使网络预测出来了没有label的人，也会被loss所惩罚。他们的做法是加一个mask，让没有label的点不参与loss计算。</p></li><li><p>每个阶段都会单独计算L2 Loss并加起来。</p></li><li><p>在对各个部位NMS出来的一大堆可能的散点进行匹配的时候，文中做了几个关键的条件放松来加速运算：</p><ul><li><p>本来这是一个全连接图预测问题，即人体中的每个点都应该和其他所有点进行matching。</p></li><li><p>第一个放松点是只根据先验知识采用最少量但最make sense的连接数（如头和脖子，手腕和手肘，脚踝和膝盖等），且仅对这些连接进行计算匹配。在这时其实人体骨骼模型已经成了一个spanning tree skeleton了。值得注意的是，他们最后还是加入了一些冗余匹配对，如耳朵和肩膀、手腕和肩膀等。这些冗余可以保证在拥挤的场景中更不容易出错。</p></li><li><p>第二个放松点是把本来是一个树状的、但连续的匹配问题（如本来应该匹配手腕-手肘-肩膀-脖子-……），但这里将其转换为了这些joints间的两两匹配问题（如手腕-手肘，手肘-肩膀，肩膀-脖子，……）。这是因为相邻关节的树模型已经被PAF充分model了，所以不需要。</p><p><img data-src="image-20221214002603030.png" alt="image-20221214002603030"></p></li></ul></li><li><p>在生成某个关节点的confidence map的GT时，先找出每个人对应的关节点p，然后以p为中心通过高斯模糊扩散开来。在合并多个人这个关节的扩散形成的blob时，若有交叉，取交叉二者/N者的最大值。不用average的原因是average可能会让多个blob变成一个，如图：</p><img data-src="image-20221214003719893.png" alt="image-20221214003719893" style="zoom:50%;"></li><li><p>有趣的是，本文除了做HPE，还可以做车辆关键点检测和脚的关键点检测，非常牛。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;taxonomy&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#taxonomy&quot;&gt;&lt;/a&gt; Taxonomy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;By method: Top-Down; Bottom-Up&lt;/li&gt;
&lt;li&gt;By time dimension: Frame; Time Sequence&lt;/li&gt;
&lt;li&gt;By input type: Monocular; Multi-View&lt;/li&gt;
&lt;li&gt;By human number: Single; Multiple&lt;/li&gt;
&lt;li&gt;By output type: Skeleton; Mesh (SMPL, SCAPE, DensePose)&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="HPE" scheme="https://www.miracleyoo.com/tags/HPE/"/>
    
  </entry>
  
  <entry>
    <title>Point Cloud Clustering 聚类方法论文总结</title>
    <link href="https://www.miracleyoo.com/2023/03/27/point-cloud-clustering-paper/"/>
    <id>https://www.miracleyoo.com/2023/03/27/point-cloud-clustering-paper/</id>
    <published>2023-03-28T00:43:27.000Z</published>
    <updated>2023-04-23T00:44:09.792Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本操作"><a class="markdownIt-Anchor" href="#基本操作"></a> 基本操作</h2><ol><li>降噪</li><li>点云聚类（DBSCAN）</li><li>追踪（Tracking）</li><li>ID（Identification）</li></ol><span id="more"></span><h2 id="real-time-people-tracking-and-identification-from-sparse-mm-wave-radar-point-clouds"><a class="markdownIt-Anchor" href="#real-time-people-tracking-and-identification-from-sparse-mm-wave-radar-point-clouds"></a> Real-time People Tracking and Identification from Sparse mm-Wave Radar Point-clouds</h2><p><img data-src="image-20221216011045607.png" alt="image-20221216011045607"></p><p><img data-src="image-20221216011117452.png" alt="image-20221216011117452"></p><h3 id="question"><a class="markdownIt-Anchor" href="#question"></a> Question</h3><ul><li><p>怎么做的tracking？</p><ul><li><p>使用了converted-measurements Kalman filter (CM-KF) 方法预测目标人物位置、速度、Extension（？）。</p></li><li><p>用了NN-JPDA（nearest-neighbors joint probabilistic data association）算法计算轨迹。</p><blockquote><p>The MTT association between new observations and trajectories is achieved using an approximation of the nearest-neighbors joint probabilistic data association (NN-JPDA) algorithm</p></blockquote></li></ul></li></ul><h3 id="radar-part"><a class="markdownIt-Anchor" href="#radar-part"></a> Radar Part</h3><p><strong>※ 本文含有大量关于使用FMCW雷达采集和处理数据的细节，完全可以用作相关参考。</strong></p><ul><li><p>首先，雷达用的是MIMO雷达（multiple-input multiple-output)。</p></li><li><p>发射器3个，接收器4个，等效为一个发射器12个接收器。</p></li><li><p>发射信号被分布在两个空间走向上，分别是经(Azimuth, AZ)和纬(Elevation, EL)。两个方向上的信号以时分复用的方式交替发射，以取得两个方向上的角度，用以计算Point Cloud中每个点的空间位置。</p></li><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo separator="true">,</mo><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\theta, \phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>分别是一个反射点在 AZ 和 EL 方向上的角度，R是反射点相对雷达的距离。xyz可以只靠<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">,</mo><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">R,\theta,\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>来计算出来。</p><img data-src="image-20221216143959204.png" alt="image-20221216143959204" style="zoom:50%;"></li></ul><h3 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h3><ul><li><p>本文中，并没有针对可变数量的人物进行适配。仅仅支持或3人或8人。所谓的ID只是一个N分类CNN罢了。</p><blockquote><p>Identification: a deep NN classifier is applied to a temporal sequence of K subsequent point-clouds associated with each trajectory, with the objective of discerning among a set of Q pre-defined subject identities.</p></blockquote></li><li><p>聚类还是DBSCAN方法。</p></li></ul><h2 id="human-tracking-and-identification-through-a-millimeter-wave-radar"><a class="markdownIt-Anchor" href="#human-tracking-and-identification-through-a-millimeter-wave-radar"></a> Human tracking and identification through a millimeter wave radar</h2><h3 id="questions"><a class="markdownIt-Anchor" href="#questions"></a> Questions</h3><ul><li><p>遮挡情况怎么办，多个人离得近有重合时候怎么办？</p><ul><li>凉拌，论文避开了这个问题没说。</li></ul></li><li><p>mmWave Radar的具体参数？</p><ul><li>频段为77–81 GHz。</li><li>Bandwidth是 4 GHz。</li><li>The Chirp Cycle Time 𝑇𝑐 是 162.14 μs。</li><li>Frequency Slope 是 70 GHz∕ms。</li><li>Range resolution （距离分辨率）是 4.4 cm。</li><li>Maximum unambiguous range （最大有效工作距离）是 5 m。</li><li>可分辨的最大径向速度是 2 m∕s。</li><li>速度分辨率是 0.26 m∕s。</li><li>128 chirps 每帧。</li><li>每秒33帧。</li></ul></li><li><p>怎么做的human track？</p><ul><li>这个过程是逐帧的。每当有新的人被检出，或是有被检出的人无法与之前帧的任何一个之前记录在案的人匹配成功，则添加一个新的track record</li><li>帧间目标匹配用的是匈牙利算法做的。</li><li>若有某个之前检出并记录过的人物在从某帧开始的连续D帧都没再出现过，则删除这个人。</li><li>最后，用Kalman Filter来预测和纠正轨迹。</li></ul><img data-src="image-20221216095831664.png" alt="image-20221216095831664" style="zoom:50%;"></li><li><p>Detection &amp; Association 步骤中，匈牙利算法匹配已知人物和新一帧中的所有检出人物时，具体的匹配对象是什么（已知人物是一种什么形态被存贮记录的）？</p><ul><li><p>无论是track record中记录的人还是新一帧检测出来的人，其实际representation形式都是在x和y方向上的位置和速度。</p><blockquote><p>For each track we maintain a state which consists of location and velocity along the x and y axes. For each track the initial state consists of the first detection location and velocity.</p></blockquote></li></ul></li><li><p>Kalman Filter的具体作用？</p><ol><li><p>降噪。纠正传感器噪声。</p></li><li><p>当目标物体有几帧缺失时，补全。</p></li><li><p>在本文中Kalman Filter是在track prediction部分使用的。它每次会结合过去对当前帧结果（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>t</mi></msup><mo separator="true">,</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>^</mo></mover><mi>x</mi><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>^</mo></mover><mi>y</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{x}^t,\hat{y}^t,\hat{v}_x^t,\hat{v}_y^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.176664em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span></span></span></span>）的预测和当前的实际CNN Output（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>t</mi></msup><mo separator="true">,</mo><msup><mi>y</mi><mi>t</mi></msup><mo separator="true">,</mo><msubsup><mi>v</mi><mi>x</mi><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mi>y</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">x^t,y^t,v^t_x,v^t_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.176664em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span></span></span></span>）得到一个新的综合了二者的结果（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>x</mi><mo>˚</mo></mover><mi>t</mi></msup><mo separator="true">,</mo><msup><mover accent="true"><mi>y</mi><mo>˚</mo></mover><mi>t</mi></msup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>˚</mo></mover><mi>x</mi><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>˚</mo></mover><mi>y</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathring{x}^{t},\mathring{y}^{t},\mathring{v}^{t}_x,\mathring{v}^{t}_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.176664em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.34722em;"><span class="mord">˚</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.31944em;"><span class="mord">˚</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.34722em;"><span class="mord">˚</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.34722em;"><span class="mord">˚</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7935559999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span></span></span></span>），并以此为基础预测下一帧的结果（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo separator="true">,</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>^</mo></mover><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo separator="true">,</mo><msubsup><mover accent="true"><mi>v</mi><mo>^</mo></mover><mi>y</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\hat{x}^{t+1},\hat{y}^{t+1},\hat{v}^{t+1}_x,\hat{v}^{t+1}_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.197216em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span></span></span></span>）。</p></li><li><p>具体可参考<a href="https://sikasjc.github.io/2018/05/08/kalman_filter/">Kalman Filter 卡尔曼滤波</a>，如下图，橙色是基于之前结果对小车当前相对距离的预测，紫色是对当前距离的观测，二者都有误差且服从正态分布，绿色则是综合了预测和观测的分布。</p><p><img data-src="image-20221216134555511.png" alt="image-20221216134555511"></p></li></ol></li><li><p>User Identification具体怎么做的？</p><ul><li><p>首先让人失望的是，还是softmax+classification-based方法，还是fixed number of human。</p></li><li><p>然后具体的做法是：</p><ol><li>先是正常的sliding windows聚合2s以内的所有点，步长为0.5s。</li><li>对于所有人，先使用一个大小相同的bbox圈起来这个人的所有点。</li><li>然后做Voxel Grid，有点像pooling操作，聚合成一个个空间块。</li><li>Flatten这些voxel grid到一维，输入一个classifier（BiLSTM+MLP）过softmax出每个人的概率。</li></ol><img data-src="image-20221216110618763.png" alt="image-20221216110618763" style="zoom:50%;"></li></ul></li><li><p>mmWave信号是如何转换成3D点云的？</p><ol><li>首先FMCW雷达先记录所有的反射。</li><li>然后通过range-FFT之后移除clutter。</li><li>最后在估算完速度和角度之后生成point cloud。</li></ol></li><li><p>怎么做到的区分未训练的闯入者和每个训练对象的？</p><ul><li>在softmax loss分类，还加了一个center loss，旨在让来自同一个人的samples的intra-class distance最小，拥有相似的中心点，聚集在一起。</li><li>一旦来了新的人，他的sample feature中心点会远离前面的所有人，就会被区分开。</li><li>值得注意的是，这个所谓的相似的中心点并不是根据同一个人的features自然聚合训练出来的，而是通过对每个人类别label过一个embedding得到的。这个embedding会和每个该类的人的输出feature map做loss。</li><li>具体说道怎么根据这个embedding区分，那就是对每个人物bbox内得到的feature map，分别和每个已知人的bbox逐点算距离，距离太大就算闯入者，反之如果有一个还算小，那就是这类了。</li></ul></li></ul><h3 id="points-2"><a class="markdownIt-Anchor" href="#points-2"></a> Points</h3><ul><li><p>用的VICON Mocap System。</p></li><li><p>他们在论文中指出mmWave雷达很能穿，泡沫、塑料、木材、甚至铝对点云密度造成的影响都小于1%。</p><ul><li>用的材料是统一的3mm厚<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>5</mn></msup><mi>m</mi><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">10^5mm^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span><span class="mord mathnormal">m</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>大小的板子。约为1.5张A4纸大。</li><li>问题是，他们直接把材料怼到雷达脸上测的（1cm away from the sensor），为了防止radar signal transmitted in the line-of-sight condition。算是又避开了一个重要问题。估计在可视范围内放obstacles会出问题。</li><li>这种程度的穿透其实只在sensor有用，比如可以装配到屏幕、家具下面，而对应用端意义有限，即没法真的在日常空间中穿透物体。</li></ul></li><li><p>Clustering用的是DBScan，参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.05</mn><mo separator="true">,</mo><mi>α</mi><mo>=</mo><mn>0.25</mn><mo separator="true">,</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi>P</mi><mi>t</mi><mi>s</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.05, \alpha=0.25, MinPts=20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span></span></span></span></p><blockquote><p>DBScan has two parameters, namely Eps which indicates the maximum distance of two points in the same cluster and MinPts which indicates the minimum point number in a cluster to cope with noise points. In practice, we choose 0.05 as Eps and 20 as MinPts. 𝛼 was set to 0.25 in the customized distance function.</p></blockquote></li></ul><h3 id="about-fmcw-radar"><a class="markdownIt-Anchor" href="#about-fmcw-radar"></a> About FMCW Radar</h3><ul><li><p>步骤：</p><ol><li>range-FFT: 用来算目标物体距离。</li><li>Clutter Removal: 去掉静态物体。具体做法是对于每个range bin，对于每个天线，减去其均值。理由没看懂。(?)</li></ol><blockquote><p>As we are interested in identifying people moving in the scene, the background, corresponding to stationary objects, needs to be removed before performing Doppler FFT. This is performed by subtracting a mean for each range bin per antenna across the chirps in a frame. With this step in the processing pipeline, the millimeter wave radar is able to generate a point cloud which does not contain static obstacles. However, this does not guarantee that the point cloud does not contain noise. While the users move in the scene, parts of the background objects are occluded and the reflections from these areas changes over time, leading to noise in the radar point cloud.</p></blockquote><ol start="3"><li><p>Doppler-FFT: 用来算目标物体速度。这一步前必须先把静态物体移除到位。</p></li><li><p>Angle Estimation：用天线组合来算目标物体相对雷达角度。</p></li></ol></li><li><p>很好奇究竟是如何从距离速度和角度还原出来如此高精度的3D空间坐标信息的。(?)</p></li></ul><h3 id="improvable-points"><a class="markdownIt-Anchor" href="#improvable-points"></a> Improvable Points</h3><ul><li>首先，这是针对特定人群的训练。无法泛化，训练集里面有谁谁才能用。</li><li>其次，他们算center loss时候只考虑了类内区别最小，没考虑让类间不同最大。</li><li>没有考虑人们距离太近时候的点云重合混杂问题，避而不谈。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;基本操作&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#基本操作&quot;&gt;&lt;/a&gt; 基本操作&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;降噪&lt;/li&gt;
&lt;li&gt;点云聚类（DBSCAN）&lt;/li&gt;
&lt;li&gt;追踪（Tracking）&lt;/li&gt;
&lt;li&gt;ID（Identification）&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="clustering" scheme="https://www.miracleyoo.com/tags/clustering/"/>
    
    <category term="point-cloud" scheme="https://www.miracleyoo.com/tags/point-cloud/"/>
    
  </entry>
  
  <entry>
    <title>mmWave Radar Fusion 论文总结</title>
    <link href="https://www.miracleyoo.com/2023/03/09/mmwave-radar-fusion-paper/"/>
    <id>https://www.miracleyoo.com/2023/03/09/mmwave-radar-fusion-paper/</id>
    <published>2023-03-10T02:32:23.000Z</published>
    <updated>2023-04-23T01:32:52.873Z</updated>
    
    <content type="html"><![CDATA[<h2 id="immfusion-robust-mmwave-rgb-fusion-for-3d-human-body-reconstruction-in-all-weather-conditions"><a class="markdownIt-Anchor" href="#immfusion-robust-mmwave-rgb-fusion-for-3d-human-body-reconstruction-in-all-weather-conditions"></a> ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions</h2><ul><li>2022, Arxiv</li><li>Question: How to merge mmWave radar with RGB frames to do 3D human mesh reconstruction?</li><li>Spec: Single person, 3D mesh, RGB + mmWave Radar.</li><li>Features: Robust in extreme weather/conditions like rain, smoke, low light, and occlusion.</li></ul><span id="more"></span><h3 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h3><ul><li><p>Merging scheme: Using three branches: Image branch, radar point cloud branch, and a fusion branch. All three branches are concatenated and sent to a transformer. A human template is also concatenated as “positional encoding” (actually it’s more like prior knowledge encoding.)</p><p><img data-src="image-20230329171848182-1682213515149-69.png" alt="image-20230329171848182"></p></li><li><p>Previous fusion methods:</p><ol><li>Point-level fusion method: Concatenate image features or projected RGB pixels to the point clouds as extended features of the point-based model.<ul><li>This fusion strategy is not suitable for mmWave-RGB fusion due to the sparsity and noise of radar points.</li><li>Undesirable issues like <strong>randomly missing</strong> and <strong>temporally flicking</strong> would lead to fetching fewer or even wrong image features.</li></ul></li><li>DeepFusion: Treat image features as K and V, answering Qs from mmWave point cloud features.</li><li>TokenFusion: Do feature extraction and go through the transformer separately for both image and radar features. Fusion happens in the last stage.</li></ol></li><li><p>They perform positional encoding by attaching the 3D coordinates of each joint and vertex in a <strong>human template</strong> mesh to the global vector.</p></li><li><p>Raw radar side input has a shape of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1024</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">1024\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>, which means 1024 mmWave radar point cloud in the cropped body region. Here it’s not clear how can this dimension keeps the same among all samples, but there should be a sampling mechanism to always sample 1024 points exactly.</p></li><li><p>Image feature part, they use HRNet to extract the feature of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span></span></span></span> cropped body region.</p></li><li><p>They use <code>PointNet++</code> to process raw radar point clouds, and the length of the resulting feature map (<code>L</code>) represents the number of seed points sampled by the Farthest Point Sample (FPS).</p></li><li><p>Results:</p><p><img data-src="image-20230329183402590-1682213515151-78.png" alt="image-20230329183402590"></p></li></ul><h2 id="mmwave-radar-and-vision-fusion-for-object-detection-in-autonomous-driving-a-review"><a class="markdownIt-Anchor" href="#mmwave-radar-and-vision-fusion-for-object-detection-in-autonomous-driving-a-review"></a> MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review</h2><ul><li>2022, Sensors (MDPI)</li><li>This paper discussed various fusion methods used by previous mmWave + Vision sensor papers.</li></ul><h3 id="fusion-methods"><a class="markdownIt-Anchor" href="#fusion-methods"></a> Fusion Methods</h3><img data-src="image-20230329190214285-1682213515150-70.png" alt="image-20230329190214285" style="zoom:50%;"><ol><li><p>Data Level</p><ul><li>Mature, but not the mainstream method.</li><li>Basically, using radar to detect the ROI, and crop the visual frame accordingly.</li><li>The size of the initial ROI is determined by the distance between the obstacle and mmWave radar.</li><li>Radar’s information loss is significant.</li></ul><img data-src="image-20230329193957860-1682213515150-72.png" alt="image-20230329193957860" style="zoom:50%;"></li><li><p>Decision Level</p><ul><li>This is the mainstream fusion scheme at present.</li><li>The basic guideline is to process radar and vision data separately in parallel, let them do the final prediction also separately, and only fuse the predicted results.</li><li>Radar detection results generate a list of objects and contain information such as the distance, azimuth angle, and relative velocity of the detected objects.</li><li>The fusion method can be divided into Bayesian theory-based, Kalman Filter-based, Dempster Shafer Theory-based, and Radar Validation-based.</li></ul><img data-src="image-20230329200724366-1682213515150-71.png" alt="image-20230329200724366" style="zoom:50%;"></li><li><p>Feature Level</p><ul><li>This is a relatively new strategy.</li><li>The core idea is to extract the feature from both sensors, fuse them, and do the prediction.</li><li>The fusion methods are usually concatenation, point-wise addition, or spatial attention fusion.</li><li>Note that the goal of fusion here is to compile an RGB image-like feature map, and the object detection module here can be any traditional CV object detection algorithm.</li><li>Radar feature extraction mostly adopts the method of converting radar points to the image plane to generate a radar image. The purpose of radar feature extraction is to transform radar information into imagelike matrix information. Each radar-generated feature map’s channel represents a physical quantity such as distance, longitudinal speed, lateral speed, and so on.</li></ul><img data-src="image-20230329201007298-1682213515150-73.png" alt="image-20230329201007298" style="zoom:50%;"></li></ol><h3 id="future-trend"><a class="markdownIt-Anchor" href="#future-trend"></a> Future Trend</h3><ul><li>3D Object detection. The existing Radar-Vision fusion works are mainly 2D object detection, and 3D detection results are far worse.</li><li>Integrate new sensors.</li><li>Better ways of sensing information fusion (multi-modal fusion).<ul><li>Better ways to deal with sparseness brought by radar.</li><li>More efficient multi-sensor fusing methods.</li></ul></li></ul><h3 id="calibration"><a class="markdownIt-Anchor" href="#calibration"></a> Calibration</h3><ul><li>Coordinate transformation method: Radar and camera are placed in the same coordinate. Using the camera and radar’s absolute coordinates, perform a linear transformation to calibrate.</li><li>Sensor verification method: Radar proposes a target list at first, and then verifies and matches using vision information.</li><li>Vision-based method: Propose candidate areas for moving targets using the camera, and match radar results to it.</li></ul><h2 id="spatial-attention-fusion-for-obstacle-detection-using-mmwave-radar-and-vision-sensor"><a class="markdownIt-Anchor" href="#spatial-attention-fusion-for-obstacle-detection-using-mmwave-radar-and-vision-sensor"></a> Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor</h2><ul><li>2020, Sensors (MDPI)</li><li>Core Contribution: Proposed a novel attention-based radar-vision fusion mechanism to do obstacle detection.</li><li>Taxonomy: Feature level fusion. (Attached is another taxonomy.)</li></ul><img data-src="image-20230330151749489-1682213515150-74.png" alt="image-20230330151749489" style="zoom:50%;"><h3 id="points-2"><a class="markdownIt-Anchor" href="#points-2"></a> Points</h3><ul><li><p>Network structure: The feature extractor part is not anything new, it is a combination of modified ResNet and RetinaNet. The fusion mechanism is the part where it has the edge on.</p><img data-src="image-20230330151909029-1682213515150-76.png" alt="image-20230330151909029" style="zoom:50%;"></li><li><p>The core mechanism: SAF, aims to predict an attention map generated by radar feature, and point-wise multiply this attention map by the feature extracted from vision features.</p></li></ul><img data-src="image-20230330151942545-1682213515150-77.png" alt="image-20230330151942545" style="zoom:50%;"><ul><li><p>The “radar image” they used is transformed from radar points, which is shown as follows:</p><ul><li>They calculate the extrinsic matrix of the radar and front camera separately, transforming the radar point cloud into camera coordinates.</li><li>They calculate the pixel value of the radar image using depth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>, longitudinal velocity <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">v_x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and lateral velocity <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">v_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>. The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> here is a linear transformation for these three variables.</li><li>Lastly, since each pixel covers a too-small area of the image, they render a solid circle around each radar point. The value within the circle is the same, and the circle’s radius is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span>.</li><li>If there there are two radar points whose distance is less than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>r</mi></mrow><annotation encoding="application/x-tex">2r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span>, in the intersection area, use the value of the nearer one (the one with a smaller <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>).</li></ul><img data-src="image-20230330174231545-1682213515150-75.png" alt="image-20230330174231545" style="zoom:50%;"></li></ul><h2 id="tools"><a class="markdownIt-Anchor" href="#tools"></a> Tools</h2><ul><li><a href="https://github.com/radar-lab/ti_mmwave_rospkg">ROsPKG</a>: Turn raw radar data to point clouds.</li><li><a href="https://github.com/nesl/RadHAR">RadHAR</a>: Point Cloud Data collection and pre-processing (<a href="https://github.com/nesl/RadHAR/blob/master/DataPreprocessing/voxels.py">Voxel Generation</a>) examples can be found here. This is from a paper: <a href="https://dl.acm.org/citation.cfm?id=3356768"><em>RadHAR: Human Activity Recognition from Point Clouds Generated through a Millimeter-wave Radar</em></a></li><li>Point Cloud-related questions on TI: <a href="https://e2e.ti.com/support/sensors-group/sensors/f/sensors-forum/1049628/dca1000evm-point-cloud-data">Link</a> (They used IWR 1843 BOOST &amp; DCA 1000 EVM)</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;immfusion-robust-mmwave-rgb-fusion-for-3d-human-body-reconstruction-in-all-weather-conditions&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#immfusion-robust-mmwave-rgb-fusion-for-3d-human-body-reconstruction-in-all-weather-conditions&quot;&gt;&lt;/a&gt; ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2022, Arxiv&lt;/li&gt;
&lt;li&gt;Question: How to merge mmWave radar with RGB frames to do 3D human mesh reconstruction?&lt;/li&gt;
&lt;li&gt;Spec: Single person, 3D mesh, RGB + mmWave Radar.&lt;/li&gt;
&lt;li&gt;Features: Robust in extreme weather/conditions like rain, smoke, low light, and occlusion.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="mmwave-radar" scheme="https://www.miracleyoo.com/tags/mmwave-radar/"/>
    
    <category term="domain-fusion" scheme="https://www.miracleyoo.com/tags/domain-fusion/"/>
    
  </entry>
  
  <entry>
    <title>GNN-based Human Pose Estimation (HPE) 论文总结</title>
    <link href="https://www.miracleyoo.com/2023/02/19/gnn-hpe/"/>
    <id>https://www.miracleyoo.com/2023/02/19/gnn-hpe/</id>
    <published>2023-02-20T01:34:18.000Z</published>
    <updated>2023-04-23T00:44:00.009Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation"><a class="markdownIt-Anchor" href="#dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation"></a> DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation</h2><img data-src="image-20221219151203243.png" alt="image-20221219151203243" style="zoom:50%;"><img data-src="image-20221219154249782.png" alt="image-20221219154249782" style="zoom:50%;"><h3 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h3><ul><li>Multi-person.</li><li>Image-based. Graph is just used in their DGCM module.</li><li>Bottom-Up.</li></ul><span id="more"></span><ul><li><p>Basic Hypothesis:</p><blockquote><p>Existing bottom-up methods mainly define relations by empirically picking out edges from this graph, while omitting edges that may contain useful semantic relations.</p></blockquote><p>But actually OpenPose is using the similar idea: “Redundant PAF connections”. The difference is that OpenPose redundant connections are still empirical, while in this paper, they take into account all possible connections between two arbitrary joints, instead of limited amount.</p></li><li><p>Core idea: 本文是一篇基于RGB图片的Bottom-up 2D HPE文章。文中Graph是作为其核心模块DGCM的一部分出现的，且并没有使用GNN。这里的Graph是用来建模joints之间关系的，即摆脱了传统的基于经验的骨骼链接，也不是单纯的基于dataset计算的软性链接，而是将软性链接的值当做伯努利分布中的概率来用，每次随机筛出来几个可能的邻接矩阵来用。本文中所谓Graph的使用其实完全可以被transformer代替，本质上是在建模关节点之间的关联性。</p></li><li><p>Graph:</p><ul><li>Node: Human Joints。</li><li>Node Value: 无。</li><li>Edge: Joints间的全连接。</li><li>Edge Value: 两个joints之间的相关性。</li><li>Output: 一个joints间的邻接矩阵。</li><li>Usage: 用于生成带有随机性的、基于soft adjacency matrix的人类关节点关联矩阵。</li></ul></li></ul><h3 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h3><ul><li><blockquote><p>Bottom-up pose estimation methods try to learn two kinds of heatmaps from the deep neural network, including keypoint heatmaps and relation heatmaps.</p></blockquote></li><li><p>Soft Adjacency Matrix:</p><ul><li>基于全部训练集中任意两个joints之间的距离（normalized by scale factor s）的倒数构建软邻接矩阵。</li><li>由于任意一个点到期自身的距离是0，相应的邻接矩阵值设为1。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>A</mi><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>γ</mi><mfrac><mn>1</mn><msubsup><mi>M</mi><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mfrac><mo stretchy="false">)</mo><mo separator="true">,</mo><msubsup><mi>A</mi><mi>s</mi><mrow><mi>i</mi><mi>i</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">A_{s}^{ij}=\sigma(\gamma\frac{1}{M_d^{ij}}), A_s^{ii}=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.071664em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.5665079999999998em;vertical-align:-0.7213999999999998em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.52166em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9547714285714286em;"><span style="top:-2.1527714285714286em;margin-left:-0.10903em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-2.9836857142857145em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3472285714285714em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7213999999999998em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>。</li></ul></li><li><p>Dynamic Adjacency Matrix：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>A</mi><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup><mo>∼</mo><mi>B</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msubsup><mi>A</mi><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A_d^{ij}\sim B(x,A_s^{ij})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2438799999999999em;vertical-align:-0.3013079999999999em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.942572em;"><span style="top:-2.3986920000000005em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013079999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0746639999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，x是数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>A</mi><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">A_s^{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.071664em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是概率，B是伯努利分布。</li><li>这里的伯努利分布（01分布）实质上就是把软邻接矩阵的值变成了概率。A^(i,j)值越大，概率越大，取1的可能性越大。就这样筛选几次，得到几个可能的邻接矩阵。</li></ul></li><li><p>文中使用了金字塔式多尺度feature map，理由是人的大小远近不一样，多尺度的feature更有适应性。</p></li><li><p>由于邻接矩阵里面每个joint和它自己的relation-term永远是1，就相当于加了一个skip connection一样，不用担心每个joint只被其邻居决定。</p></li><li><p>除了这里提到的动态邻接矩阵，还有一个learnable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>×</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">K\times K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>的weights，共同起作用。</p></li></ul><h2 id="learning-dynamics-via-graph-neural-networks-for-human-pose-estimation-and-tracking"><a class="markdownIt-Anchor" href="#learning-dynamics-via-graph-neural-networks-for-human-pose-estimation-and-tracking"></a> Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking</h2><img data-src="image-20221219160516007.png" alt="image-20221219160516007" style="zoom:50%;"><h3 id="summary-2"><a class="markdownIt-Anchor" href="#summary-2"></a> Summary</h3><ul><li><p>Multi-person 2D HPE.</p></li><li><p>Top-Down.</p></li><li><p>Frame-based.</p></li><li><p>Pipeline:</p><ul><li>Current Frame 2D HRNet Branch: Crop Human -&gt; Rescale -&gt; HRNet -&gt; HeatMap-F -&gt; Argmax -&gt; Joints-F</li><li>Historical tracklets GNN Branch: Build Joints based on (Tracklets’ (0~t-1) joints’ + current frames’ all pixels’)(Visual Features, Joint Locations, Joint Type) -&gt; Connect edges in time and space -&gt; GNN -&gt; HeatMap-G -&gt; Joints-G</li><li>Merging: Hungarian(Joints-F, Joints-G) -&gt; If no matching, new ID, output Joints-G; If matched, output argmax(HeatMap-F, HeatMap-G)</li></ul></li><li><p>本文是一个Tow-Down的、基于帧序列（视频）的、2D Multi-Human HPE+Human Tracking的工作。它先用传统的剪裁预测方式预测每帧的所有可能人的可能joints，然后再将每帧每人的每个joint都通过MLP转换为一个个node的feature map，连接这些node，过一个GNN，最后得到最终预测。</p></li><li><p>Graph：</p><ul><li>Node: 已经被追踪的人的<strong>所有历史joints</strong>(FIFO内的)及当前Frame的<strong>所有像素</strong>。</li><li>Node Value: 一个基于HRNet输出feature、2D相对位置、joint类型softmax的综合feature map，综合方式是分别过MLP后average。</li><li>Edge: 帧内和帧间分别全连接。</li><li>Edge Value: 无。</li><li>Output: 对于每个此前追踪过的人，输出对其每个关节点在当前帧位置的预测。由于当前帧全图像素都是nodes，所以对于每种joint type，其实输出的相当于一个HeatMap。</li><li>Usage: 通过分析过往最终过的人的历史joints location，以及当前帧的visual feature，预测这些人的每个joint在当前帧的位置。</li></ul></li></ul><h3 id="question"><a class="markdownIt-Anchor" href="#question"></a> Question</h3><ul><li><p>怎么做的tracking？</p><ul><li>又是匈牙利算法。GNN基于tracklets预测的结果和基于当前帧HRNet预测出来的结果进行matching。其中使用的相似度是基于关键点的位置计算出来的。</li></ul></li><li><p>GNN 基于tracklets历史预测的poses和当前帧通过HRNet预测的Pose是如何结合（Aggregate and Merge）的？</p><ul><li>先匹配，匹配上的就对heatmap做平均然后argmax。</li><li>没匹配上的就给一个新ID，直接argmax。</li><li>对匹配上的人，如果FIFO已经满了，就踢一个加新的；反之直接加了。</li></ul><blockquote><p>For all the matched poses, the joint heatmaps of the two poses are first aligned according to their centers and then merged together by averaging the heatmaps.</p></blockquote></li><li><p>GNN的edge怎么定义的？</p><ul><li>Edge有两种，一种是同一帧中joints之间的链接，一种是每个joint与上一帧中所有joints的联系（包括当前帧的nodes）。</li><li>没有具体说edge的值（attr），应该是没有用。</li></ul></li></ul><h3 id="points-2"><a class="markdownIt-Anchor" href="#points-2"></a> Points</h3><ul><li><p>每个tracklet都是一个unique的被记录在案的人。</p></li><li><p>对于每个tracklet，都会分别被过一遍一个GNN进行预测。注意，GNN这里也可以理解做是Top-Down的，对于每个历史上记录在案的人，都分别过一遍GNN预测在当前帧这个人每个关节点的位置。</p></li><li><p>GNN的nodes包含某个历史tracklets（追踪的关键点）中的所有joints以及当前帧的所有像素点。</p></li><li><p>当前帧*(t)<em>的每个像素组成的node都与tracklets FIFO中最后一帧</em>(t-1)*的检测结果中的每个joint相连。</p><img data-src="image-20221220121801045.png" alt="image-20221220121801045" style="zoom:50%;"></li><li><p>之所以要对Visual Features (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">v_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>), Joint Locations(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">p_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>), Joint Type(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">c_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)向量分别做MLP，是因为它们的维度不同，且node的channel不能有多维。</p></li><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mi>k</mi></msub><mo>=</mo><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mi>M</mi><mi>L</mi><msub><mi>P</mi><mrow><mi>v</mi><mi>i</mi><mi>s</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>v</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>M</mi><mi>L</mi><msub><mi>P</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>M</mi><mi>L</mi><msub><mi>P</mi><mrow><mi>t</mi><mi>y</mi><mi>p</mi><mi>e</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>c</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J_k=Pooling(MLP_{vis}(v_k), MLP_{pos}(p_k), MLP_{type}(c_k))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> 为每个node的值。这里的pooling是average pooling。</p></li><li><p>对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">p_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>而言，全部帧的这个location都以FIFO中最后一帧*(t-1)*中人物中心点为基准进行normalization，当前帧这些nodes也不例外。</p></li><li><p>注意，当前帧*(t)*全像素组成的这些nodes里面，在算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">J_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时候不加<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">c_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>分支，即不用joint type，因为joint type是要predict的东西。</p></li><li><p>最终预测出来的东西是一个Prob，它包含了当前帧t所有像素点可能为某个joint type的概率（classification）。</p></li><li><p>对于每个tracklets，都有一个FIFO队列保存K个过去位置。</p></li></ul><h3 id="comments"><a class="markdownIt-Anchor" href="#comments"></a> Comments</h3><ul><li>非常新颖的使用GNN的方法。用多个方面的features分别过MLP通过average pooling的方法作为node的值，既可以确保这些feature的维度不match不成问题，也可以有效降低channel number，降低cost。最重要的是，对于某些node，你甚至可以移除某些features而不影响整体维度，比如对当前frame像素nodes不添加不存在的类别features。</li><li>同时也非常暴力，直接在帧内、帧间分别用全连接，且当前frame直接把全部像素点位置都变成node，这是一个巨大的开销，并不优雅，也不符合GNN的内含逻辑。</li><li>位置norm方法值得学习。</li><li>我们的项目可能也可以用上匈牙利匹配，但是匹配的是时间上的cluster，而匹配的变量可能是cluster的平均方向向量等。</li></ul><h2 id="context-modeling-in-3d-human-pose-estimation-a-unified-perspective"><a class="markdownIt-Anchor" href="#context-modeling-in-3d-human-pose-estimation-a-unified-perspective"></a> Context Modeling in 3D Human Pose Estimation: A Unified Perspective</h2><p><img data-src="image-20221221105021381.png" alt="image-20221221105021381"></p><h3 id="summary-3"><a class="markdownIt-Anchor" href="#summary-3"></a> Summary</h3><ul><li>Single image-based.</li><li>2D -&gt; 3D lifting.</li><li>Top-Down.</li><li>核心操作是一套Attention。首先他们预测出2D Pose，然后将这些Pose投影到3D空间，再用一套Encoder-Decoder网络来预测3D Heatmap。具体来说，每个voxel预测J个值，代表它是某个joint的可能性。而其中关键的Attention部分分为两部分，全局Attention和关节对Attention。前者预测某个voxel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> 含某个joint <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span>的概率，后者预测当另一个voxel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 包含joint <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 间的关联性。这个关联性来自于训练集上的prior，主要成分是所有物理（经验）肢体连接的距离平均值和标准差，这个关联性高时说明 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 在当前voxel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo separator="true">,</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">q, k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 中时大概率符合prior中的预测，反之说明这对joints连接的肢体可能过长/短了，所以当前分布可能性很小。</li><li>Graph:<ul><li>Node: 所有的voxels。</li><li>Node Value: 来自于2D坐标投影的features。</li><li>Edge: Voxel间的连接。</li><li>Edge Value: Voxel间的关联性（Pairiwise-Attention）。</li><li>Output: 每个3D voxel包含某个joint的概率。</li><li>Usage: 用于从2D预测升维到3D。</li><li>Note: 这里所谓的GNN其实就是ContextPose (Attentions)的一种特例。</li></ul></li></ul><h3 id="points-3"><a class="markdownIt-Anchor" href="#points-3"></a> Points</h3><ul><li>Pairwise-Attention（关节对Attention）的公式是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><msub><mi>e</mi><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow></msub><mo stretchy="false">)</mo><mo>∝</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−</mo><mfrac><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo>−</mo><mi>k</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mn>2</mn></msub><mo>−</mo><msub><mi>μ</mi><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>α</mi><msubsup><mi>σ</mi><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(q,k,e_{u,v}) \propto exp(-\frac{(||q-k||_2-\mu_{u,v})^2}{2\alpha\sigma^2_{u,v}+\epsilon})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.77366em;vertical-align:-0.64242em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.13124em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.214em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.42488571428571426em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.50732em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">∣</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mtight">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285716em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span><span class="mclose mtight"><span class="mclose mtight">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.64242em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></li><li>PA被normalized了，具体公式是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo>∈</mo><mi mathvariant="normal">Ω</mi></mrow></msub><msub><mi>G</mi><mi>v</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>v</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><msub><mi>e</mi><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\Sigma_{k\in\Omega}G_v(x_{v,k}) \cdot P(q,k,e_{u,v})=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999985em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">∈</span><span class="mord mtight">Ω</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>, 意味着如果voxel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> 包含关节 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span></span></span></span>，那么所有可能包含关节 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 的voxel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 它本身的概率乘以它满足 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span> 两个关节点物理距离prior的概率和为1。</li></ul><h2 id="optimizing-network-structure-for-3d-human-pose-estimation"><a class="markdownIt-Anchor" href="#optimizing-network-structure-for-3d-human-pose-estimation"></a> Optimizing Network Structure for 3D Human Pose Estimation</h2><p><img data-src="image-20221221122107913.png" alt="image-20221221122107913"></p><img data-src="image-20221221122050872.png" alt="image-20221221122050872" style="zoom:50%;"><h3 id="summary-4"><a class="markdownIt-Anchor" href="#summary-4"></a> Summary</h3><ul><li><p>本文提出了一种统一模型，它可以将2D-&gt;3D Human Pose Lifting的诸多方法（FCN，GNN，LCN）写入一个统一公式中，并取得更优的效果。</p></li><li><p>相比于普通GCN，其特点在于：</p><ul><li>普通GCN对于所有的node，做Linear的时候都用的共享的weights。</li><li>而在LCN中，计算每个输入node对输出node的贡献时所用的weights，都分别训练，不共享参数。</li></ul><img data-src="image-20221221135856400.png" alt="image-20221221135856400" style="zoom:50%;"></li><li><p>Graph:</p><ul><li>Node: J个人类Joints。</li><li>Node Value: 2D坐标的features。</li><li>Edge: Joints间的连接。</li><li>Edge Value: Joints间的关联性（邻接矩阵）。</li><li>Output: 每个Joint的3D坐标。</li><li>Usage: 用于从2D预测升维到3D。</li></ul></li></ul><h2 id="adaptive-hypergraph-neural-network-for-multi-person-pose-estimation"><a class="markdownIt-Anchor" href="#adaptive-hypergraph-neural-network-for-multi-person-pose-estimation"></a> Adaptive Hypergraph Neural Network for Multi-Person Pose Estimation</h2><p><img data-src="image-20221220191107517.png" alt="image-20221220191107517"></p><h3 id="summary-5"><a class="markdownIt-Anchor" href="#summary-5"></a> Summary</h3><ul><li><p>Image-based.</p></li><li><p>Multi-human 2D HPE.</p></li><li><p>Top-Down.</p></li><li><p>文中提出了一种“<strong>超边</strong>”(Hyperedge)，这种边不是物理的边，也不是非物理但连接两个顶点的边，而是连接了多个顶点、有着类似<strong>区分body part</strong>作用的“大边”。这种超边的分配由训练好的网络负责，生成的依据是图中的语义关系。比如如果人在拉伸右腿，那右手和右脚踝就会被分配到一个超边中，因为它们有着紧密的语义联系。</p><img data-src="image-20221220191125713.png" alt="image-20221220191125713" style="zoom:50%;"></li><li><p>本文做的是Top-Down的单张图片2D多人HPE。其核心贡献在于提出了“超边”这种由图片语义得来、可连接两个到多个joints的广义边，从而可以更好地应对一些非典型动作的图片。</p></li><li><p>Graph:</p><ul><li>Node: Human Joints.</li><li>Node Value: d dimension feature.</li><li>Edge: m条，为hyperedges。</li><li>Edge Value: 每条超边链接的joints以及链接的紧密程度。</li><li>Output: 一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">J\times m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">m</span></span></span></span>的矩阵，m是超边的数量，J是关键点的数量。代表着联系的紧密程度。</li><li>Usage: 根据图片信息更好的建立人类关键点间的高维语义联系。</li></ul></li></ul><h3 id="questions"><a class="markdownIt-Anchor" href="#questions"></a> Questions</h3><ul><li>Adaptive hypergraph矩阵可以包含可变数量的边。这个是怎么处理的？</li><li>这个矩阵是如何训练的？输入是什么？</li></ul><h3 id="points-4"><a class="markdownIt-Anchor" href="#points-4"></a> Points</h3><ul><li>对于一层GCN而言，它的forward公式是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mover accent="true"><mi>A</mi><mo>~</mo></mover><msup><mi>H</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><msup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>W</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H^{(l+1)}=\sigma (\tilde{A}H^{(l)}W^{(W+1)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879999999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1701899999999998em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201899999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span></span></span><span style="top:-3.6023300000000003em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，这里的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>代表激活函数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>A</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9201899999999998em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201899999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span></span></span><span style="top:-3.6023300000000003em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span>代表被normalized邻接矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>是这层GCN的parameter。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>是feature map，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>这里代表第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>层GCN。正常情况下，这个步骤整体都被打包进<code>pyg.GCN</code> module一起做了，但这里我们刚好有现成的邻接矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span>，就不用专门再去构造graph，弄一个专门的<code>edge_index</code>矩阵了，因为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span>就是了。</li></ul><h2 id="learning-skeletal-graph-neural-networks-for-hard-3d-pose-estimation"><a class="markdownIt-Anchor" href="#learning-skeletal-graph-neural-networks-for-hard-3d-pose-estimation"></a> Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation</h2><p><img data-src="image-20221221141417256.png" alt="image-20221221141417256"></p><h3 id="summary-6"><a class="markdownIt-Anchor" href="#summary-6"></a> Summary</h3><ul><li><p>本文提出了一种针对不同距离（hop）的nodes使用不同Aggregation层级的GCN，保证了不同距离的node得以被区分对待，降低远距离node带来的噪音。同时，它还结合了经验人类skeleton和动态可学习skeleton的优点，用同步的两个branch分别运算，进一步提升了基于video的2D-&gt;3D HPE Lifting的质量。</p></li><li><p>Graph:</p><ul><li><p>Node: J个人类Joints。</p></li><li><p>Node Value: 2D坐标的features。</p></li><li><p>Edge: Joints间的连接。分为两部分，一部分是物理连接，一部分是动态可学习的连接。</p></li><li><p>Edge Value: Joints间的关联性（邻接矩阵）。</p></li><li><p>Output: 每个Joint的3D坐标。</p></li><li><p>Usage: 用于从2D预测升维到3D。</p><blockquote><p>Given 2D keypoints <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">X ∈ R^{N\times 2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>, with N nodes, the model outputs better 3D positions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo>×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">Y ∈ R^{N\times 3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>.</p></blockquote></li></ul></li></ul><h3 id="points-5"><a class="markdownIt-Anchor" href="#points-5"></a> Points</h3><ul><li><p>决定文中设计思路的几个重要观察：</p><ol><li>Joints Graph中距离遥远的node虽然有时也会提供有价值的信息，但也导入了许多不相干的噪音。</li><li>动态Joints Graph的构建很有效，但在不同的动作中joints间的关联性变化很大。所以虽然动态图在表征不同的动作时候很直观，如果只给定单张图，它也很容易受到outlier的影响。所以文中引入了TCN让信息在时间轴上流动。</li></ol></li><li><p>实际设计上，基本是一层D-HCSF一层TCN这样串联。</p><img data-src="image-20221221150901507.png" alt="image-20221221150901507" style="zoom:50%;"></li><li><p>Zoom in，看HCSF层，他们的核心思想是对近程和远程的node区分对待，分不同的层级对待。对于离自己近的node，用类似skip connection的连接直连最后；而离得远的nodes则先分组aggregate几次后再连到主bus上。</p></li><li><p>Dynamic Hierarchical Channel-Squeezing Fusion (D-HCSF) 层的思想：结合了固定skeleton和动态学习skeleton的长处。蓝色是固定分支，橙色是动态。</p><img data-src="GNN-based HPE/image-20221221150056706.png" alt="image-20221221150056706" style="zoom:50%;"></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#dgcn-dynamic-graph-convolutional-network-for-efficient-multi-person-pose-estimation&quot;&gt;&lt;/a&gt; DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation&lt;/h2&gt;
&lt;img data-src=&quot;image-20221219151203243.png&quot; alt=&quot;image-20221219151203243&quot; style=&quot;zoom:50%;&quot;&gt;
&lt;img data-src=&quot;image-20221219154249782.png&quot; alt=&quot;image-20221219154249782&quot; style=&quot;zoom:50%;&quot;&gt;
&lt;h3 id=&quot;summary&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt; Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multi-person.&lt;/li&gt;
&lt;li&gt;Image-based. Graph is just used in their DGCM module.&lt;/li&gt;
&lt;li&gt;Bottom-Up.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>qDVS 论文笔记</title>
    <link href="https://www.miracleyoo.com/2023/01/28/gert-qdvs-paper/"/>
    <id>https://www.miracleyoo.com/2023/01/28/gert-qdvs-paper/</id>
    <published>2023-01-29T02:42:21.000Z</published>
    <updated>2023-04-23T01:42:51.036Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Paper's full name</strong>: A 256x256 6.3pJ/pixel-event Query-driven Dynamic Vision Sensor with Energy-conserving Row-parallel Event Scanning, <a href="https://ieeexplore.ieee.org/document/9431446">Link</a></p><h2 id="idea">Idea</h2><p>This paper proposed a novel query-driven DVS (qDVS) hardware. This new hardware combines the advantages of APS and DVS, following a fixed scanning rate to inquire <strong>all</strong> pixels whether are good to fire an event. The output of qDVS is event <strong>frames</strong>. Pixels here are responsible for fewer functions, they only need to tell whether they are good to shoot and the polarity, as their address is fixed on the generated event frames. As each pixel has fewer functions, they are able to be made smaller, which results in an overall higher pixel density. Also, since the output of qDVS is already framed, machine learning researchers don't need to do accumulation themselves and hence optimized the processing pipeline.</p><span id="more"></span><h2 id="questions">Questions</h2><ul><li>They used a fixed scanning rate to interact with all pixels, but how to change this rate to better accommodate different information densities?<ul><li>Change the external clock rate. (Not sure)</li></ul></li><li>What is the "fill factor" in DVS?</li><li>Although qDVS can directly output event frames, as qDVS works at a very high clock rate according to the paper, each frame should not contain enough events for a prediction. For problems like human pose estimation, using these frames will definitely cause a severe missing torso problem, as humans don't move all his/her body parts all the time.<ul><li>Not clearly mentioned in the paper. This problem may not be solved in their paper originally.</li><li>Maybe accumulation is also used?</li></ul></li><li>Why the dynamic range is clearly lower (68dB) than other DVS cameras like Prophesee (124dB) and DAVIS346 (120dB)? Even the RGB camera has a higher dynamic range (FLIR BlackFly, 74.35dB). This means the qDVS camera doesn't have an advantage over other RGB cameras in low-lighting conditions.<ul><li>An inference is stated in <em>Hardware Design Consideration</em> Point 6.</li></ul></li><li>Will the reset process eliminate the accumulated voltage change if this change is not big enough to trigger an event?</li></ul><figure><img data-src="kuben1-027-large.gif" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="points">Points</h2><ol type="1"><li>This is a hardware paper regarding a novel query-driven DVS sensing approach (qDVS).</li><li>It aims to boost the achievable pixel density and energy efficiency.</li><li>Combines complementary advantages of RGB + DVS.</li><li>Spec: 256 x 256, 33% fill factor, 10% temporal contrast sensitivity.</li><li>Peak rate: 0.5mW, 1.2V, 80Meps, 6.3pJ/pixel-event.</li><li>Comparison between chips:</li></ol><ul><li>Traditional CMOS/APS: High pixel density, use frame scanning at a <strong>fixed clock rate</strong>, causes constant data rate and <strong>power independent of information content</strong>.</li><li>Typical DVS: Saves energy by more efficient visual event coding. <strong>Low pixel density</strong>, inefficient implementation, caused extra area and power overhead to continuously monitor for events and handle requests and acknowledge handshaking with each pixel. This leads to substantial <strong>static power</strong>.</li><li>qDVS: Uses a <strong>clocked time-division multiplexing</strong> to <strong>periodically scan</strong> the array, querying each pixel to check whether the brightness change has passed a threshold. This scanning time interval is way smaller than the APS rate.</li></ul><ol start="7" type="1"><li>gDVS achieves 2x greater pixel density, 20x greater energy efficiency than state-of-the-art.</li><li>qDVS is more suitable for deep learning since it directly outputs frames of events and does not require accumulation of events in the buffer memory, to reconstruct frames or dynamic clustering algorithms to identify object boundaries, in order to track them.</li></ol><h2 id="hardware-design-consideration">Hardware Design Consideration</h2><ol type="1"><li><p>In qDVS, the <span class="math inline">\(V_{IN}\)</span> is defined by both <span class="math inline">\(C_{PH}\)</span> and <span class="math inline">\(C_{REF}\)</span>, where <span class="math inline">\(\Delta V_{IN} = \frac{C_{PH}}{C_{PH}+C_{REF}}\cdot\Delta V_{PH} + \frac{C_{REF}}{C_{PH}+C_{REF}}\cdot\Delta V_{REF} \space\space\space\space\space (1)\)</span>.</p></li><li><p>If <span class="math inline">\(\Delta V_{IN}&gt;+\epsilon\)</span>, output an ON event, if <span class="math inline">\(\Delta V_{IN}&lt;-\epsilon\)</span>, output an OFF event.</p></li><li><p>The photodiode generates output <span class="math inline">\(V_{PH}\)</span> in a <strong>logarithmic</strong> way to the brightness. This is the reason that there is a <span class="math inline">\(log\)</span> applied to the brightness in DVS.</p></li><li><p>Capacitors work as <strong>differentiators</strong>, turning the variation of electric potential into electric potential current <span class="math inline">\(Q=It=C\Delta U\)</span>, <span class="math inline">\(I=C\frac{dU}{dt}\)</span>. Therefore, when the brightness on this photodiode doesn't change, no potential is generated after the capacitor <span class="math inline">\(C_{PH}\)</span>.</p></li><li><p>As the threshold for generating an ON/OFF event is <strong>fixed</strong> and based on <span class="math inline">\(\Delta V_{IN}\)</span>, while <span class="math inline">\(\Delta V_{IN}\)</span> is decided by two factors <span class="math inline">\(\Delta V_{IN}\)</span> and <span class="math inline">\(\Delta V_{REF}\)</span> together, if we want to make the system easier to shoot events, then it's better to use a larger <span class="math inline">\(\Delta V_{REF}\)</span>. In this way, a smaller <span class="math inline">\(\Delta V_{IN}\)</span> is able to trigger an event. Otherwise, if the <span class="math inline">\(\Delta V_{REF}\)</span> is set smaller, a larger <span class="math inline">\(\Delta V_{IN}\)</span> in required for shooting an event, which leads to a higher requirement of input brightness change.</p></li><li><p>As formula (1) shows, <span class="math inline">\(\Delta V_{PH}\)</span> cannot contribute more than <span class="math inline">\(1\times \Delta V_{PH}\)</span> as <span class="math inline">\(C_{REF}\)</span> is not negative. There is <strong>no amplifier</strong> applied after <span class="math inline">\(V_{PH}\)</span> to enlarge this value. This is called passive coupling, while in a regular DVS camera, an active amplifier is applied after the photodiode, making a relatively smaller change of potential becomes larger. This is why qDVS has a much smaller dynamic range.</p></li><li><p>Illumination intensity change sensitivity (Hardness to trigger an event) is determined by <span class="math inline">\(C_{REF}\)</span>, while the dynamic range (to which absolute brightness range events can still be effectively triggered) is determined by coupling amplification of input <span class="math inline">\(\Delta V_{PH}\)</span>.</p></li><li><p>As qDVS is a frame scan-based design, there is no need to <strong>report the row and column address</strong> for each event, which alleviates the energy consumption by acquiring these addresses, resulting in a higher information density with a fixed bus bandwidth.</p></li><li><p>The queries are processed in a <strong>row-parallel, column-serial</strong> scanned output pattern. (How does this row parallel happen? Not clear in the paper. My guess is that it is using clocked time division multiplexing technique here for rows.)</p></li><li><p>Column readout performs thresholding comparison of the pixel photodiode voltage, using <strong>bipolar voltage modulation</strong> of <span class="math inline">\(V_{REF}(V_{UP} \space and \space V_{DN})\)</span>, to detect ON and OFF temporal change events in intensity.</p></li><li><p>A Gm-boosted high-gain cascode amplifier (Gain &gt; 90dB) provides a <strong>voltage clamp</strong> on the sense line to mitigate capacitive loading on the sense line and eliminate CV2 losses incurred in APS and DDS readout.</p></li><li><p>A dynamic comparator eliminates static power losses in event generation. This is the key reason that it is more energy efficient than regular DVS. (?What does dynamic comparator means? How different from the regular DVS?)</p></li></ol><figure><img data-src="kuben2-027-large.gif" alt="img"><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;Paper&#39;s full name&lt;/strong&gt;: A 256x256 6.3pJ/pixel-event Query-driven Dynamic Vision Sensor with Energy-conserving Row-parallel Event Scanning, &lt;a href=&quot;https://ieeexplore.ieee.org/document/9431446&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;idea&quot;&gt;Idea&lt;/h2&gt;
&lt;p&gt;This paper proposed a novel query-driven DVS (qDVS) hardware. This new hardware combines the advantages of APS and DVS, following a fixed scanning rate to inquire &lt;strong&gt;all&lt;/strong&gt; pixels whether are good to fire an event. The output of qDVS is event &lt;strong&gt;frames&lt;/strong&gt;. Pixels here are responsible for fewer functions, they only need to tell whether they are good to shoot and the polarity, as their address is fixed on the generated event frames. As each pixel has fewer functions, they are able to be made smaller, which results in an overall higher pixel density. Also, since the output of qDVS is already framed, machine learning researchers don&#39;t need to do accumulation themselves and hence optimized the processing pipeline.&lt;/p&gt;</summary>
    
    
    
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="DVS" scheme="https://www.miracleyoo.com/tags/DVS/"/>
    
  </entry>
  
  <entry>
    <title>SMPL 完全攻略 -- 从定义到文章到部署</title>
    <link href="https://www.miracleyoo.com/2023/01/21/smpl-all/"/>
    <id>https://www.miracleyoo.com/2023/01/21/smpl-all/</id>
    <published>2023-01-22T01:30:32.000Z</published>
    <updated>2023-04-23T00:31:06.584Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><ul><li>SMPL主要含有两组参数，一组是人物的体态信息β，一组是人物的姿态信息θ。</li><li>SMPL本身是“相对的”，其只包含人物本身的信息，而不包含任何与环境、相机视角、位置等信息。另外，其mesh点记录的值是相对于模板人类模型标准值的。</li><li>SMPL不包含手、脸和衣服，但后续的其他文章逐渐完善了相应参数。</li></ul><span id="more"></span><ul><li>SMPLX中的参数某种程度上包含了肌肉随动作（Pose）变化的信息。他们采用了铰链模型，并搭配PCA方法分析主要关联项，得到了当人体从静态到某个特定Pose时哪些肌肉会发生形变以及如何形变的信息，而其NB也主要NB在这里。有了这些关联模型，人体运动时重建的mesh就不是那种被奇怪地拉伸的mesh了，而是更加符合人体肌肉运动规律的mesh。</li><li>他们使用的训练数据是4维扫描，即<code>3D Mesh + Time</code>的扫描。他们也正是通过分析关节点和mesh随着时间的变化，才分析得到了Pose和Mesh变形之间的相关性。</li><li>在构建MONO/SMPL+H时，由于手非常复杂且容易被遮挡，另外在全身扫描时手部分辨率也很有限，所以他们特意为了手部额外采集了一个数据集，含有各种人物、姿态和遮挡。</li></ul><h2 id="名称辨析">名称辨析</h2><ul><li><code>SMPL</code>: A Skinned Multi-Person Linear Model。人体模型。</li><li><code>MONO</code>: A hand Model with Articulated and Non-rigid defOrmations。手部模型。</li><li><code>SMPL+H</code>: A fully articulated body and Hand model。手部模型+人体模型。</li><li><code>SMPL-X</code>: SMPL eXpressive，是一个含有姿态、表情、手部动作的人体模型。</li><li><code>SMPLify-X</code>：<code>SMPL-X</code>原文中提到的用于拟合SMPL-X模型的一种方法。具体操作是先预测2D Joints，再用Optimization的方法拟合3D模型使得投影与2D Joints尽可能重合。</li><li><code>SMPLify</code>：<code>SMPLify-X</code>的前辈。方法类似，但是效果和速度差一点。</li></ul><h2 id="d模型制作和运用中常用术语">3D模型制作和运用中常用术语</h2><p>该节摘自<a href="https://zhuanlan.zhihu.com/p/256358005">SMPL论文解读和相关基础知识介绍</a></p><blockquote><ul><li>顶点（vertex）：动画模型可以看成多个小三角形（四边形）组成，每个小三角形就可以看成一个顶点。顶点越多，动画模型越精细。</li><li>骨骼点：人体的一些关节点，类似于人体姿态估计的关键点。每个骨骼点都由一个三元组作为参数去控制（可以查看欧拉角，四元数相关概念）</li><li>骨骼蒙皮（Rig）：建立骨骼点和顶点的关联关系。每个骨骼点会关联许多顶点，并且每一个顶点权重不一样。通过这种关联关系，就可以通过控制骨骼点的旋转向量来控制整个人运动。</li><li>纹理贴图：动画人体模型的表面纹理，即衣服裤子这些。</li><li>BlendShape：控制动画角色运动有两种，一种是上面说的利用Rig，还有一种是利用BlendShape。比如：生成一种笑脸和正常脸，那么通过BlendShape就可以自动生成二者过渡的动画。这种方式相比于利用Rig，可以不定义骨骼点，比较方便。</li><li>蒙皮：将模型从一个姿态转变为另一个姿态，使用的转换矩阵叫做蒙皮矩阵。（Linear Blend Skinning算法）</li><li>顶点权重(vertex weights)：用于变形网格mesh</li><li>uv map：将3D多边形网格展开到2D平面得到 UV图像</li><li>texture map：将3D多边形网格表面的纹理展开到2D平面，得到纹理图像</li><li>拓扑(topology)：重新拓扑是将高分辨率模型转换为可用于动画的较小模型的过程。两个mesh拓扑结构相同是指两个mesh上面任一个三角面片的三个顶点的ID是一样的（如某一个三角面片三个顶点是2,5,8；另一个mesh上也必有一个2,5,8组成的三角面片）</li><li>linear blend skinning algorithm</li></ul><p>每个关节的数据结构包含：关节名字、骨骼中其父节点的索引、关节的绑定姿势之逆变换（蒙皮网格顶点绑定至骨骼时，关节的位置、定向及缩放）</p></blockquote><h2 id="三种类型的smpl文章应用">三种类型的SMPL文章/应用</h2><ol type="1"><li>提出SMPL这种模型本身的定义的文章。这些文章使用大量不同动作的3D人体扫描构筑了一个平均人类模型（Male，Female，Neutral），且定义了当人物的形体和动作发生特定变化时表皮的相应变形方式。文中的“训练”指的是训练这些运动/形态参数与实际表皮形变的函数映射。</li><li>使用单张/多张不同角度照片来推理相应人物的SMPL模型位置。如SMPLify和SMPL-X。SMPLify-X（用于拟合SMPL-X模型的方法）的pipeline是：<ol type="1"><li>先自下而上用OpenPose预测人体2D关键点。</li><li>然后再结合各种Prior（身体姿态、手部姿态、身体形状、面部姿态、面部表情、极端弯曲）和各种Loss Punishment（2D与3D在平面投影的Loss，身体Parts互相穿透的Loss）去让3D的模型拟合这些关键点。</li><li>用了Optimization算法直接去拟合的3D模型参数，这个步骤没有使用深度学习。相反，使用的是Limited-memory BFGS optimizer (L-BFGS)的强化Wolfe line search（SMPL-X）、Chumpy+OpenDR（SMPLify）。</li></ol></li><li>端到端的深度学习文章。这些文章都是在SMPL-X发表之后涌现出来的。文章中不再使用Optimization-based methods，而是转而使用SMPLify-X来通过照片或多角度照片生成Ground Truth，制造完数据集后直接使用DL进行预测。</li></ol><h2 id="smpl">SMPL</h2><ul><li><p>SMPL一文与从图片/视频/XXXX预测人体形态没有任何关系，它的贡献单纯是提出了一个更好的人体模型，而这个模型可以很好地建模人的体态和姿势，同时肌肉/蒙皮形状会随着人的运动而相应变化，从而达到拟真的效果，而不会出现滑稽的拉伸形变。</p></li><li><p>SMPL提出的人体模型的输入是由两部分组成的：</p><ul><li>β：一个10维vector</li><li>θ：一个3(K+1)维vector，K是骨架节点数，这里是23。加的1是人体中心。</li></ul></li><li><p>SMPL的输出是N个顶点的坐标，维度为3N。N: 顶点数，6890。</p></li><li><p>SMPL人类模型是可微分的，也就是说，如果你有了人物的3D扫描，想用一个deep learning model来预测这个扫描，你可以直接输入图片/视频/XXXX，中间输出是<span class="math inline">\(\beta+\theta\)</span>，然后最终输出N个顶点的位置，而这N个顶点你是可以直接去和GT做L1 loss的，因为可微也就意味着可以auto backward。</p></li><li><p><span class="math inline">\(\beta\)</span>的十个参数物理意义：</p><blockquote><p>0 代表整个人体的胖瘦和大小，初始为0的情况下，正数变瘦小，负数变大胖（±5） 1 侧面压缩拉伸，正数压缩 2 正数变胖大 3 负数肚子变大很多，人体缩小 4 代表 chest、hip、abdomen的大小，初始为0的情况下，正数变大，负数变小（±5） 5 负数表示大肚子+整体变瘦 6 正数表示肚子变得特别大的情况下，其他部位非常瘦小 7 正数表示身体被纵向挤压 8 正数表示横向表胖 9 正数表示肩膀变宽</p></blockquote></li><li><p>论文核心图的解释：</p><figure><img data-src="image-20221118192351294.png" alt="image-20221118192351294"><figcaption aria-hidden="true">image-20221118192351294</figcaption></figure><ul><li>(a)是平均人体模型，男女各一个，后续的演算都是基于标准平均人体模型的</li><li>(b)是加入了人物体态参数的结果</li><li>(c)是加入了特定动作发生时肌肉/蒙皮变形补偿后的结果。注意此时只是对即将发生的动作进行补偿，但还没有真正apply动作。</li><li>(d)是实际让人物摆出了相应动作的结果</li></ul></li><li><p>再回到上图的一些重要符号表达，</p><ul><li><p><span class="math inline">\(\bar{T}\)</span>: 3N维vector，由N个串联的顶点表示的初始状态下的平均模型。这里的3并不是xyz 3D坐标，而是每个关节点相对于其父关节的轴角旋转量。这里的坐标以父节点为原点。</p><blockquote><p><span class="math inline">\(\omega\)</span> denotes the axis-angle representation of the relative rotation of part k with respect to its parent in the kinematic tree</p></blockquote></li><li><p><span class="math inline">\(\mathcal{W}\)</span> : <span class="math inline">\(4\times 3N\)</span>维，其实应该是<span class="math inline">\(K\times 3N\)</span>维才对，但为了和现存渲染引擎同步，这里取每个顶点最多被附近4个关节点的运动影响。<span class="math inline">\(\mathcal{W}\)</span>是LBS/QBS混合权重矩阵。由于顶点和其附近的关节点存在相关性，这个相关性是每个顶点对应多个关节点，且权重不一。这里就需要这样一个矩阵来记录这种相互关系，即关节点对顶点的影响权重 (第几个顶点受哪些关节点的影响且权重分别为多少)。</p></li><li><p><span class="math inline">\(J\)</span> : 用于补偿joint position因为目标人物体态变化产生的位移。它通过表皮的形状位置来推测新的joints位置。</p></li><li><p><span class="math inline">\(B_S(\overrightarrow{\beta})\)</span>里面的这个<span class="math inline">\(B_S\)</span>的作用是把已经经过PCA筛选压缩过的10个参数恢复到正常的3N维度，即对于每个顶点，应当向哪个方向变化来适应这个人的体态。这里恢复出来的值也是相对于平均模型的。</p></li><li><p><span class="math inline">\(B_S(\overrightarrow{\theta})\)</span>同理，由于我们输入的pose <span class="math inline">\(\overrightarrow{\theta}\)</span> 也只有3(K+1)维度，而想要对因为人体做出特定动作产生的形变进行补偿，也需要一个3N维度的值来对每个顶点分别建模补偿。</p></li></ul></li><li><p>而关于训练，训练过程中对于形态和姿势的训练时分开进行的。前者在一个<code>Multi-Shape Dataset</code>上训练完成，而后者在一个<code>Multi-Pose Dataset</code>上训练完成，二者是相互独立的。</p></li><li><p>模型训练主要训练的是这些参数：形态的<span class="math inline">\(\bar{T}, \mathcal{S}\)</span>和姿势的参数<span class="math inline">\(\mathcal{J},\mathcal{W},\mathcal{P}\)</span>。除去上面介绍过的<span class="math inline">\(\bar{T},\mathcal{W}\)</span>，其他几位的介绍如下</p><ul><li><span class="math inline">\(\mathcal{J}\)</span>: <span class="math inline">\(3N\times 3K\)</span>，将rest vertices转换成rest joints 的矩阵。</li><li><span class="math inline">\(\mathcal{P}\)</span>: 矩阵形状为<span class="math inline">\(3N\times 9K\)</span>，这里之所以K前面系数是9，是因为使用时把关节点的坐标从3D空间坐标处理成了其相对于根节点的旋转矩阵，而3D旋转矩阵有9个参数。<span class="math inline">\(\mathcal{P}\)</span>是这所有27*9=207个 pose blend shapes 组成的矩阵。因此，pose blend shape 函数BP(θ→;P) 完全被矩阵 P 定义。</li></ul></li></ul><h2 id="模型本身使用说明">模型本身使用说明</h2><ul><li>模型本身在Python中的使用是相当简单直接的：<code>Load Model -&gt; Assign β&amp;θ -&gt;Dump</code></li><li>Dump出来的pkl模型可以直接在Blender/Unity等软件中读取，应用到SMPL模型中。</li></ul><h2 id="smpl-x">SMPL-X</h2><ol type="1"><li>SMPL-X其实是一个大杂烩，它结合了基本姿态用的SMPL模型，手部姿态的MANO模型和面部特征的FLAME模型。</li><li>SMPL-X模型本身分别在多个数据集上训练：<ol type="1"><li><span class="math inline">\(\{S\}\)</span>：形状空间参数（shape space parameters），在3800个A-pose捕捉不同性别的变化的数据集上训练</li><li><span class="math inline">\(\{W，P，J\}\)</span> ：身体姿态空间参数（body pose space parameters），在1786个不同姿态的数据集上训练</li><li>MANO：姿态空间及姿态相关混合形状（pose space and pose corrective blendshapes），1500 手部扫描数据</li><li>FLAME：表情空间<span class="math inline">\(\{\varepsilon\}\)</span>（expression space），3800 头部高精度扫描数据上训练</li></ol></li><li>SMPL-X区分了男女模型，用了一个性别分类器。</li><li>Pipeline（上文有提）：<ol type="1"><li>先自下而上用OpenPose预测人体2D关键点。</li><li>然后再结合各种Prior（身体姿态、手部姿态、身体形状、面部姿态、面部表情、极端弯曲）和各种Loss Punishment（2D与3D在平面投影的Loss，身体Parts互相穿透的Loss）去让3D的模型拟合这些关键点。</li><li>用了Optimization算法直接去拟合的3D模型参数，这个步骤没有使用深度学习。相反，使用的是Limited-memory BFGS optimizer (L-BFGS)的强化Wolfe line search（SMPL-X）、Chumpy+OpenDR（SMPLify）。</li></ol></li><li>既可以用于从2D joints coordinates得到SMPL-X，也可以通过3D joints coordinates得到，如论文“<em>I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</em>” 中，作者就通过SMPLify-X得到了H3M数据集的SMPL版本。</li></ol><h2 id="已有数据集转化">已有数据集转化</h2><ul><li>带有图片和2D Human Joints Label的数据集：使用<a href="https://github.com/JiangWenPL/multiperson/tree/master/misc/smplify-x">SMPLify-X</a>，或<a href="https://github.com/facebookresearch/eft">EFT</a>。</li><li>带有图片和3D Human Joints Label的数据集：使用<a href="https://github.com/JiangWenPL/multiperson">multiperson</a>库中的<a href="https://github.com/JiangWenPL/multiperson/tree/master/misc/smplify-x">SMPLify-X-for-3D</a>工具.</li><li>带有图片和多视角2D Human Joints Label的数据集：使用SMPLify</li><li>只有图片：使用<a href="https://github.com/facebookresearch/eft">EFT</a>。</li></ul><h2 id="动作的生产转化导入">动作的生产/转化/导入</h2><ul><li>我们已经将相关github repo打包成了docker，可以直接一键部署使用。具体链接和详细教程后续整理后更新。</li></ul><h2 id="other-papers">Other Papers</h2><p>在看完上面几篇最重要的鼻祖论文之后，下面是几篇最近的使用了SMPL系统的文章，通过对他们的分析可以对SMPL的应用有一个更好的理解。</p><h3 id="object-occluded-human-shape-and-pose-estimation-from-a-single-color-image">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</h3><p><img data-src="image-20221129172810204.png" alt="image-20221129172810204" style="zoom:50%;"></p><ul><li><p>数据采集：</p><ul><li>他们先用Mask-RCNN和Alphapose来预测人体Mask及2D Joints。</li><li>对于不准确的label，他们手动矫正。</li><li>最后用SMPLify-X中的多视角方法得到GT。</li><li>得到GT后，他们把这个3D SMPL模型朝着图像平面投影，若投影不在Mask覆盖范围内（即被遮挡），那就给涂黑（-0.5），反之，用表面xyz构建3通道UV Map。</li></ul></li><li><p>数据处理：</p><ul><li>文中预测的基本单位是顶点（即表面上的全部点）而非骨骼、joints。</li><li>UV Map看上去是彩色的，这个颜色其实意味着UV Map有三个通道，而这三个通道是相应每个点所对应的x、y、z坐标值。</li><li>其中，对于未被遮挡的点（Mask点亮的点），把xy给norm到<span class="math inline">\([-0.5,0.5]\)</span>即可；而对被遮挡的点，其xyz是<span class="math inline">\([-0.5,-0.5,-0.5]\)</span>，即全黑（以-0.5为黑）。</li></ul></li><li><p>Pipeline：</p><ol type="1"><li>Pipeline分为两部分，Train和Predict。</li><li>Train会先训练一个UV Map Inpainting Network。把之前处理过的、过了norm且遮挡部分给涂黑的UV Map给送到一个网络里，试图输出被涂黑区域得以被重新预测的UV Map。这就把3D SMPL HPE转成了一个image inpainting问题，即填补空缺部分图像。</li><li>Predict过程中，会先预测Mask，然后直接concatenate共同作为输入。</li><li>然后RGB Image预测全体UV Map时候用的是另一个完全不同于训练时UV Map Inpainting Network的branch，但是在这个branch里，Inpainting Branch的高维度feature被拿了进来作为constrain。这里说的不清楚，应该是把这个feature map concatenate到RGB image过了Encoder之后的feature map上作为prior。需要参考代码理解。</li></ol></li><li><p>Points：</p><ul><li><p>UV Map Inpainting Sub-Network的Loss由三部分构成：</p><ol type="1"><li><p>首先是预测的UV Map和GT UV Map的L1 Loss。</p></li><li><p>然后是一个挺有趣的term，用于保证人体的每个部位预测出的UV Map是平滑的：对每个点计算一个它与其下方和右方点的差的绝对值。然后通过最小化这个值，保证预测的UV Map的平滑性。<span class="math inline">\(L_{tv}=\sum_{k}\sum_{(i,j)\in R_k}(|P_{i+1,j}-P_{i,j}|+|P_{i,j+1}-P_{i,j}|)\)</span>。其中<span class="math inline">\(R_k\)</span>是第k个身体部位对应的区域。</p></li><li><p>最后是关于人体各部位交界处的点。这些点在多个部位的UV Map上都有坐标。这个loss term旨在计算这些点在所有UV Map上的均值与GT的L1 Loss，以保证交界点处的值也平滑。</p></li><li><p>他们把3D点投影成2D时用了weak perspective projection弱透视投影。</p></li><li><p>速度相较于基于优化的SMPLify-X快得多，从30s降低到了13ms。</p></li><li><p>他们也用了已有的数据集+Occlusion的方式在多个数据集上得到了结果，加遮挡的方式值得学习。</p><p><img data-src="image-20221129175700470.png" alt="image-20221129175700470" style="zoom:50%;"></p></li></ol></li></ul></li></ul><h2 id="monocular-one-stage-regression-of-multiple-3d-people">Monocular, One-stage, Regression of Multiple 3D People</h2><ul><li><p>Idea: 本文的中心思想是用单张图像解析生成图中的多人SMPL模型。</p></li><li><p>Questions:</p><ol type="1"><li>2D label的数据集可以通过EFT（Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation）生成，但那些只有3D joints coordinates但没有SMPL的数据集是怎么用上的？</li></ol></li><li><p>Pipeline:</p><ul><li>首先过一个Backbone提取feature map。</li><li>然后这个feature map被送到三个branch中，它们分别用来预测：<ol type="1"><li>人体中心位置的热力图<span class="math inline">\(C_m\in R^{1\times H \times W}\)</span>。它预测的是2D的人体中心位置。</li><li>相机参数map <span class="math inline">\(A_m\)</span>。这里的<span class="math inline">\(A_m\)</span>形状是<span class="math inline">\(R^{3\times W\times H}\)</span>，它的意义是：如果图片上某个pixel是某个人体中心的话，对应的这个人的相关相机参数。每个pixel预测的相机参数含有三个值，分别是<span class="math inline">\((s, t_x, t_y)\)</span>。<span class="math inline">\(s\)</span>是scale，包含了人的body size和depth。而<span class="math inline">\(t_x\)</span>和<span class="math inline">\(t_y\)</span>则是投影相对图片中心在x和y方向上的移动距离。<span class="math inline">\(t_x, t_y \in (-1,1)\)</span>，被normalized过。</li><li>SMPL map <span class="math inline">\(S_m\)</span>。这个和<span class="math inline">\(A_m\)</span>类似，也是假定每个pixel上都有一个可能的人体中心，然后这个人对应的SMPL参数。形状为<span class="math inline">\(R^{142\times H\times W}\)</span>。对于每个pixel，其中10个参数是shape，132个是pose。</li></ol></li><li>上面整个过程可以概括为：知道人大致在哪-&gt;知道这个位置的人的位移和scaling-&gt;知道这个人的形状和pose。</li></ul></li><li><p>Points:</p><ul><li><p>Collision-aware representation: 基于中心的方法往往使用的是bbox的中心，而这个中心在人体中并没有实际意义，且容易落到人体外部。本文选择了计算未被遮挡的人体躯干的几个点的均值作为中心点。如果躯干点都被遮挡了，就计算全部可见joints的中心点。</p><blockquote><p>We define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips).</p></blockquote></li><li><p>Multiple-human same-center情况的处理：使用类似于正正离子相互排斥的概念，让相近的两个人体中心尽可能远离彼此。</p></li></ul></li></ul><h2 id="exemplar-fine-tuning-for-3d-human-model-fitting-towards-in-the-wild-3d-human-pose-estimation">Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation</h2><ul><li><p>Idea: 3D HPE的野外数据和2D不同，很难取得。本文旨在利用已有的大规模2D数据集，通过高质量的优化匹配得到对应的3D labels，进而辅助训练。</p></li><li><p>Questions:</p><ol type="1"><li>本文拟合的是SMPL还是3D骨架<ul><li>拟合的是SMPL，3D坐标是SMPL pose parameter <span class="math inline">\(\theta\)</span> 的计算后的结果。</li></ul></li><li>本文与SMPLify-X有何不同<ul><li>SMPLify-X只是一种单纯的fitting-based method，而本文则是结合了fitting和regression二者。</li></ul></li><li>有哪些文章使用了其公开的3D SMPL数据labels</li><li><strong>prior terms到底长什么样？怎么用到计算循环中的？</strong></li></ol></li><li><p>Pipeline：EFT方法是优化方法与回归方法的结合。具体操作是先正常训练一个神经网络，让它从2D坐标和输入图像预测对应的3D坐标。回归方法到这里就结束了，但EFT这才刚刚开始。EFT针对每一个图片实例，再得到神经网络的预测结果后，再以这个结果作为起点放到fitting method中去进一步优化，并且设定了一个限制就是新的优化后的神经网络权重<span class="math inline">\(w\)</span>与训练好的最佳权重<span class="math inline">\(w^*\)</span>之间差距不能太大。值得注意的是，这个优化后的权重<span class="math inline">\(w^+\)</span>仅服务于这一张图，用完就扔了。</p></li><li><p>Points：</p><ul><li><p>从2D坐标得到3D位置有两种做法：基于优化（fitting）的方法和基于回归（regression）的方法。前者通过构建多个Loss惩罚项、结合多种prior来逐步得到最能满足限制条件的3D坐标，而后者则直接通过深度网络来一步到位预测。</p></li><li><p>而本文提出的EFT则是结合了上面两种方法的方法。</p><figure><img data-src="image-20221205130430922.png" alt="image-20221205130430922"><figcaption aria-hidden="true">image-20221205130430922</figcaption></figure><p>上面三组公式分别是优化法、回归法，及EFT的公式。</p><p><span class="math inline">\(\Theta\)</span>代表SMPL模型的全部参数，其中，<span class="math inline">\(\theta,\beta\)</span>分别代表pose（以铰链夹角的形式展现）和shape参数。<span class="math inline">\(J\)</span>是3D location，<span class="math inline">\(\hat{J}\)</span>是2D location，<span class="math inline">\(\pi\)</span>是Projection matrix，用于把3D坐标投影到图像的2D平面，<span class="math inline">\(M\)</span>用于把SMPL参数转换为absolute 3D坐标。<span class="math inline">\(L_{pose}、L_{shape}\)</span>分别是pose、shape的prior。<span class="math inline">\(\Phi\)</span>是神经网络,<span class="math inline">\(w\)</span>是神经网络的权重。<span class="math inline">\(I\)</span>是输入图片。其中，<span class="math inline">\(\Theta{w}=\Phi_w(I)\)</span></p></li><li><p>优化方法的Loss包含三部分：</p><ol type="1"><li>2D Loss</li><li>Pose Prior</li><li>Shape Prior</li></ol></li><li><p>回归方法的Loss包含三部分：</p><ol type="1"><li><p>2D Loss</p></li><li><p>3D Loss</p></li><li><p>SMPL Loss</p></li></ol></li><li><p>EFT方法的Loss包含三部分：</p><ol type="1"><li>2D Loss</li><li>进一步优化（fitting）后的神经网络参数与预训练网络的神经网络参数的差距Loss</li><li>Shape Prior</li></ol></li><li><p>EFT中有个参数<span class="math inline">\(\gamma\)</span>，这个参数决定了优化后的神经网络参数<span class="math inline">\(w\)</span>与预训练的<span class="math inline">\(w^*\)</span>的相似程度。如果<span class="math inline">\(\gamma\)</span>很大，那么基本这个优化步骤就没用了，因为<span class="math inline">\(w\)</span>不敢跑远，结果将会和神经网络直接预测出来的结果类似。再fitting-based method中，也有这个<span class="math inline">\(\gamma\)</span>，作用相似，如果太大的话，得到的结果将会是pose prior的平均pose。</p></li><li><p>与纯优化方案不同，这里EFT优化的不是<span class="math inline">\(\Theta, \pi\)</span>，而是神经网络的参数<span class="math inline">\(w\)</span>。</p></li><li><p>EFT中没有使用pose prior，因为它假定了神经网络已经隐式地编码了prior。</p></li><li><p>文章中帮我们跑了这些数据集（2D joints -&gt; SMPL）：<code>COCO, MPII, LSPet, PoseTrack, and OCHuman</code> datasets。</p></li><li><p>值得一提的是，由于优化方案有最低可见joints限制，所以不符合要求的人物labels已经被删掉了，所以得到的3D标签并非与原来的一一对应。</p></li></ul></li></ul><h2 id="问题">问题</h2><ul><li>已有的SMPL数据集都是怎么来的，他们是：<ul><li>仿真</li><li>3D扫描</li><li>采集RGB的图片，同时通过mocap得到pose（2D/3D），结合SMPLify-X/<a href="https://files.is.tue.mpg.de/black/papers/MoSh.pdf">MoSh</a>/<a href="https://amass.is.tue.mpg.de/">Mosh++</a>得到SMPL-X模型GT。这里mocap可以是使用marker的，也可以是不使用marker的multi-view vision-based mocap system。</li><li>录多角度/单角度Video，全程逐帧应用SMPL预测，得到GT。过程中需要先用openpose预测得到2D坐标。</li></ul></li><li>最重要的几个SMPL数据集？<ol type="1"><li>AMASS，这个数据集是个整合数据集，把一大堆mocap数据集都给整合进来了，然后用他们自己的Mosh++跑出来SMPL，如果有手部mocap的还会把手部的mesh也进行分别优化。</li></ol></li></ul><h2 id="实践操作">实践/操作</h2><ul><li><p>首先，SMPL有一系列论文，其为：</p><ul><li><p>SMPL：最开始的一篇</p></li><li><p>SMPL+H：加了手部参数的SMPL</p></li><li><p>SMPLX：加了面部和手部参数的SMPL</p><blockquote><p>A new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face.</p></blockquote></li></ul></li></ul><h2 id="reference">Reference</h2><h3 id="papers">Papers</h3><ol type="1"><li><a href="https://smpl.is.tue.mpg.de/">SMPL</a>: <a href="https://smpl.is.tue.mpg.de/download.php">Code</a></li><li><a href="https://smpl-x.is.tue.mpg.de/index.html">SMPL-X</a>: <a href="https://github.com/vchoutas/smplx">Original</a>, <a href="https://github.com/vchoutas/smplify-x">Multi-View</a></li><li><a href="https://mano.is.tue.mpg.de/">Embodied Hands (MANO)</a></li><li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object-Occluded_Human_Shape_and_Pose_Estimation_From_a_Single_Color_CVPR_2020_paper.pdf">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</a></li></ol><h3 id="blogs">Blogs</h3><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/256358005">SMPL论文解读和相关基础知识介绍</a></li><li><a href="https://www.zhihu.com/question/292017089">想弄懂SMPL模型该如何学习</a></li><li><a href="https://zhuanlan.zhihu.com/p/419779571">SMPL-X论文学习笔记</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SMPL主要含有两组参数，一组是人物的体态信息β，一组是人物的姿态信息θ。&lt;/li&gt;
&lt;li&gt;SMPL本身是“相对的”，其只包含人物本身的信息，而不包含任何与环境、相机视角、位置等信息。另外，其mesh点记录的值是相对于模板人类模型标准值的。&lt;/li&gt;
&lt;li&gt;SMPL不包含手、脸和衣服，但后续的其他文章逐渐完善了相应参数。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
    <category term="SMPL" scheme="https://www.miracleyoo.com/tags/SMPL/"/>
    
  </entry>
  
  <entry>
    <title>WSL的克隆，WSL1与WSL2的克隆共存</title>
    <link href="https://www.miracleyoo.com/2022/11/24/wsl-clone/"/>
    <id>https://www.miracleyoo.com/2022/11/24/wsl-clone/</id>
    <published>2022-11-25T01:09:03.000Z</published>
    <updated>2023-04-23T00:12:45.458Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：WSL1和WSL2各有各的特点。WSL2支持GPU和外接USB硬件设备访问，效率也更高；但一旦涉及到Windows内部的文件系统访问，那只能说是慢的不能行，尤其是运行涉及到大量文件操作的脚本时。但是我用WSL很久了，也有了不少个性化设置，安装了很多Library。如果重头再来一遍，耗时耗力且可能某些内容与之前的环境不兼容。于是想法来了：能不能直接把已有的WSL克隆一份，并升级为WSL2，或者相反？</p><span id="more"></span><h2 id="具体操作">具体操作</h2><ol type="1"><li><p>导出已有的WSL Distribution到一个tar压缩包。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--export</span> &lt;distribution name&gt; &lt;export file name&gt;</span><br></pre></td></tr></table></figure><p>例如如果你安装的是Ubuntu，想把它导出到当前目录，文件名为<code>ubuntu.tar</code>，则：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--export</span> Ubuntu ubuntu.tar</span><br></pre></td></tr></table></figure></li><li><p>导出之后，自然就是导入了。选好导入后的压缩包准备解压的位置，然后：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--import</span> &lt;new distribution name&gt; &lt;install location&gt; &lt;export file name&gt;</span><br></pre></td></tr></table></figure><p>如果你想给新的系统命名为<code>Ubuntu-WSL2</code>,新WSL位置放<code>.\Ubuntu-WSL2</code>，则：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--import</span> Ubuntu<span class="literal">-WSL2</span> .\Ubuntu<span class="literal">-WSL2</span> ubuntu.tar</span><br></pre></td></tr></table></figure><p>值得一提的是，如果你想把新的克隆版也装到类似原本WSL的安装位置，这个位置在：</p><p><code>**%USERPROFILE%\AppData\Local\Packages\&lt;distribution package name&gt;\LocalState\ext4.vhdx**</code></p><p>以Ubuntu为例，这个目录是：<code>%USERPROFILE%\AppData\Local\Packages\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\LocalState\ext4.vhdx</code></p></li><li><p>导入完之后，必须要先运行一次，以把新的系统写入Windows Terminal的配置文件中。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> &lt;new distribution name&gt;</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> Ubuntu<span class="literal">-WSL2</span></span><br></pre></td></tr></table></figure><p><img data-src="image-20221124151115574.png" alt="image-20221124151115574" style="zoom:50%;"></p><p>我这里是把WSL2作为Main，克隆后输出的WSL1，所以图示如上。</p></li><li><p>如果你的目的只是单纯的克隆一下WSL，那到第三步其实已经完成了。但如果你还想更改WSL版本，那么：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--set-version</span> &lt;new distribution name&gt; <span class="number">2</span></span><br></pre></td></tr></table></figure><p>这里的<code>2</code>代表WSL2，如果你原本的WSL是WSL2，希望把克隆后的WSL版本降至<code>1</code>，那么把这里的<code>2</code>替换为<code>1</code>即可。如：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">--set-version</span> Ubuntu<span class="literal">-WSL1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>此时你会注意到，虽然登录进去了，但是你的user是root，不再是之前你自己的用户名。当然这个不需要慌，很容易就改了。</p><p>只需要在<code>/etc/wsl.conf</code>文件中加上这样两行即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[user]</span><br><span class="line">default=&lt;YOUR_PREVIOUS_USERNAME&gt;</span><br></pre></td></tr></table></figure><p>如果没有这个文件，创建一个该文件即可。</p><p>做完这步之后，你需要彻底关闭该WSL，重新进入后生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --terminate Ubuntu-WSL1</span><br></pre></td></tr></table></figure><p>另外，如果你不想要更改默认用户，但想要某次以该用户进入WSL，可以使用以下命令：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-d</span> Ubuntu<span class="literal">-WSL1</span> <span class="literal">-u</span> &lt;YOUR_PREVIOUS_USERNAME&gt;</span><br></pre></td></tr></table></figure></li><li><p>最后，如果你想查看确认当前安装的所有WSL系统版本：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl <span class="literal">-l</span> <span class="literal">-v</span></span><br></pre></td></tr></table></figure><p>有类似如下这两项说明已经成功：</p><p><img data-src="image-20221124152610692.png" alt="image-20221124152610692" style="zoom:50%;"></p></li></ol><h2 id="reference">Reference</h2><ul><li><a href="https://endjin.com/blog/2021/11/setting-up-multiple-wsl-distribution-instances">Setting up multiple WSL distribution instances</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：WSL1和WSL2各有各的特点。WSL2支持GPU和外接USB硬件设备访问，效率也更高；但一旦涉及到Windows内部的文件系统访问，那只能说是慢的不能行，尤其是运行涉及到大量文件操作的脚本时。但是我用WSL很久了，也有了不少个性化设置，安装了很多Library。如果重头再来一遍，耗时耗力且可能某些内容与之前的环境不兼容。于是想法来了：能不能直接把已有的WSL克隆一份，并升级为WSL2，或者相反？&lt;/p&gt;</summary>
    
    
    
    
    <category term="tech" scheme="https://www.miracleyoo.com/tags/tech/"/>
    
    <category term="wsl" scheme="https://www.miracleyoo.com/tags/wsl/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV与Qt不兼容问题</title>
    <link href="https://www.miracleyoo.com/2022/11/22/opencv-qt-comp/"/>
    <id>https://www.miracleyoo.com/2022/11/22/opencv-qt-comp/</id>
    <published>2022-11-23T01:12:42.000Z</published>
    <updated>2023-04-23T00:13:01.350Z</updated>
    
    <content type="html"><![CDATA[<p>今天测试WSL2时，发现OpenCV弹窗显示图片一直会报错，所以就试着解决了一下。</p><p>报的错是：<code>QObject::moveToThread:&lt;XXXX&gt; Current thread is not the object's thread &lt;XXXX&gt;</code>，且是一大堆错连着。</p><span id="more"></span><p>Google了一会儿，发现很多人指出是安装的qtpy和OpenCV存在兼容性问题，只要将OpenCV Downgrade一下就好了。具体的命令是：</p><p><code>pip install opencv-python==4.1.2.30</code></p><p>这个版本是确认可用的，你也可以试试其附近的版本。</p><p>我用的测试代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> qtpy</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="built_in">print</span>(qtpy.__version__)</span><br><span class="line">img=cv2.imread(<span class="string">&#x27;~/workdir/output.png&#x27;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;test&#x27;</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">1</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><p>如果跑完不报错，问题就解决了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天测试WSL2时，发现OpenCV弹窗显示图片一直会报错，所以就试着解决了一下。&lt;/p&gt;
&lt;p&gt;报的错是：&lt;code&gt;QObject::moveToThread:&amp;lt;XXXX&amp;gt; Current thread is not the object&#39;s thread &amp;lt;XXXX&amp;gt;&lt;/code&gt;，且是一大堆错连着。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="opencv" scheme="https://www.miracleyoo.com/tags/opencv/"/>
    
    <category term="qt" scheme="https://www.miracleyoo.com/tags/qt/"/>
    
  </entry>
  
  <entry>
    <title>新浪微博图床失效应对方法</title>
    <link href="https://www.miracleyoo.com/2022/11/18/sina-img-recover/"/>
    <id>https://www.miracleyoo.com/2022/11/18/sina-img-recover/</id>
    <published>2022-11-19T01:10:32.000Z</published>
    <updated>2023-04-23T00:10:49.690Z</updated>
    
    <content type="html"><![CDATA[<p>浏览之前的笔记时发现Markdown中所有上传至新浪图床上的内容都无法显示了，说实话还是很急的，毕竟内容比较多。首先明确一点，虽然你无法访问这些图了，但它们还在新浪服务器上，只是需要换一个口进去。</p>然后在网上转了一圈，有不少人提这是因为防盗链问题之类，加<meta name="referrer" content="no-referrer"><p>保平安。但问题是我是Markdown，加这个根本无从谈起。网上一些迁移脚本试了一下也并没有效果，因为下载不下来就是下载不下来嘛。</p><span id="more"></span><p>最后在<a href="https://www.31du.cn/blog/sinaimgurl.html">这里</a>发现了答案：</p><blockquote><p>因为<strong>wx1/2/3/4</strong>、<strong>ww1/2/3/4</strong> 与 <strong>ws1/2/3/4</strong> 为前缀的节点目前都被限制了，而 <strong>tva1/2/3/4</strong> 为前缀的节点目前仍可顺利打开。那么<strong>只要把网址前缀中的 wx、ww 与 ws 都改成 tva 系列应该就可以暂时继续使用微博图床的外链图片了</strong>。</p></blockquote><p>如果在Typora里，只需要正则匹配一下<code>ws\d</code>替换为<code>tva1</code>即可，然后在<code>格式-图像-移动所有图片到...</code>选项中将这些图片全部拉到本地即可。如果需要批量处理，可以参考这个<a href="https://github.com/wangshub/markdown-img-backup">脚本</a>，但需要修改一下，把上述替换加进代码中去。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;浏览之前的笔记时发现Markdown中所有上传至新浪图床上的内容都无法显示了，说实话还是很急的，毕竟内容比较多。首先明确一点，虽然你无法访问这些图了，但它们还在新浪服务器上，只是需要换一个口进去。&lt;/p&gt;
然后在网上转了一圈，有不少人提这是因为防盗链问题之类，加
&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;
&lt;p&gt;保平安。但问题是我是Markdown，加这个根本无从谈起。网上一些迁移脚本试了一下也并没有效果，因为下载不下来就是下载不下来嘛。&lt;/p&gt;</summary>
    
    
    
    
    <category term="web" scheme="https://www.miracleyoo.com/tags/web/"/>
    
    <category term="tech" scheme="https://www.miracleyoo.com/tags/tech/"/>
    
    <category term="hexo" scheme="https://www.miracleyoo.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>SLURM集群中多次登录时保存的Tmux Session不见/消失/时隐时现/随机出现的问题</title>
    <link href="https://www.miracleyoo.com/2022/11/17/slurm-tmux/"/>
    <id>https://www.miracleyoo.com/2022/11/17/slurm-tmux/</id>
    <published>2022-11-18T01:17:05.000Z</published>
    <updated>2023-04-23T00:17:38.936Z</updated>
    
    <content type="html"><![CDATA[<p>先说结论，不少Slurm集群使用了两到多个login node来缓解入口机器的压力，均衡负载。这些node每个实际上都可以被看做是一个独立的服务器，只是共享了文件系统和其他一些服务。而Tmux是依赖于node，即这里是login node的。如果换了node，自然也就找不到你之前detach后存档的tmux窗口了。</p><span id="more"></span><p>不幸的是，你能登录到哪个node不说是完全随机的，但是由系统根据负载动态分配的，所以说到底你并无法控制你ssh到cluster后被自动分配到了哪个主机。当然，你可以通过<code>hostname -s</code>或<code>cat /etc/hostname</code>来获知你当前的主机名，以判断你之前save的tmux session是不是在相同的node上。</p><p>至于解决方案，有三种：</p><ol type="1"><li><p>如果你的SLURM集群支持集群内部node相互ssh，那问题就好办了，直接ssh过去就好了。例如，如果你已知自己的tmux session建立在<code>login2</code> node上，而经过上一步的check hostname，你得知当前的主机名称是<code>login1</code>，那解决方案就简单直接了：<code>ssh login2</code>或<code>ssh -Y login2</code>. 来源：<a href="https://docs.ycrc.yale.edu/clusters-at-yale/guides/tmux/">Link</a></p><blockquote><p>注意：如果你登录server时候用的是pub和private key file认证，那么很有可能你无法直接在多个login node中跳转。若想做到这点，需要你将自己的private key上传到server，并使用<code>ssh -i &lt;your/id_rsa.pub&gt; loginN</code> 命令登录。</p></blockquote></li><li><p>如果很遗憾，由于安全设定原因，你无法在cluster内从一个node ssh到另一个，那其实解决方案就比较脏了，建议多次登录，开多个tab登录，使用多个可能的机器登录，挑选被分配到<code>login2</code> node的那个session使用吧。据我的经验来看，如果一个机器登不上<code>login2</code>，比起在同一个机器上继续尝试，使用另一台机器试试成功概率更大。当然这也许有点玄学成分了。</p></li><li><p>参考这篇文章，他们的大致做法是每次启动tmux时写一个文件保存当前login node，然后你本地登录的时候对此进行解析，如果不在一个node上就重新来。但看下来他们需要不同的login node有一个专属的名字，可以直接ssh到特定login node。<a href="https://sumner.verhaaklab.com/slurm/how_to_interactive_via_tmux/">Webpage Link</a>，以及<a href="https://github.com/TheJacksonLaboratory/sumnerdocs/tree/master/docs/confs/bin">GitHub Link</a>。</p></li></ol><p>小Tips：如果可能，把tmux放login1上，一般负载没那么高的时候经验来看更倾向于被分配到第一个login node。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;先说结论，不少Slurm集群使用了两到多个login node来缓解入口机器的压力，均衡负载。这些node每个实际上都可以被看做是一个独立的服务器，只是共享了文件系统和其他一些服务。而Tmux是依赖于node，即这里是login node的。如果换了node，自然也就找不到你之前detach后存档的tmux窗口了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="server" scheme="https://www.miracleyoo.com/tags/server/"/>
    
    <category term="slurm" scheme="https://www.miracleyoo.com/tags/slurm/"/>
    
  </entry>
  
  <entry>
    <title>相机校准 相机标定 Intrinsic/Extrinsic Calibration详解 绝对Extrinsic矩阵测得实操 Event Camera/DVS</title>
    <link href="https://www.miracleyoo.com/2022/11/04/camera-calibration/"/>
    <id>https://www.miracleyoo.com/2022/11/04/camera-calibration/</id>
    <published>2022-11-05T05:07:36.000Z</published>
    <updated>2023-04-22T18:35:11.525Z</updated>
    
    <content type="html"><![CDATA[<p>先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关于如何通过实验方法标定得到相对于坐标原点（O点，Origin）的Extrinsic Matrix的讲解，尤其是缺少中文教程。于是我就把参考各种英文资料（主要是一些大学的slides和有关采集Mocap数据集的文献）总结的一些理论推导和实验方法，以及实际能用的代码整理得到了本文。</p><span id="more"></span><p>首先是几张非常重要的Slides，后面都会refer到，可以先自行熟悉下。另外，本篇不是100%从零开始的教程，篇幅限制并无法展开所有的细节，若想深度理解，请自行结合几个大学（CMU，Stanford）相应的Slides一起学习。</p><img data-src="image-20221021152707045-1667622796441-1.png" alt="image-20221021152707045" style="zoom:33%;"><p><img data-src="image-20221021165722788-1667622796442-2.png" alt="image-20221021165722788"></p><h2 id="coordinates"><a class="markdownIt-Anchor" href="#coordinates"></a> Coordinates</h2><ul><li>在整个相机的投影与校准过程中，一共涉及3个坐标系。它们分别是：<ol><li>世界坐标系：以空间中某点为原点建立欧拉坐标系，设定xyz方向后形成的坐标系。</li><li>相机坐标系。该坐标系的原点是相机的焦点。焦点一般在相机内部，也可能落在的相机外部，这取决于focal length。坐标系的指向：x和y就是相平面的横纵坐标方向（相机视角方向），z是与xy平面垂直的方向，亦即镜头指向的前方。</li><li>图像坐标系（也可以分成两个：图像坐标系(m)和像素坐标系(pixel)）。值得注意的一点是每个像素并不是真正的一个点，pixel坐标系所代表的整数值是每个像素点的中心。</li></ol></li><li>考虑功能性，还有一个同质坐标系，用于实际运算。</li></ul><img data-src="image-20221021144051423-1667622796442-3.png" alt="image-20221021144051423" style="zoom: 25%;"><h2 id="intrinsic"><a class="markdownIt-Anchor" href="#intrinsic"></a> Intrinsic</h2><ul><li><p>理想状况下（无Skewness和Distortion），Intrinsic 矩阵Encode的信息有：Focal Length、Image Sensor的长宽（in pixel），每像素代表的米数(pixel/m)，也即相机的分辨率。</p></li><li><p>非理想情况下，Skewness和Distortion也会被放到Intrinsic中。</p></li><li><p>关于当提高/降低分辨率时候的Intrinsic变化：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi><mo separator="true">,</mo><msub><mi>c</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">k,l,c_x,c_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>都要乘以分辨率提高的系数。</p><p><img data-src="image-20221026155238653-1667622796442-4.png" alt="image-20221026155238653"></p></li><li><p>参见Intrinsic的计算过程，由于计算时已经考虑了目标物体深度对成像位置的影响，所以Intrinsic其实是包含了透视(perspective)信息的。</p></li><li><p>Intrinsic可使相机坐标系转化为图片坐标系。</p></li></ul><h2 id="extrinsic"><a class="markdownIt-Anchor" href="#extrinsic"></a> Extrinsic</h2><ul><li><p>Extrinsic 可以看作是两个矩阵写在了一起：旋转矩阵R和平移矩阵T。前三列是R，最后一列是T。 其实，虽然经常写作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>R</mi><mi mathvariant="normal">∣</mi><mi>T</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[R|T]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mclose">]</span></span></span></span>，但事实上还有一个相似变换S，这个S是个对角线矩阵，对角线上的值为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>S</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>S</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>S</mi><mi>z</mi></msub><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[S_x, S_y, S_z, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>。S直接和R乘在一起，与T无关。</p><img data-src="image-20221026162455644-1667622796442-5.png" alt="image-20221026162455644" style="zoom:50%;"></li><li><p>Extrinsic可使世界坐标系转化为相机坐标系。</p></li></ul><h2 id="skewness-and-distortion"><a class="markdownIt-Anchor" href="#skewness-and-distortion"></a> Skewness and Distortion</h2><ul><li><p>Skewness指的是相机Sensor的两个轴不垂直，即xy之间有一个小夹角。通常这不会发生，但如果有制造方面的问题，这也是可能的。</p></li><li><p>相机的Skewness</p></li><li><p>Skewness的解决方法是把这个夹角找到，并在Intrinsic中反映出来。</p><p><img data-src="image-20221026154924465-1667622796442-6.png" alt="image-20221026154924465"></p></li><li><p>Distortion包含：</p><ul><li><em>Radial Distortion</em> (径向畸变)：</li><li><em>Tangential distortion</em> (切向畸变)：本质上是相平面和相机坐标系存在一个夹角，即“图像Sensor和镜头截面不平行”。</li></ul></li><li><p>关于Distortion的计算：</p><p><img data-src="image-20221026163826434-1667622796442-7.png" alt="image-20221026163826434"></p></li></ul><h2 id="homogeneous-coordinates"><a class="markdownIt-Anchor" href="#homogeneous-coordinates"></a> Homogeneous Coordinates</h2><ul><li>同质坐标的主要用意是把本来在分母上的z（深度）给挪走，以便让投影这个Transformation从non-linear变成Linear。</li><li>注意同质坐标虽然在视觉效果上是在原本的坐标(u,v)或(x,y,z)下面加了一个1，但是实际上这个1在欧式坐标系中并不存在。当我们后面列出方程校准时，应该回到原本的欧式坐标系解。</li></ul><h2 id="imu"><a class="markdownIt-Anchor" href="#imu"></a> IMU</h2><ul><li>IMU输出三个方向角速度和三个轴向加速度的值，使用时也需要校准。</li><li>具体校准方法参见Kalibr和DV，因为我没用上，所以不多展开。</li></ul><h2 id="dvs"><a class="markdownIt-Anchor" href="#dvs"></a> DVS</h2><ul><li>DVS的校准主要分为两种方法：<ul><li>一种是直接用paired的RGB进行校准，毕竟这里的RGB和DVS share同一组透镜。</li><li>如果没有这个RGB，就直接用accumulate的frame做校准。</li></ul></li></ul><h2 id="methods"><a class="markdownIt-Anchor" href="#methods"></a> Methods</h2><h3 id="解方程直接校准p矩阵"><a class="markdownIt-Anchor" href="#解方程直接校准p矩阵"></a> 解方程直接校准P矩阵</h3><ul><li><p>在Paper <em>DHP19: Dynamic Vision Sensor 3D Human Pose Dataset</em>里， 他们采用的方法是：直接在经过Mocap校准的空间中放置一系列Markers，然后在DVS的RGB（APS）输出frame中直接进行手动标注，得到其在image plane中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(u, v)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span>坐标， 然后解方程。</p><img data-src="image-20221021155115039-1667622796442-8.png" alt="image-20221021155115039" style="zoom:40%;"></li><li><p>上图中提到了一个点：从投影矩阵计算相机坐标系的原点，即相机的焦点位置的方法：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><msup><mi>Q</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>c</mi><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">C=Q^{-1}c_4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。具体的推理其实很简单，主要就靠一个条件公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>C</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">PC=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>，即原点的投影是0。</p></li><li><p>细节上，他们用了38个Marker，并8次改变它们的位置，通过最小平方法解得最接近的11个P中参数值。这里的最小平方法的意义在于通过增加数据点取平均P值来减小误差。其实11组式子就够了，但这里还是用了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>38</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">8\times38\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>个公式，就在于此。</p></li><li><p>具体的最小平方法介绍及代码：<a href="https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html">Link</a></p></li><li><p>这个全矩阵P其实包含了Camera Intrinsic <em>K</em>， Camera Extrinsic <em>RT</em>, 以及Camera Skewness。</p></li><li><p>理论：</p><p><img data-src="image-20221026175034098-1667622796442-9.png" alt="image-20221026175034098"></p><img data-src="image-20221026175051047-1667622796442-10.png" alt="image-20221026175051047" style="zoom:50%;"></li><li><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix using Numpy</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svd_calibration</span>(<span class="params">points_3d, points_2d</span>):</span><br><span class="line">    <span class="comment"># points_3d: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># points_2d: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">0</span>] == points_2d.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">1</span>] == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> points_2d.shape[<span class="number">1</span>] == <span class="number">2</span></span><br><span class="line">    num_points = points_3d.shape[<span class="number">0</span>]</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">0</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">1</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">least_square_calibrate_camera_projection_matrix_np</span>(<span class="params">x,y,z,u,v</span>):</span><br><span class="line">    <span class="comment"># x,y,z: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># u,v: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(x) == <span class="built_in">len</span>(y) == <span class="built_in">len</span>(z) == <span class="built_in">len</span>(u) == <span class="built_in">len</span>(v)</span><br><span class="line">    num_points = <span class="built_in">len</span>(x)</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = -u[i] * x[i], -u[i] * y[i], -u[i] * z[i], -u[i]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = -v[i] * x[i], -v[i] * y[i], -v[i] * z[i], -v[i]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br></pre></td></tr></table></figure></li><li><p>注意：SVD这里是用于解决Least Squares Problem的，如果直接用<code>np.linalg.lstsq</code>函数的话（b取全0），会解得一个全0矩阵（因为0永远是一个解）。</p></li><li><p>SVD的解法细节：</p><p><img data-src="image-20221027230723755-1667622796442-11.png" alt="image-20221027230723755"></p></li><li><p>解SVD的时候可以选择把P矩阵右下角<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo separator="true">,</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">P_{(3,4)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.03853em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">3</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span></span></span></span>设为1。不设是homogeneous解法，设了之后是inhomogeneous。</p></li></ul><h3 id="kalibr"><a class="markdownIt-Anchor" href="#kalibr"></a> kalibr</h3><ul><li><p><a href="https://github.com/ethz-asl/kalibr">Link</a></p></li><li><p>Used for:</p><ol><li><strong>Multi-Camera Calibration</strong>: Intrinsic and extrinsic calibration of a camera-systems with non-globally shared overlapping fields of view</li><li><strong>Visual-Inertial Calibration (CAM-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a camera-system along with IMU intrinsic parameters</li><li><strong>Multi-Inertial Calibration (IMU-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a base inertial sensor along with IMU intrinsic parameters (requires 1-aiding camera sensor).</li><li><strong>Rolling Shutter Camera Calibration</strong>: Full intrinsic calibration (projection, distortion and shutter parameters) of rolling shutter cameras.</li></ol></li><li><p>简单说就是主攻多相机/IMU系统。<a href="https://github.com/ethz-asl/kalibr/wiki/multiple-camera-calibration">多个相机</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/Multi-IMU-and-IMU-intrinsic-calibration">多个IMU</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/camera-imu-calibration">相机+IMU</a>等。</p></li><li><p>校准出来的Extrinsic结果并不是相对原点绝对的，而是多个设备间相对的。比如IMU+Cam校准出来的Extrinsic就是IMU相对于Cam坐标的变换。</p><p>引用一段<a href="https://github.com/ethz-asl/kalibr/wiki/yaml-formats">原话</a>：</p><blockquote><ul><li><strong>T_cn_cnm1</strong><br>camera extrinsic transformation, always with respect to the last camera in the chain<br>(e.g. cam1: T_cn_cnm1 = T_c1_c0, takes cam0 to cam1 coordinates)</li><li><strong>T_cam_imu</strong><br>IMU extrinsics: transformation from IMU to camera coordinates (T_c_i)</li><li><strong>timeshift_cam_imu</strong><br>timeshift between camera and IMU timestamps in seconds (t_imu = t_cam + shift)</li></ul></blockquote></li><li><p>综上所述，Kalibr并不是满足我们需求的校准方案。</p></li></ul><h3 id="dv-calibration"><a class="markdownIt-Anchor" href="#dv-calibration"></a> DV Calibration</h3><ul><li><a href="https://inivation.gitlab.io/dv/dv-docs/docs/tutorial-calibration/">Tutorial Link</a>, <a href="https://gitlab.com/inivation/dv/dv-imu-cam-calibration">Code Link</a></li><li>单个多个DVS都可以。</li><li>基于Kalibr的方案。</li><li>对于单个DVS，校准主要进行的是undistortion，且可以在校准后直接应用于相机后续的图像，让后面的record都不再有失真。</li><li>这里的校准可以有效应对之前Upal教授提出的扭曲问题，应在后续操作中应用。</li></ul><h3 id="opencv-camera-calibration"><a class="markdownIt-Anchor" href="#opencv-camera-calibration"></a> OpenCV Camera Calibration</h3><ul><li><p>这个校准会使用chessboard，而关于3d坐标，他们用了棋盘上两个相邻的点的实际距离是已知的这个特性（因为打印的标准棋盘，间距是固定的，如30mm），来提供相应的3D坐标信息。</p></li><li><p>这个校准会分别输出Intrinsic matrix (mtx), rotation matrix (R, rvecs), translation matrix (T, tvecs), Distortion coefficients (dist)。这些输出可以直接被用来纠偏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate camera matrixes</span></span><br><span class="line">ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-<span class="number">1</span>], <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">&#x27;left12.jpg&#x27;</span>)</span><br><span class="line">h,  w = img.shape[:<span class="number">2</span>]</span><br><span class="line">newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), <span class="number">1</span>, (w,h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># undistort</span></span><br><span class="line">dst = cv.undistort(img, mtx, dist, <span class="literal">None</span>, newcameramtx)</span><br><span class="line"><span class="comment"># crop the image</span></span><br><span class="line">x, y, w, h = roi</span><br><span class="line">dst = dst[y:y+h, x:x+w]</span><br><span class="line">cv.imwrite(<span class="string">&#x27;calibresult.png&#x27;</span>, dst)</span><br></pre></td></tr></table></figure></li><li><p>关于OpenCV校准出来的Extrinsic Matrix，由于世界坐标系必定有一个原点，所以它们也是毫无疑问有一个原点的。但这个世界坐标系原点实际上只有参考意义（第一张校准图的左上角棋盘点），并无法直接使用。同时，相机坐标系的原点是相机的焦点，而这个焦点也是几乎不可预知和测量位置的（它可能在相机内部或外部，但校准并不会告诉你这个点位置，所以你也无法通过直接测量相机O点和实际世界O点之间的相对位置来纠正Extrinsic。）</p><blockquote><p><a href="https://www.appsloveworld.com/opencv/100/91/opencv-camera-calibration-world-coordinate-system-origin">Link</a>: I believe it used to be the position of the top-left corner of the checkerboard in the first calibration image, but it may have changed. You can visualized it by writing a few lines of code that project point (0,0,0) (in calibrated scene coordinates) in all the calibration images, then plotting its projected image coordinates on top of the image themselves.</p><p>You should really not depend on it being anything meaningful, and instead locate a separate feature in 3D and roto-translate the reference frame to it after calibration.</p></blockquote></li><li><p>实际上，不要想通过OpenCV的校准来直接得到有实际意义的Extrinsic，若想得到，请自行用前面提到的Method 1来实际label一些已知3D坐标的Markers对应的2D点，用Least Squares解得。</p></li><li><p>但是，OpenCV的校准可以提供有效的Distortion Coefficient和Intrinsic，并可直接被用于畸变补偿。</p></li></ul><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><h3 id="blogswebsites"><a class="markdownIt-Anchor" href="#blogswebsites"></a> Blogs/Websites</h3><ul><li><a href="****https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html****">Least Squares Regression in Python</a></li><li><a href="https://math.stackexchange.com/questions/974193/why-does-svd-provide-the-least-squares-and-least-norm-solution-to-a-x-b">Why does SVD provide the least squares and least norm solution to 𝐴𝑥=𝑏?</a></li><li><a href="https://math.stackexchange.com/questions/772039/how-does-the-svd-solve-the-least-squares-problem">How does the SVD solve the least squares problem?</a></li><li><a href="https://www.quora.com/What-is-the-real-world-coordinate-system-camera-calibration-refer-to-in-computer-vision">What is the “real world coordinate system” camera calibration refer to in computer vision?</a></li><li><a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">numpy linalg svd</a></li><li><a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html">OpenCV Camera Calibration</a></li></ul><h3 id="slides"><a class="markdownIt-Anchor" href="#slides"></a> Slides</h3><ul><li><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud810/slides/Unit-3/3C-L3.pdf#fromHistory">Udacity CS4495/6495 Introduction to Computer Vision 3C-L3 Calibrating cameras</a></li><li><a href="https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf">CMU - Camera Matrix</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture2_camera_models.pdf">Stanford - Lecture 2</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture3_camera_calibration.pdf">Stanford - Lecture 3</a></li></ul><h3 id="papers"><a class="markdownIt-Anchor" href="#papers"></a> Papers</h3><ul><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">A Flexible New Technique for Camera Calibration</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关于如何通过实验方法标定得到相对于坐标原点（O点，Origin）的Extrinsic Matrix的讲解，尤其是缺少中文教程。于是我就把参考各种英文资料（主要是一些大学的slides和有关采集Mocap数据集的文献）总结的一些理论推导和实验方法，以及实际能用的代码整理得到了本文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="camera calibration" scheme="https://www.miracleyoo.com/tags/camera-calibration/"/>
    
    <category term="camera matrix" scheme="https://www.miracleyoo.com/tags/camera-matrix/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>MMD模型导入Blender后应用动捕BVH文件</title>
    <link href="https://www.miracleyoo.com/2022/09/16/mmd-blender-bvh/"/>
    <id>https://www.miracleyoo.com/2022/09/16/mmd-blender-bvh/</id>
    <published>2022-09-17T01:03:32.000Z</published>
    <updated>2023-04-23T01:11:05.532Z</updated>
    
    <content type="html"><![CDATA[<p>如果你手里有一个MMD模型(<code>.pmx</code>)，并且想在Blender将任务骨骼与某个动捕文件（<code>.bvh</code>）同步，即应用已有的动捕数据到mmd模型中，那么下面的教程可以帮到你。</p><span id="more"></span><h2 id="插件准备">插件准备</h2><ul><li><a href="https://github.com/absolute-quantum/cats-blender-plugin">CATS</a><ul><li>读入<code>.pmx</code>文件（读入时候应该做了自动scaling，如果用<a href="https://github.com/powroupi/blender_mmd_tools">mmd_tools</a>读入则会默认大10倍）</li><li>修正模型和材料（包括常见的紫色眼睛皮肤问题）</li><li>重命名日文的骨骼、材料、mesh等到英文（在后面map动捕文件的各个骨骼到模型骨架时非常重要）</li><li>精简骨骼（比如无效的外骨骼等会被优化，内部骨骼也会被简化为动捕等常用的骨骼形式，取消IK骨等）</li><li>注意：不需要另行安装<code>mmd_tools</code>，CATS会默认pre-install。</li></ul></li><li><a href="https://www.rokoko.com/integrations/3d-character-animation-in-blender#Section-integrations-download-links">Rokoko</a>：<ul><li>作用：map动捕文件的各个骨骼到模型骨架</li><li>自动侦测骨骼间的对应关系，基本一键匹配后都是对的，即使名称什么有着较大区别也OK。</li><li>一旦你自己做过了一次匹配，这组mapping就会被它自动记录，后续相似的模型都可以直接一键。</li><li>如果不放心，或是某组匹配很常用想记录，可以直接export对应关系。</li></ul></li></ul><h2 id="模型导入与处理">模型导入与处理</h2><ul><li><p>首先是用<code>CATS</code>插件导入<code>.pmx</code>模型。这一步没什么问题，也不需要特别的操作，只管导入即可。如果右边的panel被隐藏了，显示的快捷键是<code>N</code>。</p><figure><img data-src="image-20220803233840197.png" alt="image-20220803233840197"><figcaption aria-hidden="true">image-20220803233840197</figcaption></figure></li><li><p>修正模型。刚导入的时候人物可能有各种小问题，但只要经过修正就都OK了。这个步骤是真的完完全全的“一键”操作，真的只用点一下<code>Fix Model</code>就好了。</p><p><img data-src="image-20220803234114008.png" alt="image-20220803234114008" style="zoom: 67%;"></p></li><li><p>修正完成后，请一定记得点一下<code>Start Pose Mode</code>，否则如果后面直接把<code>.bvh</code>文件应用过来，你会发现一些常见的诡异问题，如：手指尖、身上的一些饰物被留在了原地不动，只有身体其他部分在动，导致剧烈变形（这是由于手指/其他位置可能在模型中有多块骨头控制，而<code>.bvh</code>文件中对应位置只有更少的骨骼数，这就导致了某些骨骼没有得到mapping）；亦或是身体骨骼连接比较奇怪，如果旋转某块骨头就会连带其他一些意想不到的部位转动…… Anyway，解法就是简单的点一下<code>Start Pose Mode</code>，程序帮忙进行了一个骨骼和mesh间的绑定修正。这之后即使你再点击<code>Stop Pose Mode</code>也不会有问题，可方向用。</p></li><li><p>处理好之后的模型如图：</p><p><img data-src="image-20220803234942232.png" alt="image-20220803234942232" style="zoom:67%;"></p></li></ul><h2 id="与动捕文件匹配">与动捕文件匹配</h2><p>动捕文件直接导入后会默认出现一个骨架，这个骨架往往很大，所以你可能看不见它，此时需要缩小view很多倍才能看到全貌。我们的任务就是让这个骨架和修正后的人物模型骨架相匹配。</p><ul><li><p>导入<code>.bvh</code>文件。如果你知道其正常的缩放比例，如0.1或0.01，可以在load的时候选择。如果想后面看着动态调节，可以在导入并选中动捕骨骼后按下快捷键<code>S</code>，只需要移动鼠标即可进行等比例缩放。如果想缩放某个特定坐标轴，按下<code>S</code>后再单独按<code>X</code>, <code>Y</code>, 或<code>Z</code>即可。</p><p><img data-src="image-20220803235859075.png" alt="image-20220803235859075" style="zoom:50%;"></p></li><li><p>导入成功后，你将得到一个类似这样的结果：</p><p><img data-src="image-20220804000011284.png" alt="image-20220804000011284" style="zoom:50%;"></p></li><li><p>下一步，让动捕骨骼处于放松（Rest）状态，即T字状态。有两种方式：</p><ol type="1"><li><p>直接点击骨骼Panel的<code>Rest Position</code>即可。好处是方便，坏处是一旦点了这个你就无法在此基础上进行调整。</p><p><img data-src="image-20220804000607036.png" alt="image-20220804000607036" style="zoom:50%;"></p></li><li><p>删除动捕骨骼的所有变形。注意这个是Temporary的，只要你不打帧上去，这个并不会影响你这一帧的实际位置，所以不用慌。</p><p><img data-src="image-20220804001114077.png" alt="image-20220804001114077" style="zoom:50%;"></p><p>操作步骤：选择动捕骨骼-&gt;选择Pose Mode-&gt;按下<code>A</code>键全选动捕骨骼-&gt;在下拉菜单<code>Pose</code>中清楚全部Transform。完成后，你将得到一个T字动捕骨骼。</p><p><img data-src="image-20220804001253169.png" alt="image-20220804001253169" style="zoom:50%;"></p></li></ol></li><li><p>然后分两种情况：如果人物的默认放松姿态也是T字，那么就可以直接进入下一步。否则，进行如下操作：先缩放动捕骨骼到和人基本重合的size，然后分别调整左右大臂的旋转角度，使得动捕骨骼和人物骨骼的手臂平行或重合。这一步是为了保证二者在这个用于对齐的帧有着相似的骨骼形态。注意：不能调整人物的大臂来贴合动捕文件，这个即使调了也无效。</p></li><li><p>骨骼匹配：</p><p><img data-src="image-20220804002622271.png" alt="image-20220804002622271" style="zoom:40%;"></p><ol type="1"><li><p>打开<code>Rokoko</code> Panel，在<code>Retargeting</code>选项卡里选择动作的源和目标。这里源就是动捕文件，目标就是模型的骨骼。</p></li><li><p>点击<code>Rebuild Bone List</code>进行骨骼匹配。注意这里如果有些骨骼互相没有匹配好，需要手动匹配。方法是切换到<code>Pose Mode</code>后点选目标骨骼，确认其名称，然后填写到Panel对应的下方映射表格中。</p></li><li><p>骨骼匹配完成后，记着点击<code>Use Pose</code>栏的<code>Current</code>选项，表示用于对齐的是当前动捕骨骼的状态，而非Rest状态。</p></li><li><p>最后，点击<code>Retarget Animation</code>，All Set。</p><p><img data-src="image-20220804004103963.png" alt="image-20220804004103963" style="zoom:50%;"></p></li></ol></li></ul><h2 id="常见问题">常见问题</h2><ul><li><p>如果<code>.bvh</code>文件比较“飘”，脚不在地上，且偏离O点太远，我们可以做的是，先在<code>Object Mode</code>下平移动捕骨骼到合适的中心位置，然后按下<code>Shift+C</code>将cursor重置与O点，接下来选择<code>Set Origin to 3D Cursor</code>把动捕骨骼的中心点设定到坐标原点。在做完这一步之后再执行上面的<code>Retarget Animation</code>。</p><figure><img data-src="image-20220804012518890.png" alt="image-20220804012518890"><figcaption aria-hidden="true">image-20220804012518890</figcaption></figure></li></ul><h2 id="reference">Reference</h2><ul><li><a href="https://www.youtube.com/watch?v=Nyxeb48mUfs&amp;ab_channel=CGDive">Retargeting using Rokoko (COMPLETE guide) - Blender 2.8, 2.9, 3.0</a>：最重要的一个参考，本教程大部分来源于此。</li><li><a href="https://github.com/Rokoko/rokoko-studio-live-blender">Rokoko官方Wiki</a></li><li><a href="https://sites.google.com/a/cgspeed.com/cgspeed/motion-capture/the-3dsmax-friendly-bvh-release-of-cmus-motion-capture-database?authuser=0">CMU Bvh动捕数据集</a></li><li><a href="https://blog.csdn.net/linjf520/article/details/121696940">Key 3D Rigging Terms to Get You Moving - 关于 3D Rigging 的一些术语</a></li><li><a href="https://www.bilibili.com/read/cv15281069/">MMD to Blender 教程，快来动手制作你的 MMD 吧~（材质篇） - 哔哩哔哩</a></li><li><a href="https://www.bilibili.com/read/cv9778708/">【MMD/Blender】联动渲染基础教程 - 哔哩哔哩</a></li><li><a href="https://www.bilibili.com/read/cv10390257/">Blender 2D 渲染材质分享及经验总结 - 哔哩哔哩</a></li><li><a href="https://blender.stackexchange.com/questions/53886/move-3d-cursor-back-to-center-hotkey">Move 3D cursor back to center hotkey?</a></li><li><a href="https://www.youtube.com/watch?v=_ojeeuNtJM8&amp;t=96s&amp;ab_channel=Chris%27Tutorials">How to Quickly Set Object Origin in Blender 2.9 (Tutorial)</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;如果你手里有一个MMD模型(&lt;code&gt;.pmx&lt;/code&gt;)，并且想在Blender将任务骨骼与某个动捕文件（&lt;code&gt;.bvh&lt;/code&gt;）同步，即应用已有的动捕数据到mmd模型中，那么下面的教程可以帮到你。&lt;/p&gt;</summary>
    
    
    
    
    <category term="blender" scheme="https://www.miracleyoo.com/tags/blender/"/>
    
    <category term="MMD" scheme="https://www.miracleyoo.com/tags/MMD/"/>
    
    <category term="modeling" scheme="https://www.miracleyoo.com/tags/modeling/"/>
    
  </entry>
  
  <entry>
    <title>记一个不常见SLURM Device Busy BUG</title>
    <link href="https://www.miracleyoo.com/2022/06/11/slurm-resource-busy-bug-report/"/>
    <id>https://www.miracleyoo.com/2022/06/11/slurm-resource-busy-bug-report/</id>
    <published>2022-06-12T02:41:32.000Z</published>
    <updated>2023-04-23T02:42:08.321Z</updated>
    
    <content type="html"><![CDATA[<h2 id="运行环境">运行环境</h2><ul><li>SLURM 深度学习集群。</li><li>系统：Ubuntu 20.04.5 LTS</li><li>造成BUG的程序：深度学习训练代码。</li><li>具体造成BUG的库：python的<code>multiprocessing</code>。</li></ul><span id="more"></span><h2 id="现象">现象</h2><ul><li><p>训练了几十个epoch都没有问题，突然狂报这个错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">OSError: [Errno 16] Device or resource busy: <span class="string">&#x27;.nfsf7f2453cdbd6a8c40001fce6&#x27;</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/multiprocessing/util.py&quot;</span>, line 300, <span class="keyword">in</span> _run_finalizers</span><br><span class="line">    finalizer()</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/multiprocessing/util.py&quot;</span>, line 224, <span class="keyword">in</span> __call__</span><br><span class="line">    res = self._callback(*self._args, **self._kwargs)</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/multiprocessing/util.py&quot;</span>, line 133, <span class="keyword">in</span> _remove_temp_dir</span><br><span class="line">    rmtree(tempdir)</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/shutil.py&quot;</span>, line 734, <span class="keyword">in</span> rmtree</span><br><span class="line">    _rmtree_safe_fd(fd, path, onerror)</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/shutil.py&quot;</span>, line 690, <span class="keyword">in</span> _rmtree_safe_fd</span><br><span class="line">    onerror(os.unlink, fullname, sys.exc_info())</span><br><span class="line">  File <span class="string">&quot;/work/&lt;USER&gt;/anaconda3/envs/ROMP/lib/python3.9/shutil.py&quot;</span>, line 688, <span class="keyword">in</span> _rmtree_safe_fd</span><br><span class="line">    os.unlink(entry.name, dir_fd=topfd)</span><br></pre></td></tr></table></figure></li><li><p><code>.nfs&lt;xxxxxxxxxx&gt;</code>后面的这一串符号会不断变化，其他报错信息基本稳定。</p></li></ul><h2 id="解决">解决</h2><ul><li><p>经过查询得到，OSError [Errno 16] Device or resource busy 往往是某个资源正在被其他程序访问。然而这里的“资源”都是<code>multiprocessing</code>库用来在进程间交流信息的临时文件。</p></li><li><p>当该错误和<code>.nfs&lt;xxxxxxx&gt;</code>一起出现时，表明该资源位于一个网络位置上。这里是因为集群的储存都是以挂载的网络位置方式进行的。</p></li><li><p>经过一番Google，最初认为可能原因和某几个帖子一样，是disk quota已经用光了，所以会无法新建文件等，所以我先删除了一波东西，然而error依旧，且往各个位置下载大文件也并不会报类似的错误，所以暂时认为不是这个原因。</p></li><li><p>最终解决方案是，因为这些文件都是临时文件，所以问题一定是出在了储存临时文件的位置处，大概率是集群本身的问题（集群还是挺经常出各种奇怪问题的），所以只需要在程序开始处重新指定临时文件存储目录即可。</p></li><li><p>值得注意的是，跑代码需要在GPU node上跑，每个node都有自己的临时文件文件夹（估计是为了加速，把cache存到离本机最近的物理储存位置），所以更改的目标位置可以就设为<code>/tmp</code>。</p></li><li><p>具体代码如下（Python）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line">tempfile.tempdir = <span class="string">&#x27;/tmp&#x27;</span></span><br></pre></td></tr></table></figure><p>注意需要将这两行代码放到程序入口文件的最上方。如果你的暂存文件夹本来就是<code>/tmp</code>，换成任意一个你喜欢的路径即可。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;运行环境&quot;&gt;运行环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SLURM 深度学习集群。&lt;/li&gt;
&lt;li&gt;系统：Ubuntu 20.04.5 LTS&lt;/li&gt;
&lt;li&gt;造成BUG的程序：深度学习训练代码。&lt;/li&gt;
&lt;li&gt;具体造成BUG的库：python的&lt;code&gt;multiprocessing&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="server" scheme="https://www.miracleyoo.com/tags/server/"/>
    
    <category term="slurm" scheme="https://www.miracleyoo.com/tags/slurm/"/>
    
    <category term="bug-report" scheme="https://www.miracleyoo.com/tags/bug-report/"/>
    
  </entry>
  
  <entry>
    <title>Linux(Ubuntu)装机与配置笔记</title>
    <link href="https://www.miracleyoo.com/2022/04/11/linux-setup/"/>
    <id>https://www.miracleyoo.com/2022/04/11/linux-setup/</id>
    <published>2022-04-12T01:11:47.000Z</published>
    <updated>2023-04-23T01:48:26.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘相关">硬盘相关</h2><ol type="1"><li><p><strong>df命令</strong> <code>df</code>：检查linux服务器的文件系统的磁盘空间占用情况。<strong>它只会显示已经挂载的磁盘信息！</strong></p><p><code>df -h</code>, 即<code>--human-readble</code>：以1024的倍数的方式显示大小。(e.g., 1023M)</p><p><code>df -T</code>：查看所有磁盘的文件系统类型(type)</p><span id="more"></span></li><li><p><strong><code>fdisk</code>命令</strong></p><p><code>fdisk</code>：强大的磁盘监视和操作工具。</p><p><code>fdisk -l</code>会显示<strong>所有的</strong>磁盘和分区！不论有没有挂载，都会被列出来。</p></li><li><p><strong>mount命令</strong></p><p><code>mount</code>：挂载一个文件系统</p><p><code>mount -t ntfs &lt;source&gt; &lt;target&gt;</code>：以ntfs文件系统的形式从源目录挂载到目标目录。t表示types类型</p><p><code>mount -a</code>：挂载 fstab 中的所有文件系统。a表示all</p></li><li><p><strong>blkid命令</strong></p><p><code>sudo blkid</code>：获取各个分区的UUID和分区类型TYPE</p></li><li><p>物理磁盘与磁盘分区：一个物理磁盘在<code>fdisk -l</code>中的显示往往类似于<code>/dev/sda</code>，<code>/dev/sdb</code>，<code>/dev/nvme0n1</code>。一般情况下是不带数字的，sda sdb是最常见的命名。而分区命名则是如：<code>/dev/sda1</code>，<code>/dev/sdb2</code>之类在物理磁盘的后面带上数字表示分区编号。</p><p>但有些如双系统中，可能会出现最后一个例子中展示的命名，这种磁盘的分区则是以<code>p[x]</code>结尾，如<code>/dev/nvme0n1p1</code>，<code>/dev/nvme0n1p9</code>。</p></li><li><p>Linux开机后不会自动挂载Windows文件格式NTFS的磁盘。</p></li><li><p><code>sudo chmod -R 777 &lt;Folder_Name&gt;</code> 可以取消一个文件夹的全部访问权限。</p></li><li><p><code>chmod</code>命令对ext3/4文件系统，即Linux格式的文件系统才有效，对其他文件系统，如vfat(Fat32)，NTFS都是无效的。</p></li><li><p>/etc/fstab` 文件是掌管硬盘自动挂载配置的文件，包含自动挂载分区过程的必要信息。每一条记录格式如下：</p><p><code>[Device] [Mount Point] [File System Type] [Options] [Dump] [Pass]</code></p><p>如：</p><p><code>UUID=B45A01D55A019570 /data ntfs defaults 0 2</code></p><p>其中：</p><p><code>[Options]</code> ：<code>defaults</code>表示用默认的<code>rw, suid, dev, exec, auto, nouser, async</code>等选项（不同内核和文件系统不同）进行挂载，这些选项的含义：<code>rw</code> 可读写；<code>suid</code> 执行程序时遵守<code>uuid</code>；<code>dev</code> 解释字符或禁止特殊设备；<code>exec</code> 允许执行二进制文件；<code>auto</code> 可以<code>-a</code>方式加载；<code>nouser</code> 禁止普通用户挂载此文件系统；<code>async</code> 所有I/O异步完成。</p><p><code>[Dump]</code> ：是否开启分区备份，0表示关闭</p><p><code>[Pass]</code>：系统启动时检查分区错误的顺序，root为1，其他为2，0为不检查。</p></li><li><p>在<code>fstab</code>文件中添加记录前一定要先尝试用mount命令手动挂载。</p></li></ol><h3 id="参考">参考</h3><ol type="1"><li><a href="https://blog.csdn.net/qxqxqzzz/article/details/89790688">Ubuntu18.04 开机自动挂载其他硬盘</a></li><li><a href="https://blog.csdn.net/ybdesire/article/details/79145180">Linux查看与挂载新磁盘</a></li></ol><h2 id="cuda的安装">CUDA的安装</h2><ol type="1"><li><p>这里是官方<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#environment-setup">指南</a>，注意使用左侧的导航链接，以免翻看中看漏信息。下载链接在<a href="https://developer.nvidia.com/cuda-downloads">这里</a>。</p></li><li><p>检查自己的GPU是否是CUDA-capable，在终端中输入<code>lspci | grep -I NVIDIA</code> ，会显示自己的NVIDIA GPU版本信息，去CUDA的官网查看自己的GPU版本是否在CUDA的支持列表中。</p></li><li><p>检查自己的Linux版本是否支持 CUDA（Ubuntu 稳定支持版没问题）。</p></li><li><p>检查其他问题。这里就不详述了，正常情况下一般OK，这里主要要检查是否安装了<code>gcc</code>，是否安装了<code>kernel header</code>和 <code>package development</code>。如果害怕出现问题可以参考<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">官网</a>执行这几步检测。</p></li><li><p>于<a href="https://developer.nvidia.com/cuda-downloads">CUDA官网</a>下载与系统对应的CUDA版本。最后一个选项选择<code>runfile</code>，因为其所需步骤最少，也因此最不容易出问题。所有选项完成后，你会看到如下两行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run</span><br><span class="line">sudo sh cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure><p>先不要执行第二条<code>sudo</code>开头的指令，只使用<code>wget</code>下载。</p></li><li><p>如果之前有安装过其他版本的CUDA并希望将其卸载，使用<code>sudo nvidia-uninstall</code>卸载。如果该命令不在系统路径中，则使用<code>sudo /usr/bin/nvidia-uninstall</code>（位置可能变化）卸载。如果还是没有，或是之前的驱动已经损坏，则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br><span class="line">sudo <span class="built_in">chmod</span> +x NVIDIA-Linux-x86_64-410.93.run</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-410.93.run --uninstall</span><br></pre></td></tr></table></figure></li><li><p>屏蔽<code>nouveau</code>驱动。</p></li></ol><h3 id="nouveau是什么">Nouveau是什么</h3><blockquote><h4 id="nouveau-accelerated-open-source-driver-for-nvidia-cards">Nouveau: Accelerated Open Source driver for nVidia cards</h4><p>The <strong>nouveau</strong> project aims to build high-quality, free/libre software drivers for <a href="https://nouveau.freedesktop.org/wiki/CodeNames/">nVidia cards</a>. “Nouveau” [<em>nuvo</em>] is the French word for “new”. Nouveau is composed of a Linux kernel KMS driver (nouveau), Gallium3D drivers in Mesa, and the Xorg DDX (xf86-video-nouveau). The kernel components have also been ported to <a href="https://nouveau.freedesktop.org/wiki/NetBSD/">NetBSD</a>.</p></blockquote><p>简单说，nouveau是Linux系统默认的给NVIDIA卡预装的一个图形加速驱动，而这个驱动会与CUDA产生部分冲突，所以在安装CUDA之前需要将其禁用，否则会出现卡在开机登录界面无法进入图形界面（仍然可以ssh访问），黑屏，鼠标键盘输入被禁用等问题中的一个或多个（亲身经历）。</p><p>继续安装教程：</p><ol start="6" type="1"><li><p>刚才说到要屏蔽<code>nouveau</code>，那么怎么知道你有没有装它呢？ 使用<code>lsmod | grep nouveau</code>命令，如果没有输出，就可以判定你没有运行<code>nouveau</code>，可以直接进入下一步，否则：</p><ol type="1"><li><p>Create a file at <code>/etc/modprobe.d/blacklist-nouveau.conf</code> with the following contents:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure></li><li><p>Regenerate the kernel initramfs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure></li><li><p>Restart.</p></li><li><p>Run <code>lsmod | grep nouveau</code> again. If there is no output, then you succeed.</p></li></ol></li><li><p>此后建议进入一个非图形界面安装，这里可以在重启后使用<code>ssh</code>接入，也可以在重启后按<code>alt+ctrl+f1</code>，进入<strong>text mode</strong>，登录账户。</p></li><li><p>输入 <code>sudo service lightdm stop</code> 关闭图形化界面。</p></li><li><p>执行刚才官网中给出的第二条命令：<code>sudo sh cuda_10.2.89_440.33.01_linux.run</code>。注意这里的版本会不断有变化。注意这里有一个点，即你是否要同时安装OpenGL，如果你是双显，且主显是非NVIDIA的GPU需要选择no，否则yes。同理，如果准备选no，也可以一开始就加上参数<code>--no-opengl-files</code>。 另外，如果不能直接执行，使用<code>sudo chmod a+x cuda_xx.xx.xx_linux.run</code>为其赋权。</p></li><li><p>安装成功后，会提示你将cuda的几个路径添加到系统路径中，这里重复一下，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/NsightCompute-2019.1<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>使用<code>nvcc -V</code>检测是否安装成功。当然也可以同时测试<code>nvidia-smi</code>。这里可能会报错并提示需要apt安装一个包，按提示来。</p></li><li><p>此时可能要再安装一个nvidia driver，具体适配版本可以在<a href="https://www.nvidia.com/download/index.aspx">这里</a>找到。</p></li><li><p>reboot，此时你可能会发现GUI的登录界面消失了，你面对的只有一个空白的壁纸/纯色。不要慌张，只要切换一下display manager就好了。如果你默认的是<code>gbm3</code> display manager，那最简单的方法就是直接安装<code>lightdm</code>并切换过去就好了。</p><p>首先按<code>CTRL+ALT+F5</code>进入纯命令行模式，然后输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install lightdm</span><br><span class="line">sudo dpkg-reconfigure lightdm</span><br></pre></td></tr></table></figure><p>如果提示服务没起来，用<code>sudo systemctl start lightdm</code>。</p></li><li><p>完成。</p></li></ol><h2 id="nvidia-driver-的安装与使用">NVIDIA Driver 的安装与使用</h2><p>这里需要澄清一个问题，即NVIDIA Driver，NVIDIA utils (390), CUDA 之间的关系。NVIDIA Driver是需要单独安装的，它与Nouveau是并列关系。系统默认的<strong>显卡驱动</strong>是后者。utils 是服务于Driver的，比如<code>nvidia-smi</code>, <code>nvidia-settings</code>等就是utils的组件。而CUDA是建构在Driver之上的一个服务于深度学习程序的<strong>指令转换器</strong>，所以要现有前两者，CUDA才能正常运行。</p><p>安装Driver有两种方法，一种是在GUI中的<strong>Software &amp; Updates</strong>中选择Additional Drivers中的第一行<strong>Proprietary, tested</strong>的那个NVIDIA Driver选项，然后Apply Changes即可。</p><figure><img data-src="ubuntu-18.04-nvidia-drivers-430.png" alt="ubuntu 18.04 nvidia-drivers 430"><figcaption aria-hidden="true">ubuntu 18.04 nvidia-drivers 430</figcaption></figure><p>也可以在命令行中用<code>sudo ubuntu-drivers autoinstall</code> 选项来进行安装。这里就不展开了，<a href="https://www.linuxbabe.com/ubuntu/install-nvidia-driver-ubuntu-18-04">这篇文章</a>讲的非常详细。</p><h3 id="error-shooting">Error Shooting</h3><ul><li>如果系统分辨率突然被固定为了一个很小的数值，且无法调整，请尝试<code>nvcc --version</code>和<code>nvidia-smi</code>，往往是由于CUDA相关的两个路径没有很好的添加到正确的位置。如果想保险，直接在<code>~/.bashrc</code>或<code>~/.zshrc</code>中添加那两行<code>export</code>命令，然后重启。</li><li>如果你电脑物理连接了两个显示器，而你登录后发现桌面是空的，也没有Dock，什么都没有，有可能是他们在另一个显示器上，即使你没打开那个显示器。</li></ul><h3 id="参考-1">参考</h3><ol type="1"><li><a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA下载官网</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">NVIDIA 官方安装指南（英文）</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">NVIDIA 官方安装指南中前置检查部分</a></li><li><a href="https://www.pugetsystems.com/labs/hpc/How-To-Install-CUDA-10-together-with-9-2-on-Ubuntu-18-04-with-support-for-NVIDIA-20XX-Turing-GPUs-1236/">How To Install CUDA 10 (together with 9.2) on Ubuntu 18.04 with support for NVIDIA 20XX Turing GPUs</a></li><li><a href="https://blog.csdn.net/lipi37/article/details/90407099">Ubuntu 安装 cuda 时卡在登录界面（login loop)的解决方案之一</a></li><li><a href="https://blog.csdn.net/wkk15903468980/article/details/56489704">ubuntu安装cuda循环登录</a></li><li><a href="https://blog.csdn.net/qq_33200967/article/details/80689543">Ubuntu安装和卸载CUDA和CUDNN</a></li><li><a href="https://blog.csdn.net/wf19930209/article/details/81879514">Linux安装CUDA的正确姿势</a></li></ol><h2 id="cuda-与-cudnn-的联系">CUDA 与 CUDNN 的联系</h2><ol type="1"><li>要先装CUDA再装CUDNN。</li><li>前者是平台，后者是基于平台的深度学习加速器。加速可以应用于几乎全部深度学习平台。还是要安的。</li><li>一般深度学习使用安装runtime版本即可。</li><li><a href="https://developer.nvidia.com/rdp/cudnn-download">CUDNN官方下载</a>，<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html">CUDNN官方安装步骤</a></li></ol><h2 id="修复ubuntu中检测到系统程序错误的问题">修复Ubuntu中“检测到系统程序错误”的问题</h2><h3 id="问题描述">问题描述</h3><p>每次开机时都会有“<strong>Ubuntu xx.xx 在启动时检测到系统程序错误</strong> ”弹窗出现。即使点击报告下次还会继续出现。</p><h3 id="问题来源">问题来源</h3><p>之前的某个时刻某个程序崩溃了，而Ubuntu想让你决定要不要把这个问题报告给开发者，这样他们就能够修复这个问题。</p><h3 id="解决办法">解决办法</h3><ol type="1"><li><code>sudo rm /var/crash/*</code> ：删除这些错误报告。但是如果又有一个程序崩溃了，你就会再次看到“检测到系统程序错误”的错误。你可以再次删除这些报告文件，或者选择禁用Apport来彻底地摆脱这个错误弹窗。如果你这样做，系统中任何程序崩溃时，系统都不会再通知你。但这未必一件坏事，除非你愿意填写错误报告。如果你不想填写错误报告，那么这些错误通知存不存在都不会有什么区别。</li><li><code>sudo vim /etc/default/apport</code> 永久屏蔽这些报错。</li></ol><h3 id="参考-2">参考</h3><ol type="1"><li><a href="https://blog.csdn.net/hywerr/article/details/72582082">如何修复ubuntu中检测到系统程序错误的问题</a></li><li><a href="https://itsfoss.com/how-to-fix-system-program-problem-detected-ubuntu/">How To Fix System Program Problem Detected In Ubuntu</a></li></ol><h2 id="安装python3.6版本的anaconda">安装Python3.6版本的Anaconda</h2><p>由于之前使用的一些开源库和软件对3.7的支持性尚还有问题，而Anaconda默认Python版本为3.6， 所以有必要把Anaconda降级为3.6版本。</p><p>安装方法：</p><ol type="1"><li><p>到Anaconda官网下载并安装最新3.7版本。</p></li><li><p>世界线开始分歧，你可以选择保留3.7版本的Anaconda，并创建一个虚拟环境，或是直接替换Python版本。</p><ol type="1"><li><p>对前者， 若只要一个python环境不要packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name ana36 python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure><p>反之，如果要安装一个新的Anaconda，包含默认的所有packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n ana36 anaconda python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure></li><li><p>对后者，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python=3.6</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="添加vim拷贝至系统剪贴板快捷键支持">添加Vim拷贝至系统剪贴板快捷键支持</h2><p>(from: <a href="http://vim.wikia.com/wiki/Mac_OS_X_clipboard_sharing">link</a>)</p><p>Having trouble copying selected text from Vim (not MacVim)? Since using <code>"+y</code> or '"*y' in Vim on a Mac doesn't actually copy the selected text to the system clipboard, you might find it beneficial to do the following:</p><ol type="1"><li>Open your <code>~/.vimrc</code> file</li><li>add <code>vmap '' :w !pbcopy</code></li><li>Save it and <code>source</code> the file</li></ol><p>现在，你就可以在 visual mode， 即在Esc命令模式后按下v键后的选择模式中，选好需要拷贝区域后，连击两次<code>'</code> ，即使用 <code>''</code>来拷贝所选区域。</p><h2 id="在maclinux上使用ssh挂载远程网络硬盘">在Mac/Linux上使用ssh挂载远程网络硬盘</h2><p>TL;DR：</p><ol type="1"><li>安装sshfs: <code>sudo apt-get install sshfs</code></li><li>直接在<code>~/.zshrc</code>中添加以下行：（当然，需要更改文件夹名称，以及挂载后的命名）</li></ol><h3 id="连接本地linux-server">连接本地Linux Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_misaka</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-home&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">mkdir</span> /Volumes/misaka-home</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/home/miracle /Volumes/misaka-home/ -ovolname=mk-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-storage&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">mkdir</span> /Volumes/misaka-storage</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/data /Volumes/misaka-storage/ -ovolname=mk-2T</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="连接gypsum">连接Gypsum</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_gypsum</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">mkdir</span> /Volumes/gypsum</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/home/zhongyangzha /Volumes/gypsum/ -ovolname=gp-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-scratch/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">mkdir</span> /Volumes/gypsum-scratch/</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/scratch1/zhongyangzha/ /Volumes/gypsum-scratch/ -ovolname=gp-scratch</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-work/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">mkdir</span> /Volumes/gypsum-work</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/work1/trahman/zhongyangzha /Volumes/gypsum-work/ -ovolname=gp-work</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数解释">参数解释</h3><ol type="1"><li><code>ovolname</code>：挂载上网络硬盘之后硬盘的命名</li><li><code>IdentityFile</code>：如果已经设置了免密登录，用这个参数指明ssh私钥位置即可，不需要输入密码。</li><li><code>&lt;source&gt; &lt;target&gt;</code>：网络硬盘源位置&lt;username@ip.address:/the/source/path&gt; 与本机目标挂载位置</li><li><code>reconnect,ServerAliveInterval=15,ServerAliveCountMax=3</code>：多次断线重连，可以再断开网络连接、服务器重启等问题发生后再次自动连接。</li></ol><h2 id="硬盘相关-1">硬盘相关</h2><h3 id="硬盘信息查看三幻神">硬盘信息查看三幻神</h3><ol type="1"><li><code>sudo fdisk -l</code>: 最为详细全面的硬盘信息，既包含了已挂载的盘，也包含了未挂载的盘。其含有磁盘在<code>/dev</code>中的位置，size（非常详细，既有易于阅读的单位MB/GB等，也有精确到字节和扇区的信息），硬盘型号，disklabel型号（GPT DOS）等。</li><li><code>df -h</code>: 含有所有当前已挂载的磁盘的信息，包括<code>/dev</code>位置，总空间，已用空间和可用空间，利用率，以及挂载点位置。</li><li><code>sudo blkid</code>: 全部（已挂载+未挂载）的硬盘信息，包括<code>/dev</code>位置，文件系统类型（ntfs，ext4，fat32等），Label（就是Windows上写的卷名），UUID，以及Partition UUID。</li></ol><h3 id="为什么有的硬盘开机就自动挂载了而另一些则没有">为什么有的硬盘开机就自动挂载了，而另一些则没有</h3><ol type="1"><li>在我遇到的情况里，什么都不用做就会自动挂载的是外置的USB盘符，它们其实是开机之后被一个个load起来的。而装在主机里面的那些硬盘，则需要额外进行设置以便能开机自动挂载。</li><li>设置方法：<code>sudo vim /etc/fstab</code>进行编辑即可。每个想要挂在的盘各自写一行，每行包括设备UUID，挂载点，文件系统，以及后面不怎么用改的三个参数（用 <code>default 0 0</code>填充即可）。这些信息的查看方法在上个小节有提。</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;硬盘相关&quot;&gt;硬盘相关&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;df命令&lt;/strong&gt; &lt;code&gt;df&lt;/code&gt;：检查linux服务器的文件系统的磁盘空间占用情况。&lt;strong&gt;它只会显示已经挂载的磁盘信息！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df -h&lt;/code&gt;, 即&lt;code&gt;--human-readble&lt;/code&gt;：以1024的倍数的方式显示大小。(e.g., 1023M)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df -T&lt;/code&gt;：查看所有磁盘的文件系统类型(type)&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="ssh" scheme="https://www.miracleyoo.com/tags/ssh/"/>
    
    <category term="net-disk" scheme="https://www.miracleyoo.com/tags/net-disk/"/>
    
    <category term="cuda" scheme="https://www.miracleyoo.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>blender-syn-data-gen-tutorial</title>
    <link href="https://www.miracleyoo.com/2021/11/27/blender-syn-data-gen-tutorial/"/>
    <id>https://www.miracleyoo.com/2021/11/27/blender-syn-data-gen-tutorial/</id>
    <published>2021-11-28T02:10:03.000Z</published>
    <updated>2023-04-23T01:10:48.698Z</updated>
    
    <content type="html"><![CDATA[<h1 id="blender-tutorial-on-generating-synthetic-data">Blender Tutorial on Generating Synthetic Data</h1><h2 id="how-to-make-an-ok-mmd-video">How to make an OK MMD video</h2><ul><li>MikuMikuDance (MMD) is a simple and powerful tool, as we used in the previous paper. However, users cannot easily inject scripts to access the inner variables. Also, with respect to the rendering, it has a physical limitation of 60 FPS, which restricts the direct simulation of DVS sensors. Lastly, there are many processes in the simulation loop where manual adjustment is forced, which limit the scalability. Therefore, we choose another route that concentrates on Blender and tries to rule out manual participation to the largest extent.</li></ul><span id="more"></span><ul><li><p>First, as the <code>.vmd</code> files distributed on the internet usually only contain the joints' movement and no mesh physics included, we will have a hard time dealing with the clothes materials, and physics in Blender, as Blender doesn't automatically use a preset for these factors like MMD. Therefore, we'd better take care of them in advance in MMD.</p></li><li><p>Here we are going to use a “successor” software of MMD, which is called MikuMikuMotion (MMM). Import your <code>.pmx</code> and <code>.vmd</code> files into MMM, and bake the physical mesh movement to a separate <code>.vmd</code> file. Turn the <code>Always</code> button on in the <code>Physics</code> Tab to make sure you calculate the physics for each frame, then click <code>Record</code> to bake your movement when everything is ready.</p><p><img data-src="image-20220714212046705.png" alt="image-20220714212046705" style="zoom:50%;"></p></li><li><p>Then dump your recorded motion to another <code>.vmd</code> file:</p><p><img data-src="image-20220714212445071.png" alt="image-20220714212445071" style="zoom:50%;"></p></li><li><p>Ok, then let's switch to Blender.</p></li><li><p>You have to install a plugin <a href="https://github.com/powroupi/blender_mmd_tools">mmd_tools</a> for Blender at first. It will help you to import your <code>.pmx</code> model files and <code>.vmd</code> motion files, as Blender doesn't support these file types natively. If you install it successfully, you will be able to see a panel like this on your sidebar:</p><p><img data-src="image-20220714215023678.png" alt="image-20220714215023678" style="zoom:50%;"></p></li><li><p>Click <code>Import</code>, then you could import your model that is used for recording in MMM to Blender. Then, make sure you clicked and selected the model that you just imported, and click the <code>Import</code> button under the tab <code>Motion</code> to import the baked <code>.vmd</code> motion file you get from MMM.</p></li><li><p>After that, you may notice that the skin color of your model looks magenta, then you have to follow the following step to repair that:</p><p><img data-src="image-20220714215437237.png" alt="image-20220714215437237" style="zoom:60%;"></p></li><li><p>If you also have a camera's trajectory, please click on the camera in the explorer and click <code>Motion-&gt;Import</code> in <code>MMD</code> side panel to select your <code>.vmd</code> camera motion file and import it.</p><p><img data-src="image-20220715103036422.png" alt="image-20220715103036422" style="zoom:50%;"></p></li><li><p>Then, it's time to set the lighting. Well, honestly speaking, I don't really care too much about lighting, as the requirement of generating synthetic data is not that high, so let's simply light everything up.</p><p><img data-src="image-20220714220719987.png" alt="image-20220714220719987" style="zoom:50%;"></p></li><li><p>We are almost there, and next, setting the output parameters: Remember to change your resolution, and set your output path as well as the file format. Here the resolution 346x260 is the resolution for event camera DAVIS346, which is the camera we are currently using.</p><p><img data-src="image-20220714221006003.png" alt="image-20220714221006003" style="zoom:50%;"></p></li><li><p>Next, let's set some render properties: For the render engine, you can use either <code>Eevee</code> or <code>Cycles</code>, but if you choose <code>Cycles</code>, remember to turn on the <code>GPU</code> rendering option under it to speed up. Also, if you want the background of your video to be pure white/transparent/other pure color, please check the <code>Tranparent</code> in <code>Film</code> section. Moreover, make sure to change the <code>View Transform</code> from <code>Filmic</code> to <code>Standard</code>, the color looks more <em>Real</em> that way.</p><p><img data-src="image-20220714222126060.png" alt="image-20220714222126060" style="zoom:50%;"></p></li><li><p>Lastly, continuing from the last point, if you want to have a pure color background, then you have to add a <em>Background</em> here: Go to the <code>Compositing</code> Tab on the top bar, then add an <code>Alpha Over</code> node and drag it between the existing two nodes. Connect it as the figure shows, then set the first <code>Image</code> with the pure color you desire. All set. (Notice, if there is nothing in this board, click the <code>Use Nodes</code> on the inner menu row, near the <code>Add</code> and <code>Nodes</code> to show the basic panels.)</p><p><img data-src="image-20220714222651269.png" alt="image-20220714222651269" style="zoom:50%;"></p></li><li><p>Now everything is prepared, let's render the frame/video. Click on the <code>Render</code> menu, then choose render frame/video, you can then find your video in the output folder you set before.</p><p><img data-src="image-20220714223147763.png" alt="image-20220714223147763" style="zoom:50%;"></p></li></ul><h2 id="generate-mask-video">Generate Mask Video</h2><ul><li><p>One more thing, if you want to dump the human mask, simple change the wires in <code>Composition</code> Panel is fine.</p><figure><img data-src="image-20220714223747030.png" alt="image-20220714223747030"><figcaption aria-hidden="true">image-20220714223747030</figcaption></figure></li></ul><h2 id="generate-depth-video">Generate Depth Video</h2><ul><li><p>If you even want to have the depth video, it's still easy enough: add another node in <code>Compositing</code> Tab to map a certain range of depth to <code>[0,1]</code>.</p><p><img data-src="image-20220714225113714.png" alt="image-20220714225113714" style="zoom:50%;"></p></li><li><p>You need to tweak this range carefully based on your model's actual moving range.</p></li></ul><h2 id="better-lighting-settings-like-default-material-preview">Better Lighting Settings (Like Default Material Preview)</h2><ul><li><p>You may notice that the default material preview looks great, but it's not easy to imitate that effect. Here let's take a step to try to reach a similar level of the effect by applying skybox texture.</p></li><li><p>In the beginning, you should understand what <code>Skybox</code> is used for. Skybox is used as a sort of <strong>Directional Colored Lighting</strong> in all directions. Imagine that you are surrounded by a certain environment, and the objects in each direction could reflect the light of their own color. This is what the sky box is used for. Compared to the <code>Sky box</code>, actually, it's more like a <code>sky sphere</code>, which is a sphere around you. You could control the <code>Strength</code> of the sky box, just like you modify the lighting strength.</p></li><li><p>First, let's enable a built-in plugin called <code>Node Wrangler</code> which could enable us to quickly add texture-related pipelines. Go to <code>Edit-&gt;Preference-&gt;Add-ons</code>.</p><p><img data-src="image-20220715154232096.png" alt="image-20220715154232096" style="zoom:50%;"></p></li><li><p>Modify the <code>Shader Editor</code> and switch the <code>Object</code> to <code>World</code> on its right. Also, make sure you turned <code>Use Nodes</code> option on. Then you'll be able to modify the environmental settings, like the sky box around the world. This is an informative figure. You should first select the <code>Node Groupe</code> which contains <code>Background</code> output port.</p><p><img data-src="image-20220715155114990.png" alt="image-20220715155114990" style="zoom:50%;"></p></li><li><p>Then, press <code>CTRL+T</code> to instantiate the three boxes on its left together. Next, connect the <code>General</code> option to <code>Vector</code>, and set your sky boxes' rotation angles. Usually, if you don't know how to rotate and just want to rotate like you could do in <code>Material Preview</code> mode, please make sure that you only rotate on the Z axis.</p></li><li><p>Then, it's time to load your actual sky box file. Go to your installation directory of Blender, and go to <code>&lt;Blender Installation Folder&gt;\2.93\datafiles\studiolights\world</code> directory, you can find all the default sky boxes Blender uses in its <code>Material Preview</code> mode. For me, the installation folder is at <code>C:\Program Files\Blender Foundation\Blender 2.93</code>, which is the default location.</p></li><li><p>Lastly, if you want to change the lighting strength, make sure to modify the <code>Strength</code> option in the <code>Node Group</code> box. Here I set it to 2.</p></li><li><p>Still, you may need to have a default sunlighting for a better 3D effect.</p></li><li><p>Done!</p></li></ul><h2 id="script-your-operations-and-repeat">Script Your Operations and Repeat!</h2><ul><li><p>Blender has a wonderful feature: it will record and tell you the python command equivalent for all the operations you take! Therefore, if you have done a pipeline correctly, you could simply copy all these commands and make them into a script easily. You could found the console here:</p><p><img data-src="image-20220714233705944.png" alt="image-20220714233705944" style="zoom:50%;"></p></li><li><p>You can even select multiple lines and copy them into corresponding python code blocks, which is an amazing function.</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;blender-tutorial-on-generating-synthetic-data&quot;&gt;Blender Tutorial on Generating Synthetic Data&lt;/h1&gt;
&lt;h2 id=&quot;how-to-make-an-ok-mmd-video&quot;&gt;How to make an OK MMD video&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MikuMikuDance (MMD) is a simple and powerful tool, as we used in the previous paper. However, users cannot easily inject scripts to access the inner variables. Also, with respect to the rendering, it has a physical limitation of 60 FPS, which restricts the direct simulation of DVS sensors. Lastly, there are many processes in the simulation loop where manual adjustment is forced, which limit the scalability. Therefore, we choose another route that concentrates on Blender and tries to rule out manual participation to the largest extent.&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="blender" scheme="https://www.miracleyoo.com/tags/blender/"/>
    
    <category term="MMD" scheme="https://www.miracleyoo.com/tags/MMD/"/>
    
    <category term="data-generation" scheme="https://www.miracleyoo.com/tags/data-generation/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Lighting 完全攻略</title>
    <link href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/"/>
    <id>https://www.miracleyoo.com/2021/03/11/pytorch-lightning/</id>
    <published>2021-03-12T04:09:57.000Z</published>
    <updated>2021-03-12T22:05:55.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h2><p>Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？</p><span id="more"></span><p>于是我第二次发现了Pytorch-Lightning。</p><p>真香。</p><p>但是问题还是来了。这个框架并没有因为香而变得更加易学。官网的教程很丰富，可以看出来开发者们在努力做了。但是很多相连的知识点都被分布在了不同的版块里，还有一些核心的理解要点并没有被强调出来，而是小字带过，这让我想做一个普惠的教程，包含所有我在学习过程中认为重要的概念，好用的参数，一些注意点、坑点，大量的示例代码段和一些核心问题的集中讲解。</p><p>最后，第三部分提供了一个我总结出来的易用于大型项目、容易迁移、易于复用的模板，有兴趣的可以去<a href="https://github.com/miracleyoo/pytorch-lightning-template">GitHub</a>试用。</p><h2 id="crucial"><a class="markdownIt-Anchor" href="#crucial"></a> Crucial</h2><ul><li><p>Pytorch-Lighting 的一大特点是把模型和系统分开来看。模型是像Resnet18， RNN之类的纯模型， 而系统定义了一组模型如何相互交互，如GAN（生成器网络与判别器网络）、Seq2Seq（Encoder与Decoder网络）和Bert。同时，有时候问题只涉及一个模型，那么这个系统则可以是一个通用的系统，用于描述模型如何使用，并可以被复用到很多其他项目。</p></li><li><p>Pytorch-Lighting 的核心设计思想是“自给自足”。每个网络也同时包含了如何训练、如何测试、优化器定义等内容。</p></li></ul><p><img data-src="plres.png" alt="img"></p><h2 id="推荐使用方法"><a class="markdownIt-Anchor" href="#推荐使用方法"></a> 推荐使用方法</h2><p>这一部分放在最前面，因为全文内容太长，如果放后面容易忽略掉这部分精华。</p><p>Pytorch-Lightning 是一个很好的库，或者说是pytorch的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。缺点也很明显，这个包需要学习和理解的内容还是挺多的，或者换句话说，很重。如果直接按照官方的模板写代码，小型project还好，如果是大型项目，有复数个需要调试验证的模型和数据集，那就不太好办，甚至更加麻烦了。经过几天的摸索和调试，我总结出了下面这样一套好用的模板，也可以说是对Pytorch-Lightning的进一步抽象。</p><p>欢迎大家尝试这一套代码风格，如果用习惯的话还是相当方便复用的，也不容易半道退坑。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">|-data</span><br><span class="line">|-__init__.py</span><br><span class="line">|-data_interface.py</span><br><span class="line">|-xxxdataset1.py</span><br><span class="line">|-xxxdataset2.py</span><br><span class="line">|-...</span><br><span class="line">|-model</span><br><span class="line">|-__init__.py</span><br><span class="line">|-model_interface.py</span><br><span class="line">|-xxxmodel1.py</span><br><span class="line">|-xxxmodel2.py</span><br><span class="line">|-...</span><br><span class="line">|-main.py</span><br></pre></td></tr></table></figure><p>如果对每个模型直接上plmodule，对于已有项目、别人的代码等的转换将相当耗时。另外，这样的话，你需要给每个模型都加上一些相似的代码，如<code>training_step</code>，<code>validation_step</code>。显然，这并不是我们想要的，如果真的这样做，不但不易于维护，反而可能会更加杂乱。同理，如果把每个数据集类都直接转换成pl的DataModule，也会面临相似的问题。基于这样的考量，我建议使用上述架构：</p><ul><li><p>主目录下只放一个<code>main.py</code>文件。</p></li><li><p><code>data</code>和<code>modle</code>两个文件夹中放入<code>__init__.py</code>文件，做成包。这样方便导入。两个<code>init</code>文件分别是：</p><ul><li><code>from .data_interface import DInterface</code></li><li><code>from .model_interface import MInterface</code></li></ul></li><li><p>在<code>data_interface </code>中建立一个<code>class DInterface(pl.LightningDataModule):</code>用作所有数据集文件的接口。<code>__init__()</code>函数中import相应Dataset类，<code>setup()</code>进行实例化，并老老实实加入所需要的的<code>train_dataloader</code>, <code>val_dataloader</code>, <code>test_dataloader</code>函数。这些函数往往都是相似的，可以用几个输入args控制不同的部分。</p></li><li><p>同理，在<code>model_interface </code>中建立<code>class MInterface(pl.LightningModule):</code>类，作为模型的中间接口。<code>__init__()</code>函数中import相应模型类，然后老老实实加入<code>configure_optimizers</code>, <code>training_step</code>, <code>validation_step</code>等函数，用一个接口类控制所有模型。不同部分使用输入参数控制。</p></li><li><p><code>main.py</code>函数只负责：</p><ul><li>定义parser，添加parse项。</li><li>选好需要的<code>callback</code>函数们。</li><li>实例化<code>MInterface</code>, <code>DInterface</code>, <code>Trainer</code>。</li></ul><p>完事。</p></li></ul><h2 id="lightning-module"><a class="markdownIt-Anchor" href="#lightning-module"></a> Lightning Module</h2><h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">主页面</a></p><ul><li><p>三个核心组件：</p><ul><li>模型</li><li>优化器</li><li>Train/Val/Test步骤</li></ul></li><li><p>数据流伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">    out = training_step(batch)</span><br><span class="line">    outs.append(out)</span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><p>等价Lightning代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    prediction = ...</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        <span class="comment"># do something with these</span></span><br></pre></td></tr></table></figure><p>我们需要做的，就是像填空一样，填这些函数。</p></li></ul><h3 id="组件与函数"><a class="markdownIt-Anchor" href="#组件与函数"></a> 组件与函数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api">API页面</a></p><ul><li><p>一个Pytorch-Lighting 模型必须含有的部件是：</p><ul><li><p><code>init</code>: 初始化，包括模型和系统的定义。</p></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.training_step"><code>training_step(self, batch, batch_idx)</code></a>: 即每个batch的处理函数。</p><blockquote><p>参数：</p><ul><li><strong>batch</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> | (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …) | [<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …]) – The output of your <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. A tensor, tuple or list.</li><li><strong>batch_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Integer displaying index of this batch</li><li><strong>optimizer_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – When using multiple optimizers, this argument will also be present.</li><li><strong>hiddens</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>) – Passed in if <a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.params.truncated_bptt_steps"><code>truncated_bptt_steps</code></a> &gt; 0.</li></ul><p>返回值：Any of.</p><ul><li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> - The loss tensor</li><li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li><li><code>None</code> - Training will skip to the next batch</li></ul></blockquote><p>返回值无论如何也需要有一个loss量。如果是字典，要有这个key。没loss这个batch就被跳过了。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y, z = batch</span><br><span class="line">    out = self.encoder(x)</span><br><span class="line">    loss = self.loss(out, x)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple optimizers (e.g.: GANs)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, optimizer_idx</span>):</span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># do training_step with encoder</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># do training_step with decoder</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Truncated back-propagation through time</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx, hiddens</span>):</span><br><span class="line">    <span class="comment"># hiddens are the hidden states from the previous truncated backprop step</span></span><br><span class="line">    ...</span><br><span class="line">    out, hiddens = self.lstm(data, hiddens)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;hiddens&#x27;</span>: hiddens&#125;</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization"><code>configure_optimizers</code></a>: 优化器定义，返回一个优化器，或数个优化器，或两个List（优化器，Scheduler）。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most cases</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    opt = Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">return</span> opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> generator_opt, disriminator_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [generator_opt, disriminator_opt], [discriminator_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sched = &#123;<span class="string">&#x27;scheduler&#x27;</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">                 <span class="string">&#x27;interval&#x27;</span>: <span class="string">&#x27;step&#x27;</span>&#125;  <span class="comment"># called after each training step</span></span><br><span class="line">    dis_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sched, dis_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: dis_opt, <span class="string">&#x27;frequency&#x27;</span>: n_critic&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: gen_opt, <span class="string">&#x27;frequency&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li></ul></li><li><p>可以指定的部件有：</p><ul><li><code>forward</code>: 和正常的<code>nn.Module</code>一样，用于inference。内部调用时：<code>y=self(batch)</code></li><li><code>training_step_end</code>: 只在使用多个node进行训练且结果涉及如softmax之类需要全部输出联合运算的步骤时使用该函数。同理，<code>validation_step_end</code>/<code>test_step_end</code>。</li><li><code>training_epoch_end</code>:<ul><li>在一个训练epoch结尾处被调用。</li><li>输入参数：一个List，List的内容是前面<code>training_step()</code>所返回的每次的内容。</li><li>返回：None</li></ul></li><li><code>validation_step(self, batch, batch_idx)</code>/<code>test_step(self, batch, batch_idx)</code>:<ul><li>没有返回值限制，不一定非要输出一个<code>val_loss</code>。</li></ul></li><li><code>validation_epoch_end</code>/<code>test_epoch_end</code>:</li></ul></li><li><p>工具函数有：</p><ul><li><p><code>freeze</code>：冻结所有权重以供预测时候使用。仅当已经训练完成且后面只测试时使用。</p></li><li><p><code>print</code>：尽管自带的<code>print</code>函数也可以使用，但如果程序运行在分布式系统时，会打印多次。而使用<code>self.print()</code>则只会打印一次。</p></li><li><p><code>log</code>：像是TensorBoard等log记录器，对于每个log的标量，都会有一个相对应的横坐标，它可能是batch number或epoch number。而<code>on_step</code>就表示把这个log出去的量的横坐标表示为当前batch，而<code>on_epoch</code>则表示将log的量在整个epoch上进行累积后log，横坐标为当前epoch。</p><table><thead><tr><th>LightningMoule Hook</th><th>on_step</th><th>on_epoch</th><th>prog_bar</th><th>logger</th></tr></thead><tbody><tr><td>training_step</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_step_end</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr><td>training_epoch_end</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_step_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr><td>validation_epoch_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr></tbody></table><p><code>*</code> also applies to the test loop</p><blockquote><p>参数</p><ul><li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – key name</li><li><strong>value</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>) – value name</li><li><strong>prog_bar</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the progress bar</li><li><strong>logger</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the logger</li><li><strong>on_step</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs at this step. None auto-logs at the training_step but not validation/test_step</li><li><strong>on_epoch</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</li><li><strong>reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – reduction function over step values for end of epoch. Torch.mean by default</li><li><strong>tbptt_reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – function to reduce on truncated back prop</li><li><strong>tbptt_pad_token</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – token to use for padding</li><li><strong>enable_graph</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, will not auto detach the graph</li><li><strong>sync_dist</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, reduces the metric across GPUs/TPUs</li><li><strong>sync_dist_op</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>, <a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>]) – the op to sync across GPUs/TPUs</li><li><strong>sync_dist_group</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>]) – the ddp group</li></ul></blockquote></li><li><p><code>log_dict</code>：和<code>log</code>函数唯一的区别就是，<code>name</code>和<code>value</code>变量由一个字典替换。表示同时log多个值。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;acc&#x27;</span>: acc, ..., <span class="string">&#x27;metric_n&#x27;</span>: metric_n&#125;</span><br><span class="line">self.log_dict(values)</span><br></pre></td></tr></table></figure></li><li><p><code>save_hyperparameters</code>：储存<code>init</code>中输入的所有超参。后续访问可以由<code>self.hparams.argX</code>方式进行。同时，超参表也会被存到文件中。</p></li></ul></li><li><p>函数内建变量：</p><ul><li><code>device</code>：可以使用<code>self.device</code>来构建设备无关型tensor。如：<code>z = torch.rand(2, 3, device=self.device)</code>。</li><li><code>hparams</code>：含有所有前面存下来的输入超参。</li><li><code>precision</code>：精确度。常见32和16。</li></ul></li></ul><h3 id="要点"><a class="markdownIt-Anchor" href="#要点"></a> 要点</h3><ul><li>如果准备使用DataParallel，在写<code>training_step</code>的时候需要调用forward函数，<code>z=self(x)</code></li></ul><h3 id="模板"><a class="markdownIt-Anchor" href="#模板"></a> 模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">...</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validation_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_step_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_epoch_end</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">...</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">any_extra_hook</span>(<span class="params">...</span>)</span><br></pre></td></tr></table></figure><h2 id="trainer"><a class="markdownIt-Anchor" href="#trainer"></a> Trainer</h2><h3 id="基础使用"><a class="markdownIt-Anchor" href="#基础使用"></a> 基础使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightningModule()</span><br><span class="line"></span><br><span class="line">trainer = Trainer()</span><br><span class="line">trainer.fit(model, train_dataloader, val_dataloader)</span><br></pre></td></tr></table></figure><p>如果连<code>validation_step</code>都没有，那<code>val_dataloader</code>也就算了。</p><h3 id="伪代码与hooks"><a class="markdownIt-Anchor" href="#伪代码与hooks"></a> 伪代码与hooks</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks">Hooks页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">...</span>):</span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> global_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># prepare data is called on GLOBAL_ZERO only</span></span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> gpu/tpu <span class="keyword">in</span> gpu/tpus:</span><br><span class="line">        train_on_device(model.copy())</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_on_device</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># setup is called PER DEVICE</span></span><br><span class="line">    setup()</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_pretrain_routine_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_loop()</span><br><span class="line"></span><br><span class="line">    teardown()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_loop</span>():</span><br><span class="line">    on_train_epoch_start()</span><br><span class="line">    train_outs = []</span><br><span class="line">    <span class="keyword">for</span> train_batch <span class="keyword">in</span> train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----- train_step methods -------</span></span><br><span class="line">        out = training_step(batch)</span><br><span class="line">        train_outs.append(out)</span><br><span class="line"></span><br><span class="line">        loss = out.loss</span><br><span class="line"></span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line">        optimizer_step()</span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end training epoch</span></span><br><span class="line">    logs = training_epoch_end(outs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_loop</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line">    val_outs = []</span><br><span class="line">    <span class="keyword">for</span> val_batch <span class="keyword">in</span> val_dataloader():</span><br><span class="line">        on_validation_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------- val step methods -------</span></span><br><span class="line">        out = validation_step(val_batch)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up for train</span></span><br><span class="line">    model.train()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="推荐参数"><a class="markdownIt-Anchor" href="#推荐参数"></a> 推荐参数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags">参数介绍（附视频）</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api">类定义与默认参数</a></p><ul><li><p><code>default_root_dir</code>：默认存储地址。所有的实验变量和权重全部会被存到这个文件夹里面。推荐是，每个模型有一个独立的文件夹。每次重新训练会产生一个新的<code>version_x</code>子文件夹。</p></li><li><p><code>max_epochs</code>：最大训练周期数。<code>trainer = Trainer(max_epochs=1000)</code></p></li><li><p><code>min_epochs</code>：至少训练周期数。当有Early Stop时使用。</p></li><li><p><code>auto_scale_batch_size</code>：在进行任何训练前自动选择合适的batch size。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (no scaling of batch size)</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run batch size scaling, result overrides hparams.batch_size</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="string">&#x27;binsearch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the batch size</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>auto_select_gpus</code>：自动选择合适的GPU。尤其是在有GPU处于独占模式时候，非常有用。</p></li><li><p><code>auto_lr_find</code>：自动找到合适的初始学习率。使用了该<a href="https://arxiv.org/abs/1506.01186">论文</a>的技术。当且仅当执行<code>trainer.tune(model)</code>代码时工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run learning rate finder, results override hparams.learning_rate</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run learning rate finder, results override hparams.my_lr_arg</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="string">&#x27;my_lr_arg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the lr</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>precision</code>：精确度。正常是32，使用16可以减小内存消耗，增大batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 16-bit precision</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">16</span>, gpus=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>val_check_interval</code>：进行Validation测试的周期。正常为1，训练1个epoch测试4次是0.25，每1000 batch测试一次是1000。</p><blockquote><ul><li>use (float) to check within a training epoch：此时这个值为一个epoch的百分比。每百分之多少测试一次。</li><li>use (int) to check every n steps (batches)：每多少个batch测试一次。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set 4 times during a training epoch</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set every 1000 training batches</span></span><br><span class="line"><span class="comment"># use this when using iterableDataset and your dataset has no length</span></span><br><span class="line"><span class="comment"># (ie: production cases with streaming data)</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#gpus"><code>gpus</code></a>：控制使用的GPU数。当设定为None时，使用cpu。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (ie: train on CPU)</span></span><br><span class="line">trainer = Trainer(gpus=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># int: train on 2 gpus</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list: train on GPUs 1, 4 (by bus ordering)</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;1, 4&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1: train on all gpus</span></span><br><span class="line">trainer = Trainer(gpus=-<span class="number">1</span>)</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;-1&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine with num_nodes to train on multiple GPUs across nodes</span></span><br><span class="line"><span class="comment"># uses 8 gpus in total</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>, num_nodes=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train only on GPUs 1 and 4 across nodes</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>], num_nodes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#limit-train-batches"><code>limit_train_batches</code></a>：使用训练数据的百分比。如果数据过多，或正在调试，可以使用这个。值的范围为0~1。同样，有<code>limit_test_batches</code>，<code>limit_val_batches</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 25% of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 10 batches of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run"><code>fast_dev_run</code></a>：bool量。如果设定为true，会只执行一个batch的train, val 和 test，然后结束。仅用于debug。</p><blockquote><p>Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like <code>LearningRateLogger</code> and runs for only 1 epoch</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 1 train, val, test batch and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 7 train, val, test batches and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="number">7</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="fit函数"><a class="markdownIt-Anchor" href="#fit函数"></a> .fit()函数</h3><p><code>Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None)</code>：输入第一个量一定是model，然后可以跟一个LigntningDataModule或一个普通的Train DataLoader。如果定义了Val step，也要有Val DataLoader。</p><blockquote><p>参数</p><ul><li><strong>datamodule</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule"><code>LightningDataModule</code></a>]) – A instance of <code>LightningDataModule</code>.</li><li><strong>model</strong> (<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule"><code>LightningModule</code></a>) – Model to fit.</li><li><strong>train_dataloader</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>]) – A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.</li><li><strong>val_dataloaders</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>, <a href="https://docs.python.org/3/library/typing.html#typing.List"><code>List</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>], <a href="https://docs.python.org/3/library/constants.html#None"><code>None</code></a>]) – Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped</li></ul></blockquote><h3 id="其他要点"><a class="markdownIt-Anchor" href="#其他要点"></a> 其他要点</h3><ul><li><code>.test()</code>若非直接调用，不会运行。<code>trainer.test()</code></li><li><code>.test()</code>会自动load最优模型。</li><li><code>model.eval()</code> and <code>torch.no_grad()</code> 在进行测试时会被自动调用。</li><li>默认情况下，<code>Trainer()</code>运行于CPU上。</li></ul><h3 id="使用样例"><a class="markdownIt-Anchor" href="#使用样例"></a> 使用样例</h3><ol><li>手动添加命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">hparams</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer(gpus=hparams.gpus)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gpus&#x27;</span>, default=<span class="literal">None</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="2"><li>自动添加所有<code>Trainer</code>会用到的命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="3"><li>混合式，既使用<code>Trainer</code>相关参数，又使用一些自定义参数，如各种模型超参：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> LightningModule, Trainer</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">32</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hidden_dim&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">128</span>)</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="所有参数"><a class="markdownIt-Anchor" href="#所有参数"></a> 所有参数</h3><blockquote><p><code>Trainer.``__init__</code>(<em>logger=True</em>, <em>checkpoint_callback=True</em>, <em>callbacks=None</em>, <em>default_root_dir=None</em>, <em>gradient_clip_val=0</em>, <em>process_position=0</em>, <em>num_nodes=1</em>, <em>num_processes=1</em>, <em>gpus=None</em>, <em>auto_select_gpus=False</em>, <em>tpu_cores=None</em>, <em>log_gpu_memory=None</em>, <em>progress_bar_refresh_rate=None</em>, <em>overfit_batches=0.0</em>, <em>track_grad_norm=- 1</em>, <em>check_val_every_n_epoch=1</em>, <em>fast_dev_run=False</em>, <em>accumulate_grad_batches=1</em>, <em>max_epochs=None</em>, <em>min_epochs=None</em>, <em>max_steps=None</em>, <em>min_steps=None</em>, <em>limit_train_batches=1.0</em>, <em>limit_val_batches=1.0</em>, <em>limit_test_batches=1.0</em>, <em>limit_predict_batches=1.0</em>, <em>val_check_interval=1.0</em>, <em>flush_logs_every_n_steps=100</em>, <em>log_every_n_steps=50</em>, <em>accelerator=None</em>, <em>sync_batchnorm=False</em>, <em>precision=32</em>, <em>weights_summary=‘top’</em>, <em>weights_save_path=None</em>, <em>num_sanity_val_steps=2</em>, <em>truncated_bptt_steps=None</em>, <em>resume_from_checkpoint=None</em>, <em>profiler=None</em>, <em>benchmark=False</em>, <em>deterministic=False</em>, <em>reload_dataloaders_every_epoch=False</em>, <em>auto_lr_find=False</em>, <em>replace_sampler_ddp=True</em>, <em>terminate_on_nan=False</em>, <em>auto_scale_batch_size=False</em>, <em>prepare_data_per_node=True</em>, <em>plugins=None</em>, <em>amp_backend=‘native’</em>, <em>amp_level=‘O2’</em>, <em>distributed_backend=None</em>, <em>move_metrics_to_cpu=False</em>, <em>multiple_trainloader_mode=‘max_size_cycle’</em>, <em>stochastic_weight_avg=False</em>)</p></blockquote><h3 id="log和return-loss到底在做什么"><a class="markdownIt-Anchor" href="#log和return-loss到底在做什么"></a> Log和return loss到底在做什么</h3><p>To add a training loop use the training_step method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitClassifier</span>(pl.LightningModule):</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = model</span><br><span class="line"></span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">         x, y = batch</span><br><span class="line">         y_hat = self.model(x)</span><br><span class="line">         loss = F.cross_entropy(y_hat, y)</span><br><span class="line">         <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>无论是<code>training_step</code>，还是<code>validation_step</code>，<code>test_step</code>返回值都是<code>loss</code>。返回的loss会被用一个list收集起来。</li></ul><p>Under the hood, Lightning does the following (pseudocode):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put model in train mode</span></span><br><span class="line">model.train()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    loss = training_step(batch)</span><br><span class="line">    losses.append(loss.detach())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><h4 id="training-epoch-level-metrics"><a class="markdownIt-Anchor" href="#training-epoch-level-metrics"></a> Training epoch-level metrics</h4><p>If you want to calculate epoch-level metrics and log them, use the <code>.log</code> method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logs metrics for each training_step,</span></span><br><span class="line">    <span class="comment"># and the average across the epoch, to the progress bar and logger</span></span><br><span class="line">    self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>如果在<code>x_step</code>函数中使用了<code>.log()</code>函数，那么这个量将会被逐步记录下来。每一个<code>log</code>出去的变量都会被记录下来，每一个<code>step</code>会集中生成一个字典dict，而每个epoch都会把这些字典收集起来，形成一个字典的list。</li></ul><p>The .log object automatically reduces the requested metrics across the full epoch. Here’s the pseudocode of what it does under the hood:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">epoch_metric = torch.mean(torch.stack([x[<span class="string">&#x27;train_loss&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> outs]))</span><br></pre></td></tr></table></figure><h4 id="train-epoch-level-operations"><a class="markdownIt-Anchor" href="#train-epoch-level-operations"></a> Train epoch-level operations</h4><p>If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    preds = ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;other_stuff&#x27;</span>: preds&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span><br><span class="line">   <span class="keyword">for</span> pred <span class="keyword">in</span> training_step_outputs:</span><br><span class="line">       <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><p>The matching pseudocode is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><h2 id="datamodule"><a class="markdownIt-Anchor" href="#datamodule"></a> DataModule</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html">主页面</a></p><h3 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h3><ul><li><p>首先，这个<code>DataModule</code>和之前写的Dataset完全不冲突。前者是后者的一个包装，并且这个包装可以被用于多个torch Dataset 中。在我看来，其最大的作用就是把各种train/val/test划分、DataLoader初始化之类的重复代码通过包装类的方式得以被简单的复用。</p></li><li><p>具体作用项目：</p><ul><li>Download instructions：下载</li><li>Processing instructions：处理</li><li>Split instructions：分割</li><li>Train dataloader：训练集Dataloader</li><li>Val dataloader(s)：验证集Dataloader</li><li>Test dataloader(s)：测试集Dataloader</li></ul></li><li><p>其次，<code>pl.LightningDataModule</code>相当于一个功能加强版的torch Dataset，加强的功能包括：</p><ul><li><code>prepare_data(self)</code>：<ul><li>最最开始的时候，进行一些无论GPU有多少只要执行一次的操作，如写入磁盘的下载操作、分词操作(tokenize)等。</li><li>这里是一劳永逸式准备数据的函数。</li><li>由于只在单线程中调用，不要在这个函数中进行<code>self.x=y</code>似的赋值操作。</li><li>但如果是自己用而不是给大众分发的话，这个函数可能并不需要调用，因为数据提前处理好就好了。</li></ul></li><li><code>setup(self, stage=None)</code>：<ul><li>实例化数据集（Dataset），并进行相关操作，如：清点类数，划分train/val/test集合等。</li><li>参数<code>stage</code>用于指示是处于训练周期(<code>fit</code>)还是测试周期(<code>test</code>)，其中，<code>fit</code>周期需要构建train和val两者的数据集。</li><li>setup函数不需要返回值。初始化好的train/val/test set直接赋值给self即可。</li></ul></li><li><code>train_dataloader/val_dataloader/test_dataloader</code>：<ul><li>初始化<code>DataLoader</code>。</li><li>返回一个DataLoader量。</li></ul></li></ul></li></ul><h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MNISTDataModule</span>(pl.LightningDataModule):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_dir: <span class="built_in">str</span> = <span class="string">&#x27;./&#x27;</span>, batch_size: <span class="built_in">int</span> = <span class="number">64</span>, num_workers: <span class="built_in">int</span> = <span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line"></span><br><span class="line">        self.transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.dims is returned when you call dm.size()</span></span><br><span class="line">        <span class="comment"># Setting default dims here because we know them.</span></span><br><span class="line">        <span class="comment"># Could optionally be assigned dynamically in dm.setup()</span></span><br><span class="line">        self.dims = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        self.num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># download</span></span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;fit&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">            self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assign test dataset for use in dataloader(s)</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;test&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mnist_test = MNIST(self.data_dir, train=<span class="literal">False</span>, transform=self.transform)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">val_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)</span><br></pre></td></tr></table></figure><h3 id="要点-2"><a class="markdownIt-Anchor" href="#要点-2"></a> 要点</h3><ul><li>若在DataModule中定义了一个<code>self.dims</code> 变量，后面可以调用<code>dm.size()</code>获取该变量。</li></ul><h2 id="saving-and-loading"><a class="markdownIt-Anchor" href="#saving-and-loading"></a> Saving and Loading</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html">主页面</a></p><h3 id="saving"><a class="markdownIt-Anchor" href="#saving"></a> Saving</h3><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint">ModelCheckpoint</a>: 自动储存的callback module。默认情况下training过程中只会自动储存最新的模型与相关参数，而用户可以通过这个module自定义。如观测一个<code>val_loss</code>的量，并储存top 3好的模型，且同时储存最后一个epoch的模型，等等。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span></span><br><span class="line">checkpoint_callback = ModelCheckpoint(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    filename=<span class="string">&#x27;sample-mnist-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">    save_top_k=<span class="number">3</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    save_last=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer(gpus=<span class="number">1</span>, max_epochs=<span class="number">3</span>, progress_bar_refresh_rate=<span class="number">20</span>, callbacks=[checkpoint_callback])</span><br></pre></td></tr></table></figure></li><li><p>另外，也可以手动存储checkpoint: <code>trainer.save_checkpoint(&quot;example.ckpt&quot;)</code></p></li><li><p><code>ModelCheckpoint</code> Callback中，如果<code>save_weights_only =True</code>，那么将会只储存模型的权重（相当于<code>model.save_weights(filepath)</code>），反之会储存整个模型（相当于<code>model.save(filepath)</code>）。</p></li></ul><h3 id="loading"><a class="markdownIt-Anchor" href="#loading"></a> Loading</h3><ul><li><p>load一个模型，包括它的weights、biases和超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightingModule.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.learning_rate)</span><br><span class="line"><span class="comment"># prints the learning_rate you used in this checkpoint</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">y_hat = model(x)</span><br></pre></td></tr></table></figure></li><li><p>load模型时替换一些超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LitModel</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.l1 = nn.Linear(self.hparams.in_dim, self.hparams.out_dim)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># if you train and save the model like this it will use these values when loading</span></span><br><span class="line"><span class="comment"># the weights. But you can overwrite this</span></span><br><span class="line">LitModel(in_dim=<span class="number">32</span>, out_dim=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=32, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=128, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH, in_dim=<span class="number">128</span>, out_dim=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p>完全load训练状态：load包括模型的一切，以及和训练相关的一切参数，如<code>model, epoch, step, LR schedulers, apex</code>等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = LitModel()</span><br><span class="line">trainer = Trainer(resume_from_checkpoint=<span class="string">&#x27;some/path/to/my_checkpoint.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically restores model, epoch, step, LR schedulers, apex, etc...</span></span><br><span class="line">trainer.fit(model)</span><br></pre></td></tr></table></figure></li></ul><h2 id="callbacks"><a class="markdownIt-Anchor" href="#callbacks"></a> Callbacks</h2><ul><li>Callback 是一个自包含的程序，可以与训练流程交织在一起，而不会污染主要的研究逻辑。</li><li>Callback 并非只会在epoch结尾调用。pytorch-lightning 提供了数十个hook（接口，调用位置）可供选择，也可以自定义callback，实现任何想实现的模块。</li><li>推荐使用方式是，随问题和项目变化的操作，这些函数写到lightning module里面，而相对独立，相对辅助性的，需要复用的内容则可以定义单独的模块，供后续方便地插拔使用。</li></ul><h3 id="callbacks推荐"><a class="markdownIt-Anchor" href="#callbacks推荐"></a> Callbacks推荐</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks">内建Callbacks</a></p><ul><li><p><code>EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)</code>：根据某个值，在数个epoch没有提升的情况下提前停止训练。</p><blockquote><p>参数：</p><ul><li><strong>monitor</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – quantity to be monitored. Default: <code>'early_stop_on'</code>.</li><li><strong>min_delta</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><code>float</code></a>) – minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: <code>0.0</code>.</li><li><strong>patience</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – number of validation epochs with no improvement after which training will be stopped. Default: <code>3</code>.</li><li><strong>verbose</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – verbosity mode. Default: <code>False</code>.</li><li><strong>mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – one of <code>'min'</code>, <code>'max'</code>. In <code>'min'</code> mode, training will stop when the quantity monitored has stopped decreasing and in <code>'max'</code> mode it will stop when the quantity monitored has stopped increasing.</li><li><strong>strict</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – whether to crash the training if monitor is not found in the validation metrics. Default: <code>True</code>.</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">trainer = Trainer(callbacks=[early_stopping])</span><br></pre></td></tr></table></figure></li><li><p><code>ModelCheckpoint</code>：见上文<strong>Saving and Loading</strong>.</p></li><li><p><code>PrintTableMetricsCallback</code>：在每个epoch结束后打印一份结果整理表格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pl_bolts.callbacks <span class="keyword">import</span> PrintTableMetricsCallback</span><br><span class="line"></span><br><span class="line">callback = PrintTableMetricsCallback()</span><br><span class="line">trainer = pl.Trainer(callbacks=[callback])</span><br><span class="line">trainer.fit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># at the end of every epoch it will print</span></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss│train_loss│val_loss│epoch</span></span><br><span class="line"><span class="comment"># ──────────────────────────────</span></span><br><span class="line"><span class="comment"># 2.2541470527648926│2.2541470527648926│2.2158432006835938│0</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="logging"><a class="markdownIt-Anchor" href="#logging"></a> Logging</h2><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html">Logging</a>：Logger默认是TensorBoard，但可以指定各种主流Logger<a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#supported-loggers">框架</a>，<a href="http://xn--Comet-gv5i.ml">如Comet.ml</a>，MLflow，Netpune，或直接CSV文件。可以同时使用复数个logger。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> loggers <span class="keyword">as</span> pl_loggers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(</span><br><span class="line">    save_dir=os.getcwd(),</span><br><span class="line">    version=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;lightning_logs&#x27;</span></span><br><span class="line">)</span><br><span class="line">trainer = Trainer(logger=tb_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or use the same format as others</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One Logger</span></span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=comet_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save code snapshot</span></span><br><span class="line">logger = pl_loggers.TestTubeLogger(<span class="string">&#x27;logs/&#x27;</span>, create_git_tag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Logger</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=[tb_logger, comet_logger])</span><br></pre></td></tr></table></figure><p>默认情况下，每50个batch log一次，可以通过调整参数</p></li><li><p>如果想要log输出非scalar（标量）的内容，如图片，文本，直方图等等，可以直接调用<code>self.logger.experiment.add_xxx()</code>来实现所需操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">...</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># the logger you used (in this case tensorboard)</span></span><br><span class="line">    tensorboard = self.logger.experiment</span><br><span class="line">    tensorboard.add_image()</span><br><span class="line">    tensorboard.add_histogram(...)</span><br><span class="line">    tensorboard.add_figure(...)</span><br></pre></td></tr></table></figure></li><li><p>使用log：如果是TensorBoard，那么：<code>tensorboard --logdir ./lightning_logs</code>。在Jupyter Notebook中，可以使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start tensorboard.</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir lightning_logs/</span><br></pre></td></tr></table></figure><p>在行内打开TensorBoard。</p></li><li><p>小技巧：如果在局域网内开启了TensorBoard，加上flag <code>--bind_all</code>即可使用主机名访问：</p><p><code>tensorboard --logdir lightning_logs --bind_all</code> -&gt; <code>http://SERVER-NAME:6006/</code></p></li></ul><h3 id="同时使用tensorboard和csv-logger"><a class="markdownIt-Anchor" href="#同时使用tensorboard和csv-logger"></a> 同时使用TensorBoard和CSV Logger</h3><p>如果同时使用两个Logger，PL会有睿智操作：如果保存根目录相同，他们会依次建立两个version文件夹，令人窒息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> TensorBoardLogger, CSVLogger</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_loggers</span>():</span><br><span class="line">    loggers = []</span><br><span class="line">    loggers.append(TensorBoardLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tb&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CSVLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;csv&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CometLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tt&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loggers</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_callbacks</span>(<span class="params">logger</span>):</span><br><span class="line">    callbacks = []</span><br><span class="line">    dirpath = <span class="string">f&#x27;lightning_logs/<span class="subst">&#123;logger.name&#125;</span>/version_<span class="subst">&#123;logger.version&#125;</span>/checkpoints&#x27;</span></span><br><span class="line">    callbacks.append(ModelCheckpoint(</span><br><span class="line">        dirpath=dirpath,</span><br><span class="line">        monitor=<span class="string">&#x27;loss_epoch&#x27;</span>,</span><br><span class="line">        filename=<span class="string">&#x27;&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">        save_top_k=<span class="number">3</span>,</span><br><span class="line">        mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">        save_last=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> callbacks</span><br><span class="line"></span><br><span class="line">loggers = load_loggers()</span><br><span class="line">callbacks = load_callbacks(loggers[<span class="number">0</span>])</span><br><span class="line">trainer = pl.Trainer(logger=loggers, callbacks=callbacks)</span><br></pre></td></tr></table></figure><h2 id="transfer-learning"><a class="markdownIt-Anchor" href="#transfer-learning"></a> Transfer Learning</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html#transfer-learning">主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImagenetTransferLearning</span>(<span class="title class_ inherited__">LightningModule</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a pretrained resnet</span></span><br><span class="line">        backbone = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">        num_filters = backbone.fc.in_features</span><br><span class="line">        layers = <span class="built_in">list</span>(backbone.children())[:-<span class="number">1</span>]</span><br><span class="line">        self.feature_extractor = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use the pretrained model to classify cifar-10 (10 image classes)</span></span><br><span class="line">        num_target_classes = <span class="number">10</span></span><br><span class="line">        self.classifier = nn.Linear(num_filters, num_target_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            representations = self.feature_extractor(x).flatten(<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(representations)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h2 id="关于device操作"><a class="markdownIt-Anchor" href="#关于device操作"></a> 关于device操作</h2><p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># good (self is LightningModule)</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br></pre></td></tr></table></figure><p>For tensors that need to be model attributes, it is best practice to register them as buffers in the modules’s <code>__init__</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">self.t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;t&quot;</span>, torch.rand(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>前面两段是教程中的文本。然而实际上有一个暗坑：</p><p>如果你使用了一个中继的<code>pl.LightningModule</code>，而这个module里面实例化了某个普通的<code>nn.Module</code>，而这个模型中又需要内部生成一些tensor，比如图片每个通道的mean，std之类，那么如果你从<code>pl.LightningModule</code>中pass一个<code>self.device</code>，实际上在一开始这个<code>self.device</code>永远是<code>cpu</code>。所以如果你在调用的<code>nn.Module</code>的<code>__init__()</code>中初始化，使用<code>to(device)</code>或干脆什么都不用，结果就是它永远都在<code>cpu</code>上。</p><p>但是，经过实验，虽然<code>pl.LightningModule</code>在<code>__init__()</code>阶段<code>self.device</code>还是<code>cpu</code>，当进入了<code>training_step()</code>之后，就迅速变为了<code>cuda</code>。所以，对于子模块，最佳方案是，使用一个<code>forward</code>中传入的量，如<code>x</code>，作为一个reference变量，用<code>type_as</code>函数将在模型中生成的tensor都放到和这个参考变量相同的device上即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RDNFuse</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_norm_func</span>(<span class="params">self, ref</span>):</span><br><span class="line">        self.mean = torch.tensor(np.array(self.mean_sen), dtype=torch.float32).type_as(ref)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mean&#x27;</span>):</span><br><span class="line">            self.init_norm_func(x)</span><br></pre></td></tr></table></figure><h2 id="关于limit_train_batches选项"><a class="markdownIt-Anchor" href="#关于limit_train_batches选项"></a> 关于<code>limit_train_batches</code>选项</h2><p>这里涉及到一个问题，就是每个epoch使用部分数据而非全部时，程序将会怎么工作。</p><blockquote><p>The shuffling happens when the iterator is created. In the case of the for loop, that happens just before the for loop starts. You can create the iterator manually with:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator gets created, the data has been shuffled at this point.</span></span><br><span class="line">data_iterator = <span class="built_in">iter</span>(namesTrainLoader)</span><br></pre></td></tr></table></figure><blockquote><p>By default the data loader uses <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler"><code>torch.utils.data.RandomSampler</code></a> if you set <code>shuffle=True</code> (without providing your own sampler). Its implementation is very straight forward and you can see where the data is shuffled when the iterator is created by looking at the <a href="https://github.com/pytorch/pytorch/blob/f3e620ee83f080283445aa1a7242d40e30eb6a7f/torch/utils/data/sampler.py#L103-L107"><code>RandomSampler.__iter__</code></a> method:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">    <span class="keyword">if</span> self.replacement:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br></pre></td></tr></table></figure><blockquote><p>The return statement is the important part, where the shuffling takes place. It simply creates a random permutation of the indices.</p><p>That means you will see your entire dataset every time you fully consume the iterator, just in a different order every time. Therefore there is no data lost (not including cases with <code>drop_last=True</code>) and your model will see all data at every epoch.</p></blockquote><p>总结下来，如果使用了<code>shuffle=True</code>选项，那么即使每次都不跑完整个epoch，你还是有机会见到所有的数据的。数据集的shuffle发生在<code>iter</code>被创建的时候，在我们一般的代码中，也就是内层for循环开始时。但如果你没有选择<code>shuffle=True</code>，那你将永远只能看到你设定的前面N个数据。</p><h2 id="points"><a class="markdownIt-Anchor" href="#points"></a> Points</h2><ul><li><p><code>pl.seed_everything(1234)</code>：对所有相关的随机量固定种子。</p></li><li><p>使用LR Scheduler时候，不用自己<code>.step()</code>。它也被Trainer自动处理了。<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html?highlight=scheduler#">Optimization 主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single optimizer</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">        loss = model.training_step(batch, batch_idx, ...)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Multiple optimizers</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">     <span class="keyword">for</span> opt <span class="keyword">in</span> optimizers:</span><br><span class="line">        disable_grads_for_other_optimizers()</span><br><span class="line">        train_step(opt)</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">     scheduler.step()</span><br></pre></td></tr></table></figure></li><li><p>关于划分train和val集合的方法。与PL无关，但很常用，两个例子：</p><ol><li><code>random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</code></li><li>如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br></pre></td></tr></table></figure><p>Parameters：</p><ul><li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</li><li><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</li><li><strong>generator</strong> (<a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>Generator</em></a>) – Generator used for the random permutation.</li></ul></li><li><p>如果使用了<code>PrintTableMetricsCallback</code>，那么<code>validation_step</code>不要return内容，否则会炸。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#写在前面&quot;&gt;&lt;/a&gt; 写在前面&lt;/h2&gt;
&lt;p&gt;Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
    <category term="pytorch-lightning" scheme="https://www.miracleyoo.com/tags/pytorch-lightning/"/>
    
  </entry>
  
  <entry>
    <title>Analyzing Geography Data (Beginners&#39; Tutorial)</title>
    <link href="https://www.miracleyoo.com/2021/02/03/satellite-basic/"/>
    <id>https://www.miracleyoo.com/2021/02/03/satellite-basic/</id>
    <published>2021-02-04T02:18:19.000Z</published>
    <updated>2021-03-12T23:19:59.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据来源"><a class="markdownIt-Anchor" href="#数据来源"></a> 数据来源</h2><h3 id="卫星类型"><a class="markdownIt-Anchor" href="#卫星类型"></a> 卫星类型</h3><ol><li><p>Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi>m</mi><mo>×</mo><mn>10</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">10m \times 10m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn><mi>m</mi><mo>×</mo><mn>20</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">20m \times 20m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>60</mn><mi>m</mi><mo>×</mo><mn>60</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">60m \times 60m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">0</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">0</span><span class="mord mathnormal">m</span></span></span></span>，bands中心波长从442.3nm到2185.7nm。</p><img data-src="image-20201213154533413.png" alt="image-20201213154533413" style="zoom:33%;"><span id="more"></span></li><li><p>Planet：这个数据源提供<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi>m</mi><mo>×</mo><mn>3</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">3m \times 3m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathnormal">m</span></span></span></span>的4 bands MSI image。分别是R、G、B、NIR。</p><ul><li>这个数据源似乎只对美国国内的院校机构开放，不过不是很确定，需要的可以试试。</li></ul><p><img data-src="image-20201213150111581.png" alt="Planet Specification"></p></li><li><p>RapidEyes</p></li></ol><h3 id="sentinel-2-data-source"><a class="markdownIt-Anchor" href="#sentinel-2-data-source"></a> Sentinel-2 Data Source</h3><ol><li><p>USGS Earth Explorer: <a href="https://earthexplorer.usgs.gov/">Link</a>. It support downloading data within five years. And it not only support sentinel-2 data, but also contains many other satellite datasource.</p><p><img data-src="image-20201227143234319.png" alt="image-20201227143234319"></p></li><li><p>Copernicus Open Access Hub: <a href="https://scihub.copernicus.eu/dhus/#/home">Link</a>. It supports only one year’s old data. You can request the older data, but not guarante the fetch time.</p><p><img data-src="image-20201227145324164.png" alt="image-20201227145324164"></p></li><li><p>Amazon AWS Sentinel-2 Service: <a href="https://registry.opendata.aws/sentinel-2/">Link</a></p></li></ol><h3 id="planet-data-source"><a class="markdownIt-Anchor" href="#planet-data-source"></a> Planet Data Source</h3><ul><li>官网：<a href="https://www.planet.com/">Link</a></li></ul><h3 id="specification"><a class="markdownIt-Anchor" href="#specification"></a> Specification</h3><ol><li><a href="https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/msi-instrument">Sentinel-2 MultiSpectral Instrument (MSI) Overview</a></li><li><a href="https://www.planet.com/products/satellite-imagery/files/Planet_Combined_Imagery_Product_Specs_December2017.pdf">Sentinel-2 Specification Doc</a></li><li><a href="https://www.planet.com/products/planet-imagery/">Planet Specification</a></li></ol><h2 id="软件与python包简介"><a class="markdownIt-Anchor" href="#软件与python包简介"></a> 软件与Python包简介</h2><ul><li><a href="http://www.gdal.org/">GDAL</a> –&gt; Fundamental package for processing vector and raster data formats (many modules below depend on this). Used for raster processing.</li><li><a href="https://github.com/mapbox/rasterio">Rasterio</a> –&gt; Clean and fast and geospatial raster I/O for Python. <a href="https://rasterio.readthedocs.io/en/latest/quickstart.html">Guidebook</a>.</li><li><a href="http://geopandas.org/#description">Geopandas</a> –&gt; Working with geospatial data in Python made easier, combines the capabilities of pandas and shapely.</li><li><a href="http://toblerity.org/shapely/manual.html">Shapely</a> –&gt; Python package for manipulation and analysis of planar geometric objects (based on widely deployed <a href="https://trac.osgeo.org/geos/">GEOS</a>).</li><li><a href="https://pypi.python.org/pypi/Fiona">Fiona</a> –&gt; Reading and writing spatial data (alternative for geopandas).</li><li><a href="https://pypi.python.org/pypi/pyproj?">Pyproj</a> –&gt; Performs cartographic transformations and geodetic computations (based on <a href="http://trac.osgeo.org/proj">PROJ.4</a>).</li><li><a href="https://pysal.readthedocs.org/en/latest/">Pysal</a> –&gt; Library of spatial analysis functions written in Python.</li><li><a href="http://geopy.readthedocs.io/en/latest/">Geopy</a> –&gt; Geocoding library: coordinates to address &lt;-&gt; address to coordinates.</li><li><a href="http://geo.holoviews.org/index.html">GeoViews</a> –&gt; Interactive Maps for the web.</li><li><a href="https://networkx.github.io/documentation/networkx-1.10/overview.html">Networkx</a> –&gt; Network analysis and routing in Python (e.g. Dijkstra and A* -algorithms), see <a href="http://gis.stackexchange.com/questions/65056/is-it-possible-to-route-shapefiles-using-python-and-without-arcgis-qgis-or-pgr">this post</a>.</li><li><a href="http://scitools.org.uk/cartopy/docs/latest/index.html">Cartopy</a> –&gt; Make drawing maps for data analysis and visualisation as easy as possible.</li><li><a href="http://docs.scipy.org/doc/scipy/reference/spatial.html">Scipy.spatial</a> –&gt; Spatial algorithms and data structures.</li><li><a href="http://toblerity.org/rtree/">Rtree</a> –&gt; Spatial indexing for Python for quick spatial lookups.</li><li><a href="http://www.rsgislib.org/index.html#python-documentation">RSGISLib</a> –&gt; Remote Sensing and GIS Software Library for Python.</li><li><a href="https://python-geojson.readthedocs.io/en/latest/">python-geojson</a>-&gt; Deal with geojson format files.</li></ul><h2 id="sentinel-2-大气校正"><a class="markdownIt-Anchor" href="#sentinel-2-大气校正"></a> Sentinel-2 大气校正</h2><p>Sentinel-2一般有两种standard，一个是A级别，一个是C级别。C级别的数据是你可以从任意网站上下载到的数据，它没有经过大气校正，是粗数据，每个区块的反射率可能有不同，不适合直接用作深度学习数据。</p><p>经过Sen2Cor软件校正后可以得到A级别的数据。</p><p>Sen2Cor是欧空局发布的一个软件，它即可以作为SNAP的插件安装，也可以作为独立命令行软件使用。推荐后者，更为快速、稳定，尤其适用于大量数据时，可以用脚本批处理。<a href="http://step.esa.int/main/snap-supported-plugins/sen2cor/sen2cor_v2-8/">官网链接</a></p><h3 id="查询坐标映射"><a class="markdownIt-Anchor" href="#查询坐标映射"></a> 查询坐标映射</h3><ul><li>地球是圆的。</li><li>卫星上拍的一张矩形照片所对应的区域并非是矩形的。</li><li>使用卫星图片时要先将其映射到二维展开的坐标系中。</li><li>整个地球被分为了许多预先订好的区域。</li><li>每个区域有一个编号。</li><li>编号可以在<a href="http://epsg.io/">EPSG</a>网站查询。</li></ul><h2 id="snap"><a class="markdownIt-Anchor" href="#snap"></a> SNAP</h2><ul><li><p>欧空局自己用作处理Sentinel-2的软件。<a href="https://step.esa.int/main/download/snap-download/">Link</a></p></li><li><p>只用来处理Sentinel-2，尤其是预处理，包括校正，reprojection，粗crop，统一各个bands分辨率等等，非常好用。因为是亲儿子，所以甚至可以直接读取Sentinel-2每个文件的压缩包，总之十分便利。</p></li><li><p>比较古老，interface有年代感，功能也相较于其他软件比较局限。</p></li><li><p>推荐用作第一步预处理。</p></li><li><p>使用流程：<code>读取-&gt;剪裁-&gt;correction-&gt;resize-&gt;reprojection-&gt;导出</code>。</p></li><li><p>大部分需要用到的功能都在<code>Raster-&gt;Geometric</code>中</p></li><li><p>经过尝试、搜索、确认，SNAP并不提供便利的选定区域截图，或是依据shapefile剪裁，所以目前最佳的方法是，先通过zoom地图和改变窗口大小确保需要的部分大致在view的可视范围内，然后右键，选择<code>Spatial Subset from View</code>，然后可视区域即可被剪裁。注意，这只是粗剪裁，所以尽量多包括一点，也不要少任何一部分。剪裁后的内容可以后续在ArcGIS Pro中进一步处理。</p><img data-src="image-20201227155911702.png" alt="image-20201227155911702" style="zoom:40%;"></li><li><p>展示图：</p><img data-src="image-20201227155421761.png" alt="image-20201227155421761" style="zoom:33%;"><p><img data-src="image-20201227155558109.png" alt="image-20201227155558109"></p></li></ul><h2 id="arcgis-pro"><a class="markdownIt-Anchor" href="#arcgis-pro"></a> ArcGIS Pro</h2><ul><li><p>我能找到的最强大的可用于卫星图像处理的软件。</p></li><li><p>专业，美观，支持format多，有着强大的raster functions。</p></li><li><p>版权软件，下载之前先确认自己学校或公司是否提供License。</p></li><li><p>有时导出raster会出现随机bug，导致：导出可能是纯黑的图片，导出部分没有按照期望剪裁等。很恶心，但似乎也没有更好的选择。</p></li><li><p>解决导出问题：</p><ol><li>关闭导出窗口，重新操作，多试几次。可以解决绝大部分问题。</li><li>和剪裁、mask等有关的问题可以先使用raster function进行这些操作，再直接导出前面操作的结果layer。</li></ol></li><li><p>关于剪裁后的卫星图片和原始图片有着明显亮度对比度区别的问题：</p><ol><li><p>首先，这不是一个bug，而是一个feature。。。实际上的卫星图都很暗沉的，所以软件原生提供一个“显示方法”的函数，调整图片曲线，使得显示的图片比较亮，容易看清楚细节。然而，剪裁后的图片有着不一样的统计值，所以在有些显示函数下，显示的结果和剪裁前结果不同。</p><img data-src="image-20201227155342029.png" alt="image-20201227155342029" style="zoom:33%;"></li><li><p>但是不用担心，因为导出时候并不会考虑这个显示函数。即：剪裁前后的导出图像数值是相同的。</p></li></ol></li><li><p>下面是两张展示图：</p></li></ul><p><img data-src="image-20201227155051608.png" alt="image-20201227155051608"></p><p><img data-src="image-20201227155259214.png" alt="image-20201227155259214"></p><h2 id="qgis"><a class="markdownIt-Anchor" href="#qgis"></a> QGIS</h2><ul><li><p>开源软件，支持很多插件，比如直接下载Sentinel-2数据，Sen2Cor等。<a href="https://qgis.org/en/site/">Link</a></p></li><li><p>支持很多format的数据。</p></li><li><p>问题是，不稳定，效率低，容易崩溃（软件&amp;心态），甚至左侧Explorer遇到大文件夹都要经常转很久才能进去，或是干脆就直接转崩了。我怀疑他们要分析每个文件夹所有文件之后再显示列表。。。总之，慎重。</p><p><img data-src="image-20201227160344273.png" alt="image-20201227160344273"></p></li></ul><h2 id="pythonic-method"><a class="markdownIt-Anchor" href="#pythonic-method"></a> Pythonic Method</h2><p>虽然前面介绍了几个软件，但是说实话，处理一两个可以，批量处理几十几百甚至几十万就有点力不从心了。所以最后还是狠下心研究了一遍Python处理这些数据的方法，写出了一批适合我项目用的函数。不一定适合所有人，但可以作为参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd</span><br><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Point, LineString, Polygon</span><br><span class="line"><span class="keyword">import</span> rasterio <span class="keyword">as</span> rio</span><br><span class="line"><span class="keyword">from</span> rasterio.mask <span class="keyword">import</span> mask</span><br><span class="line"><span class="keyword">from</span> rasterio.warp <span class="keyword">import</span> calculate_default_transform, reproject, Resampling</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    piece = np.array([i.bounds <span class="keyword">for</span> i <span class="keyword">in</span> shp[<span class="string">&#x27;geometry&#x27;</span>]])</span><br><span class="line">    minx = piece[:,<span class="number">0</span>].<span class="built_in">min</span>()</span><br><span class="line">    miny = piece[:,<span class="number">1</span>].<span class="built_in">min</span>()</span><br><span class="line">    maxx = piece[:,<span class="number">2</span>].<span class="built_in">max</span>()</span><br><span class="line">    maxy = piece[:,<span class="number">3</span>].<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">return</span> minx, miny, maxx, maxy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">edge_length</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the x and y edge length for a ceratin shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(maxx-minx,<span class="number">3</span>), <span class="built_in">round</span>(maxy-miny,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shape2latlong</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the shapefile unit from meters/other units to lat/long.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.to_crs(epsg=<span class="number">4326</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_latlong</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the latitude-longitude bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shp = shape2latlong(shp)</span><br><span class="line">    <span class="keyword">return</span> bbox(shp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_polygon</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Return the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> Polygon([(minx, miny), (minx, maxy), (maxx,maxy), (maxx, miny)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_polygon</span>(<span class="params">shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Merge a shapefile to one single polygon.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.dissolve(by=<span class="string">&#x27;Id&#x27;</span>).iloc[<span class="number">0</span>].geometry</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">polygon2geojson</span>(<span class="params">polygon</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn a polygon to a geojson format string.</span></span><br><span class="line"><span class="string">        This is used for rasterio mask operation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(polygon) == Polygon:</span><br><span class="line">        polygon = gpd.GeoSeries(polygon)</span><br><span class="line">    <span class="keyword">return</span> [json.loads(polygon.to_json())[<span class="string">&#x27;features&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;geometry&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sen2rgb</span>(<span class="params">img, scale=<span class="number">30</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the 12 channel float32 format sentinel-2 images to a RGB uint8 image. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (img[(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>),]/<span class="number">256</span>*scale).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cropbyshp</span>(<span class="params">raster, shp</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Crop a raster using a shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Reproject the shapefile to the same crs of raster.</span></span><br><span class="line">    shp = shp.to_crs(&#123;<span class="string">&quot;init&quot;</span>: <span class="built_in">str</span>(raster.crs)&#125;)</span><br><span class="line">    <span class="comment"># Compute the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line">    bbpoly = bbox_polygon(shp)</span><br><span class="line">    <span class="comment"># Execute the mask operation.</span></span><br><span class="line">    out_img, out_transform = mask(dataset=raster, shapes=polygon2geojson(bbpoly), crop=<span class="literal">True</span>, all_touched=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> out_img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_raster</span>(<span class="params">raster, path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Write a created raster object to file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(</span><br><span class="line">        path,</span><br><span class="line">        <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        **raster.meta</span><br><span class="line">    ) <span class="keyword">as</span> dst:</span><br><span class="line">        dst.write(raster.read())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sen_reproject</span>(<span class="params">src, dst_crs, out_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Reproject a raster to a new CRS coordinate, and save it in out_path.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        src: Input raster.</span></span><br><span class="line"><span class="string">        dst_crs: Target CRS. String.</span></span><br><span class="line"><span class="string">        out_path: The path of the output file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    transform, width, height = calculate_default_transform(</span><br><span class="line">        src.crs, dst_crs, src.width, src.height, *src.bounds)</span><br><span class="line">    kwargs = src.meta.copy()</span><br><span class="line">    kwargs.update(&#123;</span><br><span class="line">        <span class="string">&#x27;crs&#x27;</span>: dst_crs,</span><br><span class="line">        <span class="string">&#x27;transform&#x27;</span>: transform,</span><br><span class="line">        <span class="string">&#x27;width&#x27;</span>: width,</span><br><span class="line">        <span class="string">&#x27;height&#x27;</span>: height</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(out_path, <span class="string">&#x27;w&#x27;</span>, **kwargs) <span class="keyword">as</span> dst:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, src.count + <span class="number">1</span>):</span><br><span class="line">            reproject(</span><br><span class="line">                source=rio.band(src, i),</span><br><span class="line">                destination=rio.band(dst, i),</span><br><span class="line">                src_transform=src.transform,</span><br><span class="line">                src_crs=src.crs,</span><br><span class="line">                dst_transform=transform,</span><br><span class="line">                dst_crs=dst_crs,</span><br><span class="line">                resampling=Resampling.cubic)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mask_A_by_B</span>(<span class="params">A, B</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Generate a mask from B, and applied it to A.</span></span><br><span class="line"><span class="string">        All 0 values are excluded.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mask = B.<span class="built_in">sum</span>(axis=<span class="number">0</span>)&gt;<span class="number">1e-3</span></span><br><span class="line">    masked_A = mask*A</span><br><span class="line">    <span class="keyword">return</span> masked_A</span><br></pre></td></tr></table></figure><h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2><ol><li><a href="https://automating-gis-processes.github.io/2016/course-info.html">Python GIS 超完整教程</a></li><li><a href="https://zia207.github.io/geospatial-python.io/">Professor Zia’s Personal Website</a></li><li><a href="https://blog.csdn.net/sinat_28853941/article/details/78511167">sentinel-2数据下载 大气校正 转ENVI格式</a></li><li><a href="https://blog.csdn.net/lidahuilidahui/article/details/102765420">03-SNAP处理Sentinel-2 L2A级数据（一）</a></li><li><a href="https://clouds.eos.ubc.ca/~phil/courses/atsc301/html/rasterio_demo.html">UBC Course Notebook</a></li><li><a href="https://zhuanlan.zhihu.com/p/31010043">利用Sen2cor对哨兵2号（Sentinel-2）L1C多光谱数据进行辐射定标和大气校正</a></li><li><a href="https://sentinelhub-py.readthedocs.io/en/latest/">Documentation of Sentinel Hub Python package</a></li><li><a href="https://github.com/sentinel-hub/sentinelhub-py">sentinelhub-py</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;数据来源&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#数据来源&quot;&gt;&lt;/a&gt; 数据来源&lt;/h2&gt;
&lt;h3 id=&quot;卫星类型&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#卫星类型&quot;&gt;&lt;/a&gt; 卫星类型&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;10m &#92;times 10m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;20m &#92;times 20m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;60&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mn&gt;60&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;60m &#92;times 60m&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.72777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;×&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;mord mathnormal&quot;&gt;m&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，bands中心波长从442.3nm到2185.7nm。&lt;/p&gt;
&lt;img data-src=&quot;image-20201213154533413.png&quot; alt=&quot;image-20201213154533413&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="satellite" scheme="https://www.miracleyoo.com/tags/satellite/"/>
    
  </entry>
  
</feed>
