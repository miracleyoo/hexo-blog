<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Miracleyoo</title>
  
  
  <link href="https://www.miracleyoo.com/atom.xml" rel="self"/>
  
  <link href="https://www.miracleyoo.com/"/>
  <updated>2022-11-05T05:41:36.302Z</updated>
  <id>https://www.miracleyoo.com/</id>
  
  <author>
    <name>Miracle Yoo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>相机校准 相机标定 Intrinsic/Extrinsic Calibration详解 绝对Extrinsic矩阵测得实操 Event Camera/DVS</title>
    <link href="https://www.miracleyoo.com/2022/11/04/camera-calibration/"/>
    <id>https://www.miracleyoo.com/2022/11/04/camera-calibration/</id>
    <published>2022-11-05T05:07:36.000Z</published>
    <updated>2022-11-05T05:41:36.302Z</updated>
    
    <content type="html"><![CDATA[<p>先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关于如何通过实验方法标定得到相对于坐标原点（O点，Origin）的Extrinsic Matrix的讲解，尤其是缺少中文教程。于是我就把参考各种英文资料（主要是一些大学的slides和有关采集Mocap数据集的文献）总结的一些理论推导和实验方法，以及实际能用的代码整理得到了本文。</p><p>首先是几张非常重要的Slides，后面都会refer到，可以先自行熟悉下。另外，本篇不是100%从零开始的教程，篇幅限制并无法展开所有的细节，若想深度理解，请自行结合几个大学（CMU，Stanford）相应的Slides一起学习。</p><p><img src="/2022/11/04/camera-calibration/image-20221021152707045-1667622796441-1.png" alt="image-20221021152707045" style="zoom:33%;"></p><figure><img src="/2022/11/04/camera-calibration/image-20221021165722788-1667622796442-2.png" alt="image-20221021165722788"><figcaption aria-hidden="true">image-20221021165722788</figcaption></figure><h2 id="coordinates">Coordinates</h2><ul><li>在整个相机的投影与校准过程中，一共涉及3个坐标系。它们分别是：<ol type="1"><li>世界坐标系：以空间中某点为原点建立欧拉坐标系，设定xyz方向后形成的坐标系。</li><li>相机坐标系。该坐标系的原点是相机的焦点。焦点一般在相机内部，也可能落在的相机外部，这取决于focal length。坐标系的指向：x和y就是相平面的横纵坐标方向（相机视角方向），z是与xy平面垂直的方向，亦即镜头指向的前方。</li><li>图像坐标系（也可以分成两个：图像坐标系(m)和像素坐标系(pixel)）。值得注意的一点是每个像素并不是真正的一个点，pixel坐标系所代表的整数值是每个像素点的中心。</li></ol></li><li>考虑功能性，还有一个同质坐标系，用于实际运算。</li></ul><p><img src="/2022/11/04/camera-calibration/image-20221021144051423-1667622796442-3.png" alt="image-20221021144051423" style="zoom: 25%;"></p><h2 id="intrinsic">Intrinsic</h2><ul><li><p>理想状况下（无Skewness和Distortion），Intrinsic 矩阵Encode的信息有：Focal Length、Image Sensor的长宽（in pixel），每像素代表的米数(pixel/m)，也即相机的分辨率。</p></li><li><p>非理想情况下，Skewness和Distortion也会被放到Intrinsic中。</p></li><li><p>关于当提高/降低分辨率时候的Intrinsic变化：<span class="math inline">\(k,l,c_x,c_y\)</span>都要乘以分辨率提高的系数。</p><figure><img src="/2022/11/04/camera-calibration/image-20221026155238653-1667622796442-4.png" alt="image-20221026155238653"><figcaption aria-hidden="true">image-20221026155238653</figcaption></figure></li><li><p>参见Intrinsic的计算过程，由于计算时已经考虑了目标物体深度对成像位置的影响，所以Intrinsic其实是包含了透视(perspective)信息的。</p></li><li><p>Intrinsic可使相机坐标系转化为图片坐标系。</p></li></ul><h2 id="extrinsic">Extrinsic</h2><ul><li><p>Extrinsic 可以看作是两个矩阵写在了一起：旋转矩阵R和平移矩阵T。前三列是R，最后一列是T。 其实，虽然经常写作<span class="math inline">\([R|T]\)</span>，但事实上还有一个相似变换S，这个S是个对角线矩阵，对角线上的值为<span class="math inline">\([S_x, S_y, S_z, 1]\)</span>。S直接和R乘在一起，与T无关。</p><p><img src="/2022/11/04/camera-calibration/image-20221026162455644-1667622796442-5.png" alt="image-20221026162455644" style="zoom:50%;"></p></li><li><p>Extrinsic可使世界坐标系转化为相机坐标系。</p></li></ul><h2 id="skewness-and-distortion">Skewness and Distortion</h2><ul><li><p>Skewness指的是相机Sensor的两个轴不垂直，即xy之间有一个小夹角。通常这不会发生，但如果有制造方面的问题，这也是可能的。</p></li><li><p>相机的Skewness</p></li><li><p>Skewness的解决方法是把这个夹角找到，并在Intrinsic中反映出来。</p><figure><img src="/2022/11/04/camera-calibration/image-20221026154924465-1667622796442-6.png" alt="image-20221026154924465"><figcaption aria-hidden="true">image-20221026154924465</figcaption></figure></li><li><p>Distortion包含：</p><ul><li><em>Radial Distortion</em> (径向畸变)：</li><li><em>Tangential distortion</em> (切向畸变)：本质上是相平面和相机坐标系存在一个夹角，即“图像Sensor和镜头截面不平行”。</li></ul></li><li><p>关于Distortion的计算：</p><figure><img src="/2022/11/04/camera-calibration/image-20221026163826434-1667622796442-7.png" alt="image-20221026163826434"><figcaption aria-hidden="true">image-20221026163826434</figcaption></figure></li></ul><h2 id="homogeneous-coordinates">Homogeneous Coordinates</h2><ul><li>同质坐标的主要用意是把本来在分母上的z（深度）给挪走，以便让投影这个Transformation从non-linear变成Linear。</li><li>注意同质坐标虽然在视觉效果上是在原本的坐标(u,v)或(x,y,z)下面加了一个1，但是实际上这个1在欧式坐标系中并不存在。当我们后面列出方程校准时，应该回到原本的欧式坐标系解。</li></ul><h2 id="imu">IMU</h2><ul><li>IMU输出三个方向角速度和三个轴向加速度的值，使用时也需要校准。</li><li>具体校准方法参见Kalibr和DV，因为我没用上，所以不多展开。</li></ul><h2 id="dvs">DVS</h2><ul><li>DVS的校准主要分为两种方法：<ul><li>一种是直接用paired的RGB进行校准，毕竟这里的RGB和DVS share同一组透镜。</li><li>如果没有这个RGB，就直接用accumulate的frame做校准。</li></ul></li></ul><h2 id="methods">Methods</h2><h3 id="解方程直接校准p矩阵">解方程直接校准P矩阵</h3><ul><li><p>在Paper <em>DHP19: Dynamic Vision Sensor 3D Human Pose Dataset</em>里， 他们采用的方法是：直接在经过Mocap校准的空间中放置一系列Markers，然后在DVS的RGB（APS）输出frame中直接进行手动标注，得到其在image plane中的<span class="math inline">\((u, v)\)</span>坐标， 然后解方程。</p><p><img src="/2022/11/04/camera-calibration/image-20221021155115039-1667622796442-8.png" alt="image-20221021155115039" style="zoom:40%;"></p></li><li><p>上图中提到了一个点：从投影矩阵计算相机坐标系的原点，即相机的焦点位置的方法：<span class="math inline">\(C=Q^{-1}c_4\)</span>。具体的推理其实很简单，主要就靠一个条件公式：<span class="math inline">\(PC=0\)</span>，即原点的投影是0。</p></li><li><p>细节上，他们用了38个Marker，并8次改变它们的位置，通过最小平方法解得最接近的11个P中参数值。这里的最小平方法的意义在于通过增加数据点取平均P值来减小误差。其实11组式子就够了，但这里还是用了<span class="math inline">\(8\times38\times2\)</span>个公式，就在于此。</p></li><li><p>具体的最小平方法介绍及代码：<a href="https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html">Link</a></p></li><li><p>这个全矩阵P其实包含了Camera Intrinsic <em>K</em>， Camera Extrinsic <em>RT</em>, 以及Camera Skewness。</p></li><li><p>理论：</p><figure><img src="/2022/11/04/camera-calibration/image-20221026175034098-1667622796442-9.png" alt="image-20221026175034098"><figcaption aria-hidden="true">image-20221026175034098</figcaption></figure><p><img src="/2022/11/04/camera-calibration/image-20221026175051047-1667622796442-10.png" alt="image-20221026175051047" style="zoom:50%;"></p></li><li><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix using Numpy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svd_calibration</span>(<span class="params">points_3d, points_2d</span>):</span></span><br><span class="line">    <span class="comment"># points_3d: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># points_2d: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">0</span>] == points_2d.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> points_3d.shape[<span class="number">1</span>] == <span class="number">3</span></span><br><span class="line">    <span class="keyword">assert</span> points_2d.shape[<span class="number">1</span>] == <span class="number">2</span></span><br><span class="line">    num_points = points_3d.shape[<span class="number">0</span>]</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">0</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = *(points_3d[i, :]), <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = *(-points_2d[i, <span class="number">1</span>] * points_3d[i, :]), -points_2d[i, <span class="number">0</span>]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Least Square Calibration for Camera Projection Matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">least_square_calibrate_camera_projection_matrix_np</span>(<span class="params">x,y,z,u,v</span>):</span></span><br><span class="line">    <span class="comment"># x,y,z: 3D points in world coordinate</span></span><br><span class="line">    <span class="comment"># u,v: 2D points in image coordinate</span></span><br><span class="line">    <span class="comment"># return: projection matrix</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(x) == <span class="built_in">len</span>(y) == <span class="built_in">len</span>(z) == <span class="built_in">len</span>(u) == <span class="built_in">len</span>(v)</span><br><span class="line">    num_points = <span class="built_in">len</span>(x)</span><br><span class="line">    A = np.zeros((<span class="number">2</span> * num_points, <span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points):</span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">0</span>:<span class="number">4</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i, <span class="number">8</span>:<span class="number">12</span>] = -u[i] * x[i], -u[i] * y[i], -u[i] * z[i], -u[i]</span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">4</span>:<span class="number">8</span>] = x[i], y[i], z[i], <span class="number">1</span></span><br><span class="line">        A[<span class="number">2</span> * i + <span class="number">1</span>, <span class="number">8</span>:<span class="number">12</span>] = -v[i] * x[i], -v[i] * y[i], -v[i] * z[i], -v[i]</span><br><span class="line">    U, S, V = np.linalg.svd(A)</span><br><span class="line">    P = V[:，-<span class="number">1</span>].reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">return</span> P</span><br></pre></td></tr></table></figure></li><li><p>注意：SVD这里是用于解决Least Squares Problem的，如果直接用<code>np.linalg.lstsq</code>函数的话（b取全0），会解得一个全0矩阵（因为0永远是一个解）。</p></li><li><p>SVD的解法细节：</p><figure><img src="/2022/11/04/camera-calibration/image-20221027230723755-1667622796442-11.png" alt="image-20221027230723755"><figcaption aria-hidden="true">image-20221027230723755</figcaption></figure></li><li><p>解SVD的时候可以选择把P矩阵右下角<span class="math inline">\(P_{(3,4)}\)</span>设为1。不设是homogeneous解法，设了之后是inhomogeneous。</p></li></ul><h3 id="kalibr">kalibr</h3><ul><li><p><a href="https://github.com/ethz-asl/kalibr">Link</a></p></li><li><p>Used for:</p><ol type="1"><li><strong>Multi-Camera Calibration</strong>: Intrinsic and extrinsic calibration of a camera-systems with non-globally shared overlapping fields of view</li><li><strong>Visual-Inertial Calibration (CAM-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a camera-system along with IMU intrinsic parameters</li><li><strong>Multi-Inertial Calibration (IMU-IMU)</strong>: Spatial and temporal calibration of an IMU w.r.t a base inertial sensor along with IMU intrinsic parameters (requires 1-aiding camera sensor).</li><li><strong>Rolling Shutter Camera Calibration</strong>: Full intrinsic calibration (projection, distortion and shutter parameters) of rolling shutter cameras.</li></ol></li><li><p>简单说就是主攻多相机/IMU系统。<a href="https://github.com/ethz-asl/kalibr/wiki/multiple-camera-calibration">多个相机</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/Multi-IMU-and-IMU-intrinsic-calibration">多个IMU</a>，<a href="https://github.com/ethz-asl/kalibr/wiki/camera-imu-calibration">相机+IMU</a>等。</p></li><li><p>校准出来的Extrinsic结果并不是相对原点绝对的，而是多个设备间相对的。比如IMU+Cam校准出来的Extrinsic就是IMU相对于Cam坐标的变换。</p><p>引用一段<a href="https://github.com/ethz-asl/kalibr/wiki/yaml-formats">原话</a>：</p><blockquote><ul><li><strong>T_cn_cnm1</strong> camera extrinsic transformation, always with respect to the last camera in the chain (e.g. cam1: T_cn_cnm1 = T_c1_c0, takes cam0 to cam1 coordinates)</li><li><strong>T_cam_imu</strong> IMU extrinsics: transformation from IMU to camera coordinates (T_c_i)</li><li><strong>timeshift_cam_imu</strong> timeshift between camera and IMU timestamps in seconds (t_imu = t_cam + shift)</li></ul></blockquote></li><li><p>综上所述，Kalibr并不是满足我们需求的校准方案。</p></li></ul><h3 id="dv-calibration">DV Calibration</h3><ul><li><a href="https://inivation.gitlab.io/dv/dv-docs/docs/tutorial-calibration/">Tutorial Link</a>, <a href="https://gitlab.com/inivation/dv/dv-imu-cam-calibration">Code Link</a></li><li>单个多个DVS都可以。</li><li>基于Kalibr的方案。</li><li>对于单个DVS，校准主要进行的是undistortion，且可以在校准后直接应用于相机后续的图像，让后面的record都不再有失真。</li><li>这里的校准可以有效应对之前Upal教授提出的扭曲问题，应在后续操作中应用。</li></ul><h3 id="opencv-camera-calibration">OpenCV Camera Calibration</h3><ul><li><p>这个校准会使用chessboard，而关于3d坐标，他们用了棋盘上两个相邻的点的实际距离是已知的这个特性（因为打印的标准棋盘，间距是固定的，如30mm），来提供相应的3D坐标信息。</p></li><li><p>这个校准会分别输出Intrinsic matrix (mtx), rotation matrix (R, rvecs), translation matrix (T, tvecs), Distortion coefficients (dist)。这些输出可以直接被用来纠偏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate camera matrixes</span></span><br><span class="line">ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-<span class="number">1</span>], <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">&#x27;left12.jpg&#x27;</span>)</span><br><span class="line">h,  w = img.shape[:<span class="number">2</span>]</span><br><span class="line">newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), <span class="number">1</span>, (w,h))</span><br><span class="line"></span><br><span class="line"><span class="comment"># undistort</span></span><br><span class="line">dst = cv.undistort(img, mtx, dist, <span class="literal">None</span>, newcameramtx)</span><br><span class="line"><span class="comment"># crop the image</span></span><br><span class="line">x, y, w, h = roi</span><br><span class="line">dst = dst[y:y+h, x:x+w]</span><br><span class="line">cv.imwrite(<span class="string">&#x27;calibresult.png&#x27;</span>, dst)</span><br></pre></td></tr></table></figure></li><li><p>关于OpenCV校准出来的Extrinsic Matrix，由于世界坐标系必定有一个原点，所以它们也是毫无疑问有一个原点的。但这个世界坐标系原点实际上只有参考意义（第一张校准图的左上角棋盘点），并无法直接使用。同时，相机坐标系的原点是相机的焦点，而这个焦点也是几乎不可预知和测量位置的（它可能在相机内部或外部，但校准并不会告诉你这个点位置，所以你也无法通过直接测量相机O点和实际世界O点之间的相对位置来纠正Extrinsic。）</p><blockquote><p><a href="https://www.appsloveworld.com/opencv/100/91/opencv-camera-calibration-world-coordinate-system-origin">Link</a>: I believe it used to be the position of the top-left corner of the checkerboard in the first calibration image, but it may have changed. You can visualized it by writing a few lines of code that project point (0,0,0) (in calibrated scene coordinates) in all the calibration images, then plotting its projected image coordinates on top of the image themselves.</p><p>You should really not depend on it being anything meaningful, and instead locate a separate feature in 3D and roto-translate the reference frame to it after calibration.</p></blockquote></li><li><p>实际上，不要想通过OpenCV的校准来直接得到有实际意义的Extrinsic，若想得到，请自行用前面提到的Method 1来实际label一些已知3D坐标的Markers对应的2D点，用Least Squares解得。</p></li><li><p>但是，OpenCV的校准可以提供有效的Distortion Coefficient和Intrinsic，并可直接被用于畸变补偿。</p></li></ul><h2 id="reference">Reference</h2><h3 id="blogswebsites">Blogs/Websites</h3><ul><li><a href="****https://pythonnumericalmethods.berkeley.edu/notebooks/chapter16.04-Least-Squares-Regression-in-Python.html****">Least Squares Regression in Python</a></li><li><a href="https://math.stackexchange.com/questions/974193/why-does-svd-provide-the-least-squares-and-least-norm-solution-to-a-x-b">Why does SVD provide the least squares and least norm solution to 𝐴𝑥=𝑏?</a></li><li><a href="https://math.stackexchange.com/questions/772039/how-does-the-svd-solve-the-least-squares-problem">How does the SVD solve the least squares problem?</a></li><li><a href="https://www.quora.com/What-is-the-real-world-coordinate-system-camera-calibration-refer-to-in-computer-vision">What is the "real world coordinate system" camera calibration refer to in computer vision?</a></li><li><a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">numpy linalg svd</a></li><li><a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html">OpenCV Camera Calibration</a></li></ul><h3 id="slides">Slides</h3><ul><li><a href="https://s3.amazonaws.com/content.udacity-data.com/courses/ud810/slides/Unit-3/3C-L3.pdf#fromHistory">Udacity CS4495/6495 Introduction to Computer Vision 3C-L3 Calibrating cameras</a></li><li><a href="https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf">CMU - Camera Matrix</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture2_camera_models.pdf">Stanford - Lecture 2</a></li><li><a href="https://cvgl.stanford.edu/teaching/cs231a_winter1314/lectures/lecture3_camera_calibration.pdf">Stanford - Lecture 3</a></li></ul><h3 id="papers">Papers</h3><ul><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">A Flexible New Technique for Camera Calibration</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;先提一下本文的发表理由。因为实验需要用到一个DAVIS 346的Event Camera，且项目需要通过Motion Capture（Mocap）得到人体的3D准确标定，所以就研究了一下相机的Intrinsic和绝对Extrinsic标定。令人吃惊的是，网上其实并没有太多关</summary>
      
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="camera calibration" scheme="https://www.miracleyoo.com/tags/camera-calibration/"/>
    
    <category term="camera matrix" scheme="https://www.miracleyoo.com/tags/camera-matrix/"/>
    
    <category term="CV" scheme="https://www.miracleyoo.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Lighting 完全攻略</title>
    <link href="https://www.miracleyoo.com/2021/03/11/pytorch-lightning/"/>
    <id>https://www.miracleyoo.com/2021/03/11/pytorch-lightning/</id>
    <published>2021-03-12T04:09:57.000Z</published>
    <updated>2021-03-12T22:05:55.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？</p><span id="more"></span><p>于是我第二次发现了Pytorch-Lightning。</p><p>真香。</p><p>但是问题还是来了。这个框架并没有因为香而变得更加易学。官网的教程很丰富，可以看出来开发者们在努力做了。但是很多相连的知识点都被分布在了不同的版块里，还有一些核心的理解要点并没有被强调出来，而是小字带过，这让我想做一个普惠的教程，包含所有我在学习过程中认为重要的概念，好用的参数，一些注意点、坑点，大量的示例代码段和一些核心问题的集中讲解。</p><p>最后，第三部分提供了一个我总结出来的易用于大型项目、容易迁移、易于复用的模板，有兴趣的可以去<a href="https://github.com/miracleyoo/pytorch-lightning-template">GitHub</a>试用。</p><h2 id="crucial">Crucial</h2><ul><li><p>Pytorch-Lighting 的一大特点是把模型和系统分开来看。模型是像Resnet18， RNN之类的纯模型， 而系统定义了一组模型如何相互交互，如GAN（生成器网络与判别器网络）、Seq2Seq（Encoder与Decoder网络）和Bert。同时，有时候问题只涉及一个模型，那么这个系统则可以是一个通用的系统，用于描述模型如何使用，并可以被复用到很多其他项目。</p></li><li><p>Pytorch-Lighting 的核心设计思想是“自给自足”。每个网络也同时包含了如何训练、如何测试、优化器定义等内容。</p></li></ul><figure><img src="/2021/03/11/pytorch-lightning/plres.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="推荐使用方法">推荐使用方法</h2><p>这一部分放在最前面，因为全文内容太长，如果放后面容易忽略掉这部分精华。</p><p>Pytorch-Lightning 是一个很好的库，或者说是pytorch的抽象和包装。它的好处是可复用性强，易维护，逻辑清晰等。缺点也很明显，这个包需要学习和理解的内容还是挺多的，或者换句话说，很重。如果直接按照官方的模板写代码，小型project还好，如果是大型项目，有复数个需要调试验证的模型和数据集，那就不太好办，甚至更加麻烦了。经过几天的摸索和调试，我总结出了下面这样一套好用的模板，也可以说是对Pytorch-Lightning的进一步抽象。</p><p>欢迎大家尝试这一套代码风格，如果用习惯的话还是相当方便复用的，也不容易半道退坑。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">|-data</span><br><span class="line">|-__init__.py</span><br><span class="line">|-data_interface.py</span><br><span class="line">|-xxxdataset1.py</span><br><span class="line">|-xxxdataset2.py</span><br><span class="line">|-...</span><br><span class="line">|-model</span><br><span class="line">|-__init__.py</span><br><span class="line">|-model_interface.py</span><br><span class="line">|-xxxmodel1.py</span><br><span class="line">|-xxxmodel2.py</span><br><span class="line">|-...</span><br><span class="line">|-main.py</span><br></pre></td></tr></table></figure><p>如果对每个模型直接上plmodule，对于已有项目、别人的代码等的转换将相当耗时。另外，这样的话，你需要给每个模型都加上一些相似的代码，如<code>training_step</code>，<code>validation_step</code>。显然，这并不是我们想要的，如果真的这样做，不但不易于维护，反而可能会更加杂乱。同理，如果把每个数据集类都直接转换成pl的DataModule，也会面临相似的问题。基于这样的考量，我建议使用上述架构：</p><ul><li><p>主目录下只放一个<code>main.py</code>文件。</p></li><li><p><code>data</code>和<code>modle</code>两个文件夹中放入<code>__init__.py</code>文件，做成包。这样方便导入。两个<code>init</code>文件分别是：</p><ul><li><code>from .data_interface import DInterface</code></li><li><code>from .model_interface import MInterface</code></li></ul></li><li><p>在<code>data_interface</code>中建立一个<code>class DInterface(pl.LightningDataModule):</code>用作所有数据集文件的接口。<code>__init__()</code>函数中import相应Dataset类，<code>setup()</code>进行实例化，并老老实实加入所需要的的<code>train_dataloader</code>, <code>val_dataloader</code>, <code>test_dataloader</code>函数。这些函数往往都是相似的，可以用几个输入args控制不同的部分。</p></li><li><p>同理，在<code>model_interface</code>中建立<code>class MInterface(pl.LightningModule):</code>类，作为模型的中间接口。<code>__init__()</code>函数中import相应模型类，然后老老实实加入<code>configure_optimizers</code>, <code>training_step</code>, <code>validation_step</code>等函数，用一个接口类控制所有模型。不同部分使用输入参数控制。</p></li><li><p><code>main.py</code>函数只负责：</p><ul><li>定义parser，添加parse项。</li><li>选好需要的<code>callback</code>函数们。</li><li>实例化<code>MInterface</code>, <code>DInterface</code>, <code>Trainer</code>。</li></ul><p>完事。</p></li></ul><h2 id="lightning-module">Lightning Module</h2><h3 id="简介">简介</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">主页面</a></p><ul><li><p>三个核心组件：</p><ul><li>模型</li><li>优化器</li><li>Train/Val/Test步骤</li></ul></li><li><p>数据流伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">    out = training_step(batch)</span><br><span class="line">    outs.append(out)</span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><p>等价Lightning代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    prediction = ...</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        <span class="comment"># do something with these</span></span><br></pre></td></tr></table></figure><p>我们需要做的，就是像填空一样，填这些函数。</p></li></ul><h3 id="组件与函数">组件与函数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api">API页面</a></p><ul><li><p>一个Pytorch-Lighting 模型必须含有的部件是：</p><ul><li><p><code>init</code>: 初始化，包括模型和系统的定义。</p></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.training_step"><code>training_step(self, batch, batch_idx)</code></a>: 即每个batch的处理函数。</p><blockquote><p>参数：</p><ul><li><strong>batch</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> | (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …) | [<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>, …]) – The output of your <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. A tensor, tuple or list.</li><li><strong>batch_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Integer displaying index of this batch</li><li><strong>optimizer_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – When using multiple optimizers, this argument will also be present.</li><li><strong>hiddens</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a>) – Passed in if <a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.params.truncated_bptt_steps"><code>truncated_bptt_steps</code></a> &gt; 0.</li></ul><p>返回值：Any of.</p><ul><li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>Tensor</code></a> - The loss tensor</li><li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code></li><li><code>None</code> - Training will skip to the next batch</li></ul></blockquote><p>返回值无论如何也需要有一个loss量。如果是字典，要有这个key。没loss这个batch就被跳过了。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y, z = batch</span><br><span class="line">    out = self.encoder(x)</span><br><span class="line">    loss = self.loss(out, x)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple optimizers (e.g.: GANs)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx, optimizer_idx</span>):</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># do training_step with encoder</span></span><br><span class="line">    <span class="keyword">if</span> optimizer_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># do training_step with decoder</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># Truncated back-propagation through time</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx, hiddens</span>):</span></span><br><span class="line">    <span class="comment"># hiddens are the hidden states from the previous truncated backprop step</span></span><br><span class="line">    ...</span><br><span class="line">    out, hiddens = self.lstm(data, hiddens)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;hiddens&#x27;</span>: hiddens&#125;</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization"><code>configure_optimizers</code></a>: 优化器定义，返回一个优化器，或数个优化器，或两个List（优化器，Scheduler）。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># most cases</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    opt = Adam(self.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="keyword">return</span> opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># multiple optimizer case (e.g.: GAN)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> generator_opt, disriminator_opt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    generator_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    disriminator_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> [generator_opt, disriminator_opt], [discriminator_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with step-based learning rate schedulers</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    gen_sched = &#123;<span class="string">&#x27;scheduler&#x27;</span>: ExponentialLR(gen_opt, <span class="number">0.99</span>),</span><br><span class="line">                 <span class="string">&#x27;interval&#x27;</span>: <span class="string">&#x27;step&#x27;</span>&#125;  <span class="comment"># called after each training step</span></span><br><span class="line">    dis_sched = CosineAnnealing(discriminator_opt, T_max=<span class="number">10</span>) <span class="comment"># called every epoch</span></span><br><span class="line">    <span class="keyword">return</span> [gen_opt, dis_opt], [gen_sched, dis_sched]</span><br><span class="line"></span><br><span class="line"><span class="comment"># example with optimizer frequencies</span></span><br><span class="line"><span class="comment"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">configure_optimizers</span>(<span class="params">self</span>):</span></span><br><span class="line">    gen_opt = Adam(self.model_gen.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    dis_opt = Adam(self.model_disc.parameters(), lr=<span class="number">0.02</span>)</span><br><span class="line">    n_critic = <span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: dis_opt, <span class="string">&#x27;frequency&#x27;</span>: n_critic&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;optimizer&#x27;</span>: gen_opt, <span class="string">&#x27;frequency&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li></ul></li><li><p>可以指定的部件有：</p><ul><li><code>forward</code>: 和正常的<code>nn.Module</code>一样，用于inference。内部调用时：<code>y=self(batch)</code></li><li><code>training_step_end</code>: 只在使用多个node进行训练且结果涉及如softmax之类需要全部输出联合运算的步骤时使用该函数。同理，<code>validation_step_end</code>/<code>test_step_end</code>。</li><li><code>training_epoch_end</code>:<ul><li>在一个训练epoch结尾处被调用。</li><li>输入参数：一个List，List的内容是前面<code>training_step()</code>所返回的每次的内容。</li><li>返回：None</li></ul></li><li><code>validation_step(self, batch, batch_idx)</code>/<code>test_step(self, batch, batch_idx)</code>:<ul><li>没有返回值限制，不一定非要输出一个<code>val_loss</code>。</li></ul></li><li><code>validation_epoch_end</code>/<code>test_epoch_end</code>:</li></ul></li><li><p>工具函数有：</p><ul><li><p><code>freeze</code>：冻结所有权重以供预测时候使用。仅当已经训练完成且后面只测试时使用。</p></li><li><p><code>print</code>：尽管自带的<code>print</code>函数也可以使用，但如果程序运行在分布式系统时，会打印多次。而使用<code>self.print()</code>则只会打印一次。</p></li><li><p><code>log</code>：像是TensorBoard等log记录器，对于每个log的标量，都会有一个相对应的横坐标，它可能是batch number或epoch number。而<code>on_step</code>就表示把这个log出去的量的横坐标表示为当前batch，而<code>on_epoch</code>则表示将log的量在整个epoch上进行累积后log，横坐标为当前epoch。</p><table><thead><tr class="header"><th>LightningMoule Hook</th><th>on_step</th><th>on_epoch</th><th>prog_bar</th><th>logger</th></tr></thead><tbody><tr class="odd"><td>training_step</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr class="even"><td>training_step_end</td><td>T</td><td>F</td><td>F</td><td>T</td></tr><tr class="odd"><td>training_epoch_end</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr class="even"><td>validation_step*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr class="odd"><td>validation_step_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr><tr class="even"><td>validation_epoch_end*</td><td>F</td><td>T</td><td>F</td><td>T</td></tr></tbody></table><p><code>*</code> also applies to the test loop</p><blockquote><p>参数</p><ul><li><strong>name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – key name</li><li><strong>value</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>) – value name</li><li><strong>prog_bar</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the progress bar</li><li><strong>logger</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True logs to the logger</li><li><strong>on_step</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs at this step. None auto-logs at the training_step but not validation/test_step</li><li><strong>on_epoch</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>]) – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</li><li><strong>reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – reduction function over step values for end of epoch. Torch.mean by default</li><li><strong>tbptt_reduce_fx</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Callable"><code>Callable</code></a>) – function to reduce on truncated back prop</li><li><strong>tbptt_pad_token</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – token to use for padding</li><li><strong>enable_graph</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, will not auto detach the graph</li><li><strong>sync_dist</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – if True, reduces the metric across GPUs/TPUs</li><li><strong>sync_dist_op</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>, <a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>]) – the op to sync across GPUs/TPUs</li><li><strong>sync_dist_group</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://docs.python.org/3/library/typing.html#typing.Any"><code>Any</code></a>]) – the ddp group</li></ul></blockquote></li><li><p><code>log_dict</code>：和<code>log</code>函数唯一的区别就是，<code>name</code>和<code>value</code>变量由一个字典替换。表示同时log多个值。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;acc&#x27;</span>: acc, ..., <span class="string">&#x27;metric_n&#x27;</span>: metric_n&#125;</span><br><span class="line">self.log_dict(values)</span><br></pre></td></tr></table></figure></li><li><p><code>save_hyperparameters</code>：储存<code>init</code>中输入的所有超参。后续访问可以由<code>self.hparams.argX</code>方式进行。同时，超参表也会被存到文件中。</p></li></ul></li><li><p>函数内建变量：</p><ul><li><code>device</code>：可以使用<code>self.device</code>来构建设备无关型tensor。如：<code>z = torch.rand(2, 3, device=self.device)</code>。</li><li><code>hparams</code>：含有所有前面存下来的输入超参。</li><li><code>precision</code>：精确度。常见32和16。</li></ul></li></ul><h3 id="要点">要点</h3><ul><li>如果准备使用DataParallel，在写<code>training_step</code>的时候需要调用forward函数，<code>z=self(x)</code></li></ul><h3 id="模板">模板</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitModel</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">training_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">training_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">validation_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_step</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_step_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">test_epoch_end</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">configure_optimizers</span>(<span class="params">...</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">any_extra_hook</span>(<span class="params">...</span>)</span></span><br></pre></td></tr></table></figure><h2 id="trainer">Trainer</h2><h3 id="基础使用">基础使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightningModule()</span><br><span class="line"></span><br><span class="line">trainer = Trainer()</span><br><span class="line">trainer.fit(model, train_dataloader, val_dataloader)</span><br></pre></td></tr></table></figure><p>如果连<code>validation_step</code>都没有，那<code>val_dataloader</code>也就算了。</p><h3 id="伪代码与hooks">伪代码与hooks</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks">Hooks页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">...</span>):</span></span><br><span class="line">    on_fit_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> global_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># prepare data is called on GLOBAL_ZERO only</span></span><br><span class="line">        prepare_data()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> gpu/tpu <span class="keyword">in</span> gpu/tpus:</span><br><span class="line">        train_on_device(model.copy())</span><br><span class="line"></span><br><span class="line">    on_fit_end()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_on_device</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="comment"># setup is called PER DEVICE</span></span><br><span class="line">    setup()</span><br><span class="line">    configure_optimizers()</span><br><span class="line">    on_pretrain_routine_start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_loop()</span><br><span class="line"></span><br><span class="line">    teardown()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_loop</span>():</span></span><br><span class="line">    on_train_epoch_start()</span><br><span class="line">    train_outs = []</span><br><span class="line">    <span class="keyword">for</span> train_batch <span class="keyword">in</span> train_dataloader():</span><br><span class="line">        on_train_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----- train_step methods -------</span></span><br><span class="line">        out = training_step(batch)</span><br><span class="line">        train_outs.append(out)</span><br><span class="line"></span><br><span class="line">        loss = out.loss</span><br><span class="line"></span><br><span class="line">        backward()</span><br><span class="line">        on_after_backward()</span><br><span class="line">        optimizer_step()</span><br><span class="line">        on_before_zero_grad()</span><br><span class="line">        optimizer_zero_grad()</span><br><span class="line"></span><br><span class="line">        on_train_batch_end(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> should_check_val:</span><br><span class="line">            val_loop()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end training epoch</span></span><br><span class="line">    logs = training_epoch_end(outs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_loop</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    on_validation_epoch_start()</span><br><span class="line">    val_outs = []</span><br><span class="line">    <span class="keyword">for</span> val_batch <span class="keyword">in</span> val_dataloader():</span><br><span class="line">        on_validation_batch_start()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------- val step methods -------</span></span><br><span class="line">        out = validation_step(val_batch)</span><br><span class="line">        val_outs.append(out)</span><br><span class="line"></span><br><span class="line">        on_validation_batch_end(out)</span><br><span class="line"></span><br><span class="line">    validation_epoch_end(val_outs)</span><br><span class="line">    on_validation_epoch_end()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set up for train</span></span><br><span class="line">    model.train()</span><br><span class="line">    torch.set_grad_enabled(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="推荐参数">推荐参数</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags">参数介绍（附视频）</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api">类定义与默认参数</a></p><ul><li><p><code>default_root_dir</code>：默认存储地址。所有的实验变量和权重全部会被存到这个文件夹里面。推荐是，每个模型有一个独立的文件夹。每次重新训练会产生一个新的<code>version_x</code>子文件夹。</p></li><li><p><code>max_epochs</code>：最大训练周期数。<code>trainer = Trainer(max_epochs=1000)</code></p></li><li><p><code>min_epochs</code>：至少训练周期数。当有Early Stop时使用。</p></li><li><p><code>auto_scale_batch_size</code>：在进行任何训练前自动选择合适的batch size。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (no scaling of batch size)</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run batch size scaling, result overrides hparams.batch_size</span></span><br><span class="line">trainer = Trainer(auto_scale_batch_size=<span class="string">&#x27;binsearch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the batch size</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>auto_select_gpus</code>：自动选择合适的GPU。尤其是在有GPU处于独占模式时候，非常有用。</p></li><li><p><code>auto_lr_find</code>：自动找到合适的初始学习率。使用了该<a href="https://arxiv.org/abs/1506.01186">论文</a>的技术。当且仅当执行<code>trainer.tune(model)</code>代码时工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># run learning rate finder, results override hparams.learning_rate</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run learning rate finder, results override hparams.my_lr_arg</span></span><br><span class="line">trainer = Trainer(auto_lr_find=<span class="string">&#x27;my_lr_arg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call tune to find the lr</span></span><br><span class="line">trainer.tune(model)</span><br></pre></td></tr></table></figure></li><li><p><code>precision</code>：精确度。正常是32，使用16可以减小内存消耗，增大batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 16-bit precision</span></span><br><span class="line">trainer = Trainer(precision=<span class="number">16</span>, gpus=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>val_check_interval</code>：进行Validation测试的周期。正常为1，训练1个epoch测试4次是0.25，每1000 batch测试一次是1000。</p><blockquote><ul><li>use (float) to check within a training epoch：此时这个值为一个epoch的百分比。每百分之多少测试一次。</li><li>use (int) to check every n steps (batches)：每多少个batch测试一次。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set 4 times during a training epoch</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check validation set every 1000 training batches</span></span><br><span class="line"><span class="comment"># use this when using iterableDataset and your dataset has no length</span></span><br><span class="line"><span class="comment"># (ie: production cases with streaming data)</span></span><br><span class="line">trainer = Trainer(val_check_interval=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#gpus"><code>gpus</code></a>：控制使用的GPU数。当设定为None时，使用cpu。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer (ie: train on CPU)</span></span><br><span class="line">trainer = Trainer(gpus=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># equivalent</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># int: train on 2 gpus</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># list: train on GPUs 1, 4 (by bus ordering)</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;1, 4&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1: train on all gpus</span></span><br><span class="line">trainer = Trainer(gpus=-<span class="number">1</span>)</span><br><span class="line">trainer = Trainer(gpus=<span class="string">&#x27;-1&#x27;</span>) <span class="comment"># equivalent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine with num_nodes to train on multiple GPUs across nodes</span></span><br><span class="line"><span class="comment"># uses 8 gpus in total</span></span><br><span class="line">trainer = Trainer(gpus=<span class="number">2</span>, num_nodes=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train only on GPUs 1 and 4 across nodes</span></span><br><span class="line">trainer = Trainer(gpus=[<span class="number">1</span>, <span class="number">4</span>], num_nodes=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#limit-train-batches"><code>limit_train_batches</code></a>：使用训练数据的百分比。如果数据过多，或正在调试，可以使用这个。值的范围为0~1。同样，有<code>limit_test_batches</code>，<code>limit_val_batches</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 25% of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run through only 10 batches of the training set each epoch</span></span><br><span class="line">trainer = Trainer(limit_train_batches=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#fast-dev-run"><code>fast_dev_run</code></a>：bool量。如果设定为true，会只执行一个batch的train, val 和 test，然后结束。仅用于debug。</p><blockquote><p>Setting this argument will disable tuner, checkpoint callbacks, early stopping callbacks, loggers and logger callbacks like <code>LearningRateLogger</code> and runs for only 1 epoch</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default used by the Trainer</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 1 train, val, test batch and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># runs 7 train, val, test batches and program ends</span></span><br><span class="line">trainer = Trainer(fast_dev_run=<span class="number">7</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="fit函数">.fit()函数</h3><p><code>Trainer.fit(model, train_dataloader=None, val_dataloaders=None, datamodule=None)</code>：输入第一个量一定是model，然后可以跟一个LigntningDataModule或一个普通的Train DataLoader。如果定义了Val step，也要有Val DataLoader。</p><blockquote><p>参数</p><ul><li><strong>datamodule</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule"><code>LightningDataModule</code></a>]) – A instance of <code>LightningDataModule</code>.</li><li><strong>model</strong> (<a href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule"><code>LightningModule</code></a>) – Model to fit.</li><li><strong>train_dataloader</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Optional"><code>Optional</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>]) – A Pytorch DataLoader with training samples. If the model has a predefined train_dataloader method this will be skipped.</li><li><strong>val_dataloaders</strong> (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><code>Union</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>, <a href="https://docs.python.org/3/library/typing.html#typing.List"><code>List</code></a>[<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>], <a href="https://docs.python.org/3/library/constants.html#None"><code>None</code></a>]) – Either a single Pytorch Dataloader or a list of them, specifying validation samples. If the model has a predefined val_dataloaders method this will be skipped</li></ul></blockquote><h3 id="其他要点">其他要点</h3><ul><li><code>.test()</code>若非直接调用，不会运行。<code>trainer.test()</code></li><li><code>.test()</code>会自动load最优模型。</li><li><code>model.eval()</code> and <code>torch.no_grad()</code> 在进行测试时会被自动调用。</li><li>默认情况下，<code>Trainer()</code>运行于CPU上。</li></ul><h3 id="使用样例">使用样例</h3><ol type="1"><li>手动添加命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">hparams</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer(gpus=hparams.gpus)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gpus&#x27;</span>, default=<span class="literal">None</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>自动添加所有<code>Trainer</code>会用到的命令行参数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>混合式，既使用<code>Trainer</code>相关参数，又使用一些自定义参数，如各种模型超参：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"><span class="keyword">import</span> pytorch_lightning <span class="keyword">as</span> pl</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> LightningModule, Trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    model = LightningModule()</span><br><span class="line">    trainer = Trainer.from_argparse_args(args)</span><br><span class="line">    trainer.fit(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">32</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--hidden_dim&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">128</span>)</span><br><span class="line">    parser = Trainer.add_argparse_args(</span><br><span class="line">        <span class="comment"># group the Trainer arguments together</span></span><br><span class="line">        parser.add_argument_group(title=<span class="string">&quot;pl.Trainer args&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure><h3 id="所有参数">所有参数</h3><blockquote><p><code>Trainer.``__init__</code>(<em>logger=True</em>, <em>checkpoint_callback=True</em>, <em>callbacks=None</em>, <em>default_root_dir=None</em>, <em>gradient_clip_val=0</em>, <em>process_position=0</em>, <em>num_nodes=1</em>, <em>num_processes=1</em>, <em>gpus=None</em>, <em>auto_select_gpus=False</em>, <em>tpu_cores=None</em>, <em>log_gpu_memory=None</em>, <em>progress_bar_refresh_rate=None</em>, <em>overfit_batches=0.0</em>, <em>track_grad_norm=- 1</em>, <em>check_val_every_n_epoch=1</em>, <em>fast_dev_run=False</em>, <em>accumulate_grad_batches=1</em>, <em>max_epochs=None</em>, <em>min_epochs=None</em>, <em>max_steps=None</em>, <em>min_steps=None</em>, <em>limit_train_batches=1.0</em>, <em>limit_val_batches=1.0</em>, <em>limit_test_batches=1.0</em>, <em>limit_predict_batches=1.0</em>, <em>val_check_interval=1.0</em>, <em>flush_logs_every_n_steps=100</em>, <em>log_every_n_steps=50</em>, <em>accelerator=None</em>, <em>sync_batchnorm=False</em>, <em>precision=32</em>, <em>weights_summary='top'</em>, <em>weights_save_path=None</em>, <em>num_sanity_val_steps=2</em>, <em>truncated_bptt_steps=None</em>, <em>resume_from_checkpoint=None</em>, <em>profiler=None</em>, <em>benchmark=False</em>, <em>deterministic=False</em>, <em>reload_dataloaders_every_epoch=False</em>, <em>auto_lr_find=False</em>, <em>replace_sampler_ddp=True</em>, <em>terminate_on_nan=False</em>, <em>auto_scale_batch_size=False</em>, <em>prepare_data_per_node=True</em>, <em>plugins=None</em>, <em>amp_backend='native'</em>, <em>amp_level='O2'</em>, <em>distributed_backend=None</em>, <em>move_metrics_to_cpu=False</em>, <em>multiple_trainloader_mode='max_size_cycle'</em>, <em>stochastic_weight_avg=False</em>)</p></blockquote><h3 id="log和return-loss到底在做什么">Log和return loss到底在做什么</h3><p>To add a training loop use the training_step method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitClassifier</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = model</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">         x, y = batch</span><br><span class="line">         y_hat = self.model(x)</span><br><span class="line">         loss = F.cross_entropy(y_hat, y)</span><br><span class="line">         <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>无论是<code>training_step</code>，还是<code>validation_step</code>，<code>test_step</code>返回值都是<code>loss</code>。返回的loss会被用一个list收集起来。</li></ul><p>Under the hood, Lightning does the following (pseudocode):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put model in train mode</span></span><br><span class="line">model.train()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    loss = training_step(batch)</span><br><span class="line">    losses.append(loss.detach())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><h4 id="training-epoch-level-metrics">Training epoch-level metrics</h4><p>If you want to calculate epoch-level metrics and log them, use the <code>.log</code> method</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logs metrics for each training_step,</span></span><br><span class="line">    <span class="comment"># and the average across the epoch, to the progress bar and logger</span></span><br><span class="line">    self.log(<span class="string">&#x27;train_loss&#x27;</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>如果在<code>x_step</code>函数中使用了<code>.log()</code>函数，那么这个量将会被逐步记录下来。每一个<code>log</code>出去的变量都会被记录下来，每一个<code>step</code>会集中生成一个字典dict，而每个epoch都会把这些字典收集起来，形成一个字典的list。</li></ul><p>The .log object automatically reduces the requested metrics across the full epoch. Here’s the pseudocode of what it does under the hood:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">epoch_metric = torch.mean(torch.stack([x[<span class="string">&#x27;train_loss&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> outs]))</span><br></pre></td></tr></table></figure><h4 id="train-epoch-level-operations">Train epoch-level operations</h4><p>If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span></span><br><span class="line">    x, y = batch</span><br><span class="line">    y_hat = self.model(x)</span><br><span class="line">    loss = F.cross_entropy(y_hat, y)</span><br><span class="line">    preds = ...</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;other_stuff&#x27;</span>: preds&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_epoch_end</span>(<span class="params">self, training_step_outputs</span>):</span></span><br><span class="line">   <span class="keyword">for</span> pred <span class="keyword">in</span> training_step_outputs:</span><br><span class="line">       <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><p>The matching pseudocode is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">outs = []</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = training_step(val_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply and clear grads</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">training_epoch_end(outs)</span><br></pre></td></tr></table></figure><h2 id="datamodule">DataModule</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html">主页面</a></p><h3 id="介绍">介绍</h3><ul><li><p>首先，这个<code>DataModule</code>和之前写的Dataset完全不冲突。前者是后者的一个包装，并且这个包装可以被用于多个torch Dataset 中。在我看来，其最大的作用就是把各种train/val/test划分、DataLoader初始化之类的重复代码通过包装类的方式得以被简单的复用。</p></li><li><p>具体作用项目：</p><ul><li>Download instructions：下载</li><li>Processing instructions：处理</li><li>Split instructions：分割</li><li>Train dataloader：训练集Dataloader</li><li>Val dataloader(s)：验证集Dataloader</li><li>Test dataloader(s)：测试集Dataloader</li></ul></li><li><p>其次，<code>pl.LightningDataModule</code>相当于一个功能加强版的torch Dataset，加强的功能包括：</p><ul><li><code>prepare_data(self)</code>：<ul><li>最最开始的时候，进行一些无论GPU有多少只要执行一次的操作，如写入磁盘的下载操作、分词操作(tokenize)等。</li><li>这里是一劳永逸式准备数据的函数。</li><li>由于只在单线程中调用，不要在这个函数中进行<code>self.x=y</code>似的赋值操作。</li><li>但如果是自己用而不是给大众分发的话，这个函数可能并不需要调用，因为数据提前处理好就好了。</li></ul></li><li><code>setup(self, stage=None)</code>：<ul><li>实例化数据集（Dataset），并进行相关操作，如：清点类数，划分train/val/test集合等。</li><li>参数<code>stage</code>用于指示是处于训练周期(<code>fit</code>)还是测试周期(<code>test</code>)，其中，<code>fit</code>周期需要构建train和val两者的数据集。</li><li>setup函数不需要返回值。初始化好的train/val/test set直接赋值给self即可。</li></ul></li><li><code>train_dataloader/val_dataloader/test_dataloader</code>：<ul><li>初始化<code>DataLoader</code>。</li><li>返回一个DataLoader量。</li></ul></li></ul></li></ul><h3 id="示例">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNISTDataModule</span>(<span class="params">pl.LightningDataModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_dir: <span class="built_in">str</span> = <span class="string">&#x27;./&#x27;</span>, batch_size: <span class="built_in">int</span> = <span class="number">64</span>, num_workers: <span class="built_in">int</span> = <span class="number">8</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.data_dir = data_dir</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line"></span><br><span class="line">        self.transform = transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.dims is returned when you call dm.size()</span></span><br><span class="line">        <span class="comment"># Setting default dims here because we know them.</span></span><br><span class="line">        <span class="comment"># Could optionally be assigned dynamically in dm.setup()</span></span><br><span class="line">        self.dims = (<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        self.num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># download</span></span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">        MNIST(self.data_dir, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;fit&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">            self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assign test dataset for use in dataloader(s)</span></span><br><span class="line">        <span class="keyword">if</span> stage == <span class="string">&#x27;test&#x27;</span> <span class="keyword">or</span> stage <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.mnist_test = MNIST(self.data_dir, train=<span class="literal">False</span>, transform=self.transform)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">val_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_dataloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)</span><br></pre></td></tr></table></figure><h3 id="要点-1">要点</h3><ul><li>若在DataModule中定义了一个<code>self.dims</code> 变量，后面可以调用<code>dm.size()</code>获取该变量。</li></ul><h2 id="saving-and-loading">Saving and Loading</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html">主页面</a></p><h3 id="saving">Saving</h3><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint">ModelCheckpoint</a>: 自动储存的callback module。默认情况下training过程中只会自动储存最新的模型与相关参数，而用户可以通过这个module自定义。如观测一个<code>val_loss</code>的量，并储存top 3好的模型，且同时储存最后一个epoch的模型，等等。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> ModelCheckpoint</span><br><span class="line"></span><br><span class="line"><span class="comment"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span></span><br><span class="line">checkpoint_callback = ModelCheckpoint(</span><br><span class="line">    monitor=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">    filename=<span class="string">&#x27;sample-mnist-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">    save_top_k=<span class="number">3</span>,</span><br><span class="line">    mode=<span class="string">&#x27;min&#x27;</span>,</span><br><span class="line">    save_last=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer(gpus=<span class="number">1</span>, max_epochs=<span class="number">3</span>, progress_bar_refresh_rate=<span class="number">20</span>, callbacks=[checkpoint_callback])</span><br></pre></td></tr></table></figure></li><li><p>另外，也可以手动存储checkpoint: <code>trainer.save_checkpoint("example.ckpt")</code></p></li><li><p><code>ModelCheckpoint</code> Callback中，如果<code>save_weights_only =True</code>，那么将会只储存模型的权重（相当于<code>model.save_weights(filepath)</code>），反之会储存整个模型（相当于<code>model.save(filepath)</code>）。</p></li></ul><h3 id="loading">Loading</h3><ul><li><p>load一个模型，包括它的weights、biases和超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = MyLightingModule.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line">print(model.learning_rate)</span><br><span class="line"><span class="comment"># prints the learning_rate you used in this checkpoint</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">y_hat = model(x)</span><br></pre></td></tr></table></figure></li><li><p>load模型时替换一些超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LitModel</span>(<span class="params">LightningModule</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim, out_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.l1 = nn.Linear(self.hparams.in_dim, self.hparams.out_dim)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># if you train and save the model like this it will use these values when loading</span></span><br><span class="line"><span class="comment"># the weights. But you can overwrite this</span></span><br><span class="line">LitModel(in_dim=<span class="number">32</span>, out_dim=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=32, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># uses in_dim=128, out_dim=10</span></span><br><span class="line">model = LitModel.load_from_checkpoint(PATH, in_dim=<span class="number">128</span>, out_dim=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></li><li><p>完全load训练状态：load包括模型的一切，以及和训练相关的一切参数，如<code>model, epoch, step, LR schedulers, apex</code>等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = LitModel()</span><br><span class="line">trainer = Trainer(resume_from_checkpoint=<span class="string">&#x27;some/path/to/my_checkpoint.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically restores model, epoch, step, LR schedulers, apex, etc...</span></span><br><span class="line">trainer.fit(model)</span><br></pre></td></tr></table></figure></li></ul><h2 id="callbacks">Callbacks</h2><ul><li>Callback 是一个自包含的程序，可以与训练流程交织在一起，而不会污染主要的研究逻辑。</li><li>Callback 并非只会在epoch结尾调用。pytorch-lightning 提供了数十个hook（接口，调用位置）可供选择，也可以自定义callback，实现任何想实现的模块。</li><li>推荐使用方式是，随问题和项目变化的操作，这些函数写到lightning module里面，而相对独立，相对辅助性的，需要复用的内容则可以定义单独的模块，供后续方便地插拔使用。</li></ul><h3 id="callbacks推荐">Callbacks推荐</h3><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks">内建Callbacks</a></p><ul><li><p><code>EarlyStopping(monitor='early_stop_on', min_delta=0.0, patience=3, verbose=False, mode='min', strict=True)</code>：根据某个值，在数个epoch没有提升的情况下提前停止训练。</p><blockquote><p>参数：</p><ul><li><strong>monitor</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – quantity to be monitored. Default: <code>'early_stop_on'</code>.</li><li><strong>min_delta</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><code>float</code></a>) – minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. Default: <code>0.0</code>.</li><li><strong>patience</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><code>int</code></a>) – number of validation epochs with no improvement after which training will be stopped. Default: <code>3</code>.</li><li><strong>verbose</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – verbosity mode. Default: <code>False</code>.</li><li><strong>mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><code>str</code></a>) – one of <code>'min'</code>, <code>'max'</code>. In <code>'min'</code> mode, training will stop when the quantity monitored has stopped decreasing and in <code>'max'</code> mode it will stop when the quantity monitored has stopped increasing.</li><li><strong>strict</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><code>bool</code></a>) – whether to crash the training if monitor is not found in the validation metrics. Default: <code>True</code>.</li></ul></blockquote><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> Trainer</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">trainer = Trainer(callbacks=[early_stopping])</span><br></pre></td></tr></table></figure></li><li><p><code>ModelCheckpoint</code>：见上文<strong>Saving and Loading</strong>.</p></li><li><p><code>PrintTableMetricsCallback</code>：在每个epoch结束后打印一份结果整理表格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pl_bolts.callbacks <span class="keyword">import</span> PrintTableMetricsCallback</span><br><span class="line"></span><br><span class="line">callback = PrintTableMetricsCallback()</span><br><span class="line">trainer = pl.Trainer(callbacks=[callback])</span><br><span class="line">trainer.fit(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># at the end of every epoch it will print</span></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss│train_loss│val_loss│epoch</span></span><br><span class="line"><span class="comment"># ──────────────────────────────</span></span><br><span class="line"><span class="comment"># 2.2541470527648926│2.2541470527648926│2.2158432006835938│0</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="logging">Logging</h2><ul><li><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html">Logging</a>：Logger默认是TensorBoard，但可以指定各种主流Logger<a href="https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#supported-loggers">框架</a>，如Comet.ml，MLflow，Netpune，或直接CSV文件。可以同时使用复数个logger。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> loggers <span class="keyword">as</span> pl_loggers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(</span><br><span class="line">    save_dir=os.getcwd(),</span><br><span class="line">    version=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">&#x27;lightning_logs&#x27;</span></span><br><span class="line">)</span><br><span class="line">trainer = Trainer(logger=tb_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or use the same format as others</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># One Logger</span></span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=comet_logger)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save code snapshot</span></span><br><span class="line">logger = pl_loggers.TestTubeLogger(<span class="string">&#x27;logs/&#x27;</span>, create_git_tag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Logger</span></span><br><span class="line">tb_logger = pl_loggers.TensorBoardLogger(<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">comet_logger = pl_loggers.CometLogger(save_dir=<span class="string">&#x27;logs/&#x27;</span>)</span><br><span class="line">trainer = Trainer(logger=[tb_logger, comet_logger])</span><br></pre></td></tr></table></figure><p>默认情况下，每50个batch log一次，可以通过调整参数</p></li><li><p>如果想要log输出非scalar（标量）的内容，如图片，文本，直方图等等，可以直接调用<code>self.logger.experiment.add_xxx()</code>来实现所需操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">...</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># the logger you used (in this case tensorboard)</span></span><br><span class="line">    tensorboard = self.logger.experiment</span><br><span class="line">    tensorboard.add_image()</span><br><span class="line">    tensorboard.add_histogram(...)</span><br><span class="line">    tensorboard.add_figure(...)</span><br></pre></td></tr></table></figure></li><li><p>使用log：如果是TensorBoard，那么：<code>tensorboard --logdir ./lightning_logs</code>。在Jupyter Notebook中，可以使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start tensorboard.</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir lightning_logs/</span><br></pre></td></tr></table></figure><p>在行内打开TensorBoard。</p></li><li><p>小技巧：如果在局域网内开启了TensorBoard，加上flag <code>--bind_all</code>即可使用主机名访问：</p><p><code>tensorboard --logdir lightning_logs --bind_all</code> -&gt; <code>http://SERVER-NAME:6006/</code></p></li></ul><h3 id="同时使用tensorboard和csv-logger">同时使用TensorBoard和CSV Logger</h3><p>如果同时使用两个Logger，PL会有睿智操作：如果保存根目录相同，他们会依次建立两个version文件夹，令人窒息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.loggers <span class="keyword">import</span> TensorBoardLogger, CSVLogger</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_loggers</span>():</span></span><br><span class="line">    loggers = []</span><br><span class="line">    loggers.append(TensorBoardLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tb&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CSVLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;csv&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    loggers.append(CometLogger(</span><br><span class="line">        save_dir=<span class="string">&#x27;lightning_logs&#x27;</span>, name=<span class="string">&#x27;tt&#x27;</span></span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loggers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_callbacks</span>(<span class="params">logger</span>):</span></span><br><span class="line">    callbacks = []</span><br><span class="line">    dirpath = <span class="string">f&#x27;lightning_logs/<span class="subst">&#123;logger.name&#125;</span>/version_<span class="subst">&#123;logger.version&#125;</span>/checkpoints&#x27;</span></span><br><span class="line">    callbacks.append(ModelCheckpoint(</span><br><span class="line">        dirpath=dirpath,</span><br><span class="line">        monitor=<span class="string">&#x27;loss_epoch&#x27;</span>,</span><br><span class="line">        filename=<span class="string">&#x27;&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;&#x27;</span>,</span><br><span class="line">        save_top_k=<span class="number">3</span>,</span><br><span class="line">        mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">        save_last=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">return</span> callbacks</span><br><span class="line"></span><br><span class="line">loggers = load_loggers()</span><br><span class="line">callbacks = load_callbacks(loggers[<span class="number">0</span>])</span><br><span class="line">trainer = pl.Trainer(logger=loggers, callbacks=callbacks)</span><br></pre></td></tr></table></figure><h2 id="transfer-learning">Transfer Learning</h2><p><a href="https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction_guide.html#transfer-learning">主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagenetTransferLearning</span>(<span class="params">LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a pretrained resnet</span></span><br><span class="line">        backbone = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">        num_filters = backbone.fc.in_features</span><br><span class="line">        layers = <span class="built_in">list</span>(backbone.children())[:-<span class="number">1</span>]</span><br><span class="line">        self.feature_extractor = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use the pretrained model to classify cifar-10 (10 image classes)</span></span><br><span class="line">        num_target_classes = <span class="number">10</span></span><br><span class="line">        self.classifier = nn.Linear(num_filters, num_target_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.feature_extractor.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            representations = self.feature_extractor(x).flatten(<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(representations)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h2 id="关于device操作">关于device操作</h2><p>LightningModules know what device they are on! Construct tensors on the device directly to avoid CPU-&gt;Device transfer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># good (self is LightningModule)</span></span><br><span class="line">t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br></pre></td></tr></table></figure><p>For tensors that need to be model attributes, it is best practice to register them as buffers in the modules’s <code>__init__</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">self.t = torch.rand(<span class="number">2</span>, <span class="number">2</span>, device=self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;t&quot;</span>, torch.rand(<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>前面两段是教程中的文本。然而实际上有一个暗坑：</p><p>如果你使用了一个中继的<code>pl.LightningModule</code>，而这个module里面实例化了某个普通的<code>nn.Module</code>，而这个模型中又需要内部生成一些tensor，比如图片每个通道的mean，std之类，那么如果你从<code>pl.LightningModule</code>中pass一个<code>self.device</code>，实际上在一开始这个<code>self.device</code>永远是<code>cpu</code>。所以如果你在调用的<code>nn.Module</code>的<code>__init__()</code>中初始化，使用<code>to(device)</code>或干脆什么都不用，结果就是它永远都在<code>cpu</code>上。</p><p>但是，经过实验，虽然<code>pl.LightningModule</code>在<code>__init__()</code>阶段<code>self.device</code>还是<code>cpu</code>，当进入了<code>training_step()</code>之后，就迅速变为了<code>cuda</code>。所以，对于子模块，最佳方案是，使用一个<code>forward</code>中传入的量，如<code>x</code>，作为一个reference变量，用<code>type_as</code>函数将在模型中生成的tensor都放到和这个参考变量相同的device上即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RDNFuse</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_norm_func</span>(<span class="params">self, ref</span>):</span></span><br><span class="line">        self.mean = torch.tensor(np.array(self.mean_sen), dtype=torch.float32).type_as(ref)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;mean&#x27;</span>):</span><br><span class="line">            self.init_norm_func(x)</span><br></pre></td></tr></table></figure><h2 id="关于limit_train_batches选项">关于<code>limit_train_batches</code>选项</h2><p>这里涉及到一个问题，就是每个epoch使用部分数据而非全部时，程序将会怎么工作。</p><blockquote><p>The shuffling happens when the iterator is created. In the case of the for loop, that happens just before the for loop starts. You can create the iterator manually with:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator gets created, the data has been shuffled at this point.</span></span><br><span class="line">data_iterator = <span class="built_in">iter</span>(namesTrainLoader)</span><br></pre></td></tr></table></figure><blockquote><p>By default the data loader uses <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler"><code>torch.utils.data.RandomSampler</code></a> if you set <code>shuffle=True</code> (without providing your own sampler). Its implementation is very straight forward and you can see where the data is shuffled when the iterator is created by looking at the <a href="https://github.com/pytorch/pytorch/blob/f3e620ee83f080283445aa1a7242d40e30eb6a7f/torch/utils/data/sampler.py#L103-L107"><code>RandomSampler.__iter__</code></a> method:</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">    n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">    <span class="keyword">if</span> self.replacement:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">iter</span>(torch.randperm(n).tolist())</span><br></pre></td></tr></table></figure><blockquote><p>The return statement is the important part, where the shuffling takes place. It simply creates a random permutation of the indices.</p><p>That means you will see your entire dataset every time you fully consume the iterator, just in a different order every time. Therefore there is no data lost (not including cases with <code>drop_last=True</code>) and your model will see all data at every epoch.</p></blockquote><p>总结下来，如果使用了<code>shuffle=True</code>选项，那么即使每次都不跑完整个epoch，你还是有机会见到所有的数据的。数据集的shuffle发生在<code>iter</code>被创建的时候，在我们一般的代码中，也就是内层for循环开始时。但如果你没有选择<code>shuffle=True</code>，那你将永远只能看到你设定的前面N个数据。</p><h2 id="points">Points</h2><ul><li><p><code>pl.seed_everything(1234)</code>：对所有相关的随机量固定种子。</p></li><li><p>使用LR Scheduler时候，不用自己<code>.step()</code>。它也被Trainer自动处理了。<a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html?highlight=scheduler#">Optimization 主页面</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single optimizer</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">        loss = model.training_step(batch, batch_idx, ...)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Multiple optimizers</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> data:</span><br><span class="line">     <span class="keyword">for</span> opt <span class="keyword">in</span> optimizers:</span><br><span class="line">        disable_grads_for_other_optimizers()</span><br><span class="line">        train_step(opt)</span><br><span class="line">        opt.step()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> scheduler <span class="keyword">in</span> schedulers:</span><br><span class="line">     scheduler.step()</span><br></pre></td></tr></table></figure></li><li><p>关于划分train和val集合的方法。与PL无关，但很常用，两个例子：</p><ol type="1"><li><code>random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))</code></li><li>如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">mnist_full = MNIST(self.data_dir, train=<span class="literal">True</span>, transform=self.transform)</span><br><span class="line">self.mnist_train, self.mnist_val = random_split(mnist_full, [<span class="number">55000</span>, <span class="number">5000</span>])</span><br></pre></td></tr></table></figure><p>Parameters：</p><ul><li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</li><li><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</li><li><strong>generator</strong> (<a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>Generator</em></a>) – Generator used for the random permutation.</li></ul></li><li><p>如果使用了<code>PrintTableMetricsCallback</code>，那么<code>validation_step</code>不要return内容，否则会炸。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;Pytorch-Lightning这个库我“发现”过两次。第一次发现时，感觉它很重很难学，而且似乎自己也用不上。但是后面随着做的项目开始出现了一些稍微高阶的要求，我发现我总是不断地在相似工程代码上花费大量时间，Debug也是这些代码花的时间最多，而且渐渐产生了一个矛盾之处：如果想要更多更好的功能，如TensorBoard支持，Early Stop，LR Scheduler，分布式训练，快速测试等，代码就无可避免地变得越来越长，看起来也越来越乱，同时核心的训练逻辑也渐渐被这些工程代码盖过。那么有没有更好的解决方案，甚至能一键解决所有这些问题呢？&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
    <category term="pytorch-lightning" scheme="https://www.miracleyoo.com/tags/pytorch-lightning/"/>
    
  </entry>
  
  <entry>
    <title>Analyzing Geography Data (Beginners&#39; Tutorial)</title>
    <link href="https://www.miracleyoo.com/2021/02/03/satellite-basic/"/>
    <id>https://www.miracleyoo.com/2021/02/03/satellite-basic/</id>
    <published>2021-02-04T02:18:19.000Z</published>
    <updated>2021-03-12T23:19:59.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据来源">数据来源</h2><h3 id="卫星类型">卫星类型</h3><ol type="1"><li><p>Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有<span class="math inline">\(10m \times 10m\)</span>，<span class="math inline">\(20m \times 20m\)</span>，<span class="math inline">\(60m \times 60m\)</span>，bands中心波长从442.3nm到2185.7nm。</p><p><img src="/2021/02/03/satellite-basic/image-20201213154533413.png" alt="image-20201213154533413" style="zoom:33%;"></p><span id="more"></span></li><li><p>Planet：这个数据源提供<span class="math inline">\(3m \times 3m\)</span>的4 bands MSI image。分别是R、G、B、NIR。</p><ul><li>这个数据源似乎只对美国国内的院校机构开放，不过不是很确定，需要的可以试试。</li></ul><figure><img src="/2021/02/03/satellite-basic/image-20201213150111581.png" alt="Planet Specification"><figcaption aria-hidden="true">Planet Specification</figcaption></figure></li><li><p>RapidEyes</p></li></ol><h3 id="sentinel-2-data-source">Sentinel-2 Data Source</h3><ol type="1"><li><p>USGS Earth Explorer: <a href="https://earthexplorer.usgs.gov/">Link</a>. It support downloading data within five years. And it not only support sentinel-2 data, but also contains many other satellite datasource.</p><figure><img src="/2021/02/03/satellite-basic/image-20201227143234319.png" alt="image-20201227143234319"><figcaption aria-hidden="true">image-20201227143234319</figcaption></figure></li><li><p>Copernicus Open Access Hub: <a href="https://scihub.copernicus.eu/dhus/#/home">Link</a>. It supports only one year's old data. You can request the older data, but not guarante the fetch time.</p><figure><img src="/2021/02/03/satellite-basic/image-20201227145324164.png" alt="image-20201227145324164"><figcaption aria-hidden="true">image-20201227145324164</figcaption></figure></li><li><p>Amazon AWS Sentinel-2 Service: <a href="https://registry.opendata.aws/sentinel-2/">Link</a></p></li></ol><h3 id="planet-data-source">Planet Data Source</h3><ul><li>官网：<a href="https://www.planet.com/">Link</a></li></ul><h3 id="specification">Specification</h3><ol type="1"><li><a href="https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/msi-instrument">Sentinel-2 MultiSpectral Instrument (MSI) Overview</a></li><li><a href="https://www.planet.com/products/satellite-imagery/files/Planet_Combined_Imagery_Product_Specs_December2017.pdf">Sentinel-2 Specification Doc</a></li><li><a href="https://www.planet.com/products/planet-imagery/">Planet Specification</a></li></ol><h2 id="软件与python包简介">软件与Python包简介</h2><ul><li><a href="http://www.gdal.org/">GDAL</a> –&gt; Fundamental package for processing vector and raster data formats (many modules below depend on this). Used for raster processing.</li><li><a href="https://github.com/mapbox/rasterio">Rasterio</a> –&gt; Clean and fast and geospatial raster I/O for Python. <a href="https://rasterio.readthedocs.io/en/latest/quickstart.html">Guidebook</a>.</li><li><a href="http://geopandas.org/#description">Geopandas</a> –&gt; Working with geospatial data in Python made easier, combines the capabilities of pandas and shapely.</li><li><a href="http://toblerity.org/shapely/manual.html">Shapely</a> –&gt; Python package for manipulation and analysis of planar geometric objects (based on widely deployed <a href="https://trac.osgeo.org/geos/">GEOS</a>).</li><li><a href="https://pypi.python.org/pypi/Fiona">Fiona</a> –&gt; Reading and writing spatial data (alternative for geopandas).</li><li><a href="https://pypi.python.org/pypi/pyproj?">Pyproj</a> –&gt; Performs cartographic transformations and geodetic computations (based on <a href="http://trac.osgeo.org/proj">PROJ.4</a>).</li><li><a href="https://pysal.readthedocs.org/en/latest/">Pysal</a> –&gt; Library of spatial analysis functions written in Python.</li><li><a href="http://geopy.readthedocs.io/en/latest/">Geopy</a> –&gt; Geocoding library: coordinates to address &lt;-&gt; address to coordinates.</li><li><a href="http://geo.holoviews.org/index.html">GeoViews</a> –&gt; Interactive Maps for the web.</li><li><a href="https://networkx.github.io/documentation/networkx-1.10/overview.html">Networkx</a> –&gt; Network analysis and routing in Python (e.g. Dijkstra and A* -algorithms), see <a href="http://gis.stackexchange.com/questions/65056/is-it-possible-to-route-shapefiles-using-python-and-without-arcgis-qgis-or-pgr">this post</a>.</li><li><a href="http://scitools.org.uk/cartopy/docs/latest/index.html">Cartopy</a> –&gt; Make drawing maps for data analysis and visualisation as easy as possible.</li><li><a href="http://docs.scipy.org/doc/scipy/reference/spatial.html">Scipy.spatial</a> –&gt; Spatial algorithms and data structures.</li><li><a href="http://toblerity.org/rtree/">Rtree</a> –&gt; Spatial indexing for Python for quick spatial lookups.</li><li><a href="http://www.rsgislib.org/index.html#python-documentation">RSGISLib</a> –&gt; Remote Sensing and GIS Software Library for Python.</li><li><a href="https://python-geojson.readthedocs.io/en/latest/">python-geojson</a>-&gt; Deal with geojson format files.</li></ul><h2 id="sentinel-2-大气校正">Sentinel-2 大气校正</h2><p>Sentinel-2一般有两种standard，一个是A级别，一个是C级别。C级别的数据是你可以从任意网站上下载到的数据，它没有经过大气校正，是粗数据，每个区块的反射率可能有不同，不适合直接用作深度学习数据。</p><p>经过Sen2Cor软件校正后可以得到A级别的数据。</p><p>Sen2Cor是欧空局发布的一个软件，它即可以作为SNAP的插件安装，也可以作为独立命令行软件使用。推荐后者，更为快速、稳定，尤其适用于大量数据时，可以用脚本批处理。<a href="http://step.esa.int/main/snap-supported-plugins/sen2cor/sen2cor_v2-8/">官网链接</a></p><h3 id="查询坐标映射">查询坐标映射</h3><ul><li>地球是圆的。</li><li>卫星上拍的一张矩形照片所对应的区域并非是矩形的。</li><li>使用卫星图片时要先将其映射到二维展开的坐标系中。</li><li>整个地球被分为了许多预先订好的区域。</li><li>每个区域有一个编号。</li><li>编号可以在<a href="http://epsg.io/">EPSG</a>网站查询。</li></ul><h2 id="snap">SNAP</h2><ul><li><p>欧空局自己用作处理Sentinel-2的软件。<a href="https://step.esa.int/main/download/snap-download/">Link</a></p></li><li><p>只用来处理Sentinel-2，尤其是预处理，包括校正，reprojection，粗crop，统一各个bands分辨率等等，非常好用。因为是亲儿子，所以甚至可以直接读取Sentinel-2每个文件的压缩包，总之十分便利。</p></li><li><p>比较古老，interface有年代感，功能也相较于其他软件比较局限。</p></li><li><p>推荐用作第一步预处理。</p></li><li><p>使用流程：<code>读取-&gt;剪裁-&gt;correction-&gt;resize-&gt;reprojection-&gt;导出</code>。</p></li><li><p>大部分需要用到的功能都在<code>Raster-&gt;Geometric</code>中</p></li><li><p>经过尝试、搜索、确认，SNAP并不提供便利的选定区域截图，或是依据shapefile剪裁，所以目前最佳的方法是，先通过zoom地图和改变窗口大小确保需要的部分大致在view的可视范围内，然后右键，选择<code>Spatial Subset from View</code>，然后可视区域即可被剪裁。注意，这只是粗剪裁，所以尽量多包括一点，也不要少任何一部分。剪裁后的内容可以后续在ArcGIS Pro中进一步处理。</p><p><img src="/2021/02/03/satellite-basic/image-20201227155911702.png" alt="image-20201227155911702" style="zoom:40%;"></p></li><li><p>展示图：</p><p><img src="/2021/02/03/satellite-basic/image-20201227155421761.png" alt="image-20201227155421761" style="zoom:33%;"></p><figure><img src="/2021/02/03/satellite-basic/image-20201227155558109.png" alt="image-20201227155558109"><figcaption aria-hidden="true">image-20201227155558109</figcaption></figure></li></ul><h2 id="arcgis-pro">ArcGIS Pro</h2><ul><li><p>我能找到的最强大的可用于卫星图像处理的软件。</p></li><li><p>专业，美观，支持format多，有着强大的raster functions。</p></li><li><p>版权软件，下载之前先确认自己学校或公司是否提供License。</p></li><li><p>有时导出raster会出现随机bug，导致：导出可能是纯黑的图片，导出部分没有按照期望剪裁等。很恶心，但似乎也没有更好的选择。</p></li><li><p>解决导出问题：</p><ol type="1"><li>关闭导出窗口，重新操作，多试几次。可以解决绝大部分问题。</li><li>和剪裁、mask等有关的问题可以先使用raster function进行这些操作，再直接导出前面操作的结果layer。</li></ol></li><li><p>关于剪裁后的卫星图片和原始图片有着明显亮度对比度区别的问题：</p><ol type="1"><li><p>首先，这不是一个bug，而是一个feature。。。实际上的卫星图都很暗沉的，所以软件原生提供一个“显示方法”的函数，调整图片曲线，使得显示的图片比较亮，容易看清楚细节。然而，剪裁后的图片有着不一样的统计值，所以在有些显示函数下，显示的结果和剪裁前结果不同。</p><p><img src="/2021/02/03/satellite-basic/image-20201227155342029.png" alt="image-20201227155342029" style="zoom:33%;"></p></li><li><p>但是不用担心，因为导出时候并不会考虑这个显示函数。即：剪裁前后的导出图像数值是相同的。</p></li></ol></li><li><p>下面是两张展示图：</p></li></ul><figure><img src="/2021/02/03/satellite-basic/image-20201227155051608.png" alt="image-20201227155051608"><figcaption aria-hidden="true">image-20201227155051608</figcaption></figure><figure><img src="/2021/02/03/satellite-basic/image-20201227155259214.png" alt="image-20201227155259214"><figcaption aria-hidden="true">image-20201227155259214</figcaption></figure><h2 id="qgis">QGIS</h2><ul><li><p>开源软件，支持很多插件，比如直接下载Sentinel-2数据，Sen2Cor等。<a href="https://qgis.org/en/site/">Link</a></p></li><li><p>支持很多format的数据。</p></li><li><p>问题是，不稳定，效率低，容易崩溃（软件&amp;心态），甚至左侧Explorer遇到大文件夹都要经常转很久才能进去，或是干脆就直接转崩了。我怀疑他们要分析每个文件夹所有文件之后再显示列表。。。总之，慎重。</p><figure><img src="/2021/02/03/satellite-basic/image-20201227160344273.png" alt="image-20201227160344273"><figcaption aria-hidden="true">image-20201227160344273</figcaption></figure></li></ul><h2 id="pythonic-method">Pythonic Method</h2><p>虽然前面介绍了几个软件，但是说实话，处理一两个可以，批量处理几十几百甚至几十万就有点力不从心了。所以最后还是狠下心研究了一遍Python处理这些数据的方法，写出了一批适合我项目用的函数。不一定适合所有人，但可以作为参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd</span><br><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Point, LineString, Polygon</span><br><span class="line"><span class="keyword">import</span> rasterio <span class="keyword">as</span> rio</span><br><span class="line"><span class="keyword">from</span> rasterio.mask <span class="keyword">import</span> mask</span><br><span class="line"><span class="keyword">from</span> rasterio.warp <span class="keyword">import</span> calculate_default_transform, reproject, Resampling</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    piece = np.array([i.bounds <span class="keyword">for</span> i <span class="keyword">in</span> shp[<span class="string">&#x27;geometry&#x27;</span>]])</span><br><span class="line">    minx = piece[:,<span class="number">0</span>].<span class="built_in">min</span>()</span><br><span class="line">    miny = piece[:,<span class="number">1</span>].<span class="built_in">min</span>()</span><br><span class="line">    maxx = piece[:,<span class="number">2</span>].<span class="built_in">max</span>()</span><br><span class="line">    maxy = piece[:,<span class="number">3</span>].<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">return</span> minx, miny, maxx, maxy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edge_length</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the x and y edge length for a ceratin shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(maxx-minx,<span class="number">3</span>), <span class="built_in">round</span>(maxy-miny,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shape2latlong</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the shapefile unit from meters/other units to lat/long.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.to_crs(epsg=<span class="number">4326</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_latlong</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the latitude-longitude bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shp = shape2latlong(shp)</span><br><span class="line">    <span class="keyword">return</span> bbox(shp)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_polygon</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Return the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    minx, miny, maxx, maxy = bbox(shp)</span><br><span class="line">    <span class="keyword">return</span> Polygon([(minx, miny), (minx, maxy), (maxx,maxy), (maxx, miny)])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_polygon</span>(<span class="params">shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Merge a shapefile to one single polygon.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> shp.dissolve(by=<span class="string">&#x27;Id&#x27;</span>).iloc[<span class="number">0</span>].geometry</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polygon2geojson</span>(<span class="params">polygon</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn a polygon to a geojson format string.</span></span><br><span class="line"><span class="string">        This is used for rasterio mask operation.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(polygon) == Polygon:</span><br><span class="line">        polygon = gpd.GeoSeries(polygon)</span><br><span class="line">    <span class="keyword">return</span> [json.loads(polygon.to_json())[<span class="string">&#x27;features&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;geometry&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sen2rgb</span>(<span class="params">img, scale=<span class="number">30</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Turn the 12 channel float32 format sentinel-2 images to a RGB uint8 image. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (img[(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>),]/<span class="number">256</span>*scale).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cropbyshp</span>(<span class="params">raster, shp</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Crop a raster using a shapefile.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Reproject the shapefile to the same crs of raster.</span></span><br><span class="line">    shp = shp.to_crs(&#123;<span class="string">&quot;init&quot;</span>: <span class="built_in">str</span>(raster.crs)&#125;)</span><br><span class="line">    <span class="comment"># Compute the rectangular Polygon bounding box of a certain shapefile.</span></span><br><span class="line">    bbpoly = bbox_polygon(shp)</span><br><span class="line">    <span class="comment"># Execute the mask operation.</span></span><br><span class="line">    out_img, out_transform = mask(dataset=raster, shapes=polygon2geojson(bbpoly), crop=<span class="literal">True</span>, all_touched=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> out_img</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_raster</span>(<span class="params">raster, path</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Write a created raster object to file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(</span><br><span class="line">        path,</span><br><span class="line">        <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        **raster.meta</span><br><span class="line">    ) <span class="keyword">as</span> dst:</span><br><span class="line">        dst.write(raster.read())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sen_reproject</span>(<span class="params">src, dst_crs, out_path</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Reproject a raster to a new CRS coordinate, and save it in out_path.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        src: Input raster.</span></span><br><span class="line"><span class="string">        dst_crs: Target CRS. String.</span></span><br><span class="line"><span class="string">        out_path: The path of the output file.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    transform, width, height = calculate_default_transform(</span><br><span class="line">        src.crs, dst_crs, src.width, src.height, *src.bounds)</span><br><span class="line">    kwargs = src.meta.copy()</span><br><span class="line">    kwargs.update(&#123;</span><br><span class="line">        <span class="string">&#x27;crs&#x27;</span>: dst_crs,</span><br><span class="line">        <span class="string">&#x27;transform&#x27;</span>: transform,</span><br><span class="line">        <span class="string">&#x27;width&#x27;</span>: width,</span><br><span class="line">        <span class="string">&#x27;height&#x27;</span>: height</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> rio.<span class="built_in">open</span>(out_path, <span class="string">&#x27;w&#x27;</span>, **kwargs) <span class="keyword">as</span> dst:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, src.count + <span class="number">1</span>):</span><br><span class="line">            reproject(</span><br><span class="line">                source=rio.band(src, i),</span><br><span class="line">                destination=rio.band(dst, i),</span><br><span class="line">                src_transform=src.transform,</span><br><span class="line">                src_crs=src.crs,</span><br><span class="line">                dst_transform=transform,</span><br><span class="line">                dst_crs=dst_crs,</span><br><span class="line">                resampling=Resampling.cubic)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_A_by_B</span>(<span class="params">A, B</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Generate a mask from B, and applied it to A.</span></span><br><span class="line"><span class="string">        All 0 values are excluded.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mask = B.<span class="built_in">sum</span>(axis=<span class="number">0</span>)&gt;<span class="number">1e-3</span></span><br><span class="line">    masked_A = mask*A</span><br><span class="line">    <span class="keyword">return</span> masked_A</span><br></pre></td></tr></table></figure><h2 id="reference">Reference</h2><ol type="1"><li><a href="https://automating-gis-processes.github.io/2016/course-info.html">Python GIS 超完整教程</a></li><li><a href="https://zia207.github.io/geospatial-python.io/">Professor Zia's Personal Website</a></li><li><a href="https://blog.csdn.net/sinat_28853941/article/details/78511167">sentinel-2数据下载 大气校正 转ENVI格式</a></li><li><a href="https://blog.csdn.net/lidahuilidahui/article/details/102765420">03-SNAP处理Sentinel-2 L2A级数据（一）</a></li><li><a href="https://clouds.eos.ubc.ca/~phil/courses/atsc301/html/rasterio_demo.html">UBC Course Notebook</a></li><li><a href="https://zhuanlan.zhihu.com/p/31010043">利用Sen2cor对哨兵2号（Sentinel-2）L1C多光谱数据进行辐射定标和大气校正</a></li><li><a href="https://sentinelhub-py.readthedocs.io/en/latest/">Documentation of Sentinel Hub Python package</a></li><li><a href="https://github.com/sentinel-hub/sentinelhub-py">sentinelhub-py</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;数据来源&quot;&gt;数据来源&lt;/h2&gt;
&lt;h3 id=&quot;卫星类型&quot;&gt;卫星类型&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Sentinel-2：提供混合分辨率的13 Bands MSI。分辨率有&lt;span class=&quot;math inline&quot;&gt;\(10m \times 10m\)&lt;/span&gt;，&lt;span class=&quot;math inline&quot;&gt;\(20m \times 20m\)&lt;/span&gt;，&lt;span class=&quot;math inline&quot;&gt;\(60m \times 60m\)&lt;/span&gt;，bands中心波长从442.3nm到2185.7nm。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/02/03/satellite-basic/image-20201213154533413.png&quot; alt=&quot;image-20201213154533413&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="satellite" scheme="https://www.miracleyoo.com/tags/satellite/"/>
    
  </entry>
  
  <entry>
    <title>全自动种子追番系统</title>
    <link href="https://www.miracleyoo.com/2021/02/01/seed-anime-system/"/>
    <id>https://www.miracleyoo.com/2021/02/01/seed-anime-system/</id>
    <published>2021-02-02T02:53:44.000Z</published>
    <updated>2021-03-12T23:53:56.641Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言</strong>：你喜欢的番剧更新了。是你喜欢的字幕组的高清资源。里面没有放肆的圣光和暗牧。尽管它也许没在国内放送。你可以在任何设备上观看它，并且可以无缝切换。电视，手机，iPad，电脑都没有问题。它很快。</p><p>这将是本篇介绍的全自动全平台订阅追番系统。</p><p>出于各种原因，有许多番剧在B站并找不到。即使开了大会员，从国外驾梯回国，还是无法看到一些喜欢的内容。但同时，各大动漫种子站却提供了几乎无所不包的资源，其中包括新番。但依靠种子追番最常见的问题就是难以track，经常会忘记更新，而且每次都需要经过一个<code>搜索-下载种子-下载-整理-观看</code>的过程，确实还是很劝退的。如何搭建一个易于配置、足够简单、全自动化抓取、下载、串流的追番系统，便成为了一个追番人的核心需求。</p><p>下面本文将会带领你认识整套流程，如果你足够经常折腾，大致一个小时之内就可以搭建完成。祝你好运:)</p><p>PS: 本文涉及的所有脚本都可以在该<a href="https://github.com/miracleyoo/anime_renamer">GitHub</a>库上找到。</p><span id="more"></span><h2 id="软件与配置">软件与配置</h2><ul><li>源：动漫种子站。如Nyaa、ACGRIP、动漫花园、萌番组。为了避免河蟹这里就不放链接了，请自行搜索。</li><li>订阅与自动下载种子：RSS。</li><li>下载器：utorrent。</li><li>正则匹配与自动重命名：Python。</li><li>串流与自动整理：Plex。</li></ul><p>由于上面提到的软件都支持Windows和Mac，所以认为该系统适用于两者。</p><h2 id="流程">流程</h2><p>首先放一张整理出来的流程图：</p><figure><img src="/2021/02/01/seed-anime-system/image-20210129014429864.png" alt="image-20210129014429864"><figcaption aria-hidden="true">image-20210129014429864</figcaption></figure><p>该系统的整体思路是，使用RSS自动嗅探订阅的内容，用utorrent自动下载到番剧指定的目录，使用Python重命名成Plex可以支持的剧集命名格式后，使用Plex整理、匹配元数据，并提供串流服务，让你的电脑本身成为一个视频内容服务器，从而使你可以在家中任何设备上迅速、便捷地观看下载整理好的视频。这对于习惯在电视大屏幕、投影屏或iPad上观看视频的用户而言是十分友好的。</p><p>最终达到的效果是：一旦种子源网站上更新了一个你订阅的番剧的种子，你就可以第一时间打开电视、掏出iPad直接观看内容。</p><p>另外，由于Plex可以记住你所有视频的观看进度，所以你完全可以在手机上看了前半部分，然后坐到电视机前，从容地无缝继续观看。</p><p>效果如下：</p><figure><img src="/2021/02/01/seed-anime-system/image-20210129015121641.png" alt="image-20210129015121641"><figcaption aria-hidden="true">image-20210129015121641</figcaption></figure><figure><img src="/2021/02/01/seed-anime-system/image-20210129015202322.png" alt="image-20210129015202322"><figcaption aria-hidden="true">image-20210129015202322</figcaption></figure><figure><img src="/2021/02/01/seed-anime-system/image-20210129015424295.png" alt="image-20210129015424295"><figcaption aria-hidden="true">image-20210129015424295</figcaption></figure><h2 id="源站">源站</h2><p>各大动漫压制组、汉化组和搬运组（统称发布组）都会将他们的资源发布到这些源站上。有的源站互为镜像，有的站是爬取的其他站的信息。它们在追新番这项任务上的表现其实都还可以，选一个你自己喜欢的站点即可。</p><p>每个发布组都有自己的偏好的番剧，也有他们独特的规矩和讲究。如VCB-Studio就偏好做高清的压制资源，但经常不附带字幕；LIlith由于主要是搬运，所以发布速度快；喵萌开的番比较多，视频质量较高，但并不是所有番剧都是及时发布的。热门番剧往往会在第一时间就有发布组发布，而一些冷门内容的更新速度就无法保证了。另外，A发布组发布X番剧可能是及时的，Y番剧要隔几天；可能B发布组则相反。最简单的方法就是检查上一集最先是由哪个组发布的，哪个组放出的资源又有最多人下载，然后进行综合判断。</p><h2 id="rss">RSS</h2><p>引用一段Wikipedia的定义：</p><blockquote><p><strong>RSS</strong>（全称：<a href="https://zh.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> Site Summary；Really Simple Syndication[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-powers-2003-1-2">2]</a>），中文译作<strong>简易信息聚合</strong>[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-3">3]</a>，也称<strong>聚合内容</strong>[<a href="https://zh.wikipedia.org/wiki/RSS#cite_note-张锐2015-4">4]</a>，是一种<a href="https://zh.wikipedia.org/wiki/消息來源">消息来源</a>格式规范，用以<strong>聚合经常发布更新资料的网站</strong>，例如<a href="https://zh.wikipedia.org/wiki/部落格">博客</a>文章、新闻、<a href="https://zh.wikipedia.org/wiki/音訊">音频</a>或<a href="https://zh.wikipedia.org/wiki/視訊">视频</a>的网摘。RSS文件（或称做摘要、网络摘要、或频更新，提供到频道）包含全文或是节录的文字，再加上发布者所订阅之网摘资料和授权的元数据。简单来说 RSS 能够让用户订阅个人网站个人博客，当订阅的网站有新文章是能够获得通知。</p></blockquote><p>RSS的本质是订阅某个信源。信源可以不断更新和推送信息，它往往表现为一个链接。让我们举一个例子。</p><p><img src="/2021/02/01/seed-anime-system/image-20210129020139987.png" alt="image-20210129020139987" style="zoom:50%;"></p><p>假设我们想要追一个由喵萌奶茶屋发布的新番<code>IDOLY PRIDE</code>，并且只看简体的1080p版本，那么我们就可以根据其命名风格来生成对应的搜索关键词：<code>【喵萌Production】★01月新番★[偶像荣耀/IDOLY PRIDE][03][1080p][简日双语][招募翻译]</code>是其中一集的命名。那么我们只需要在搜索框中搜索：<code>喵萌Production 偶像荣耀 1080p 简日</code>，即可定位我们需要追的番的特定版本的所有后续资源。</p><figure><img src="/2021/02/01/seed-anime-system/image-20210129020710605.png" alt="image-20210129020710605"><figcaption aria-hidden="true">image-20210129020710605</figcaption></figure><p>然后我们即可把这个搜索结果的网页当做一个信源。具体获取RSS订阅链接的方法每个网站都各不相同，但大致都会有一个类似的按钮出现。点击这个按钮，在弹出的窗口的地址栏即可找到我们需要的RSS订阅链接。</p><figure><img src="/2021/02/01/seed-anime-system/image-20210129021051012.png" alt="image-20210129021051012"><figcaption aria-hidden="true">image-20210129021051012</figcaption></figure><p>复制这个链接，这一步就完成了。你成功地得到了你所需要内容的订阅链接。</p><h2 id="utorrent">utorrent</h2><ul><li><p>utorrent是一个可以下载种子文件和磁力链接的软件，支持Windows、Mac和Linux，也有网页版。</p></li><li><p>utorrent支持RSS订阅，相当于每当RSS订阅的信源有了新的内容，utorrent就会自动把它列出来。你也可以设置每当有新内容就自动下载，不过这要视订阅内容而定。每个RSS的自动下载位置可以是不同的文件夹。效果如下：</p><p><img src="/2021/02/01/seed-anime-system/image-20210130113430927.png" alt="image-20210130113430927" style="zoom:50%;"></p></li><li><p>utorrent支持文件下载完成或状态改变后执行脚本。这个特点非常重要，下一步需要用。</p></li></ul><h3 id="rss配置">RSS配置</h3><ol type="1"><li>首先，将上面提到的RSS链接添加到utorrent的订阅中：点击<code>订阅</code>后页面中的小Wi-Fi图标即可添加，也可以右键<code>添加RSS订阅</code>。这时候先不用设置什么，把URL粘贴到第一项，然后定义一个方便识别的别名即可，订阅栏先选择<code>不要自动下载所有项目</code>。待会儿会在下载器设定中详细设置。</li></ol><p><img src="/2021/02/01/seed-anime-system/image-20210130113824679.png" alt="image-20210130113824679" style="zoom:50%;"></p><p><img src="/2021/02/01/seed-anime-system/image-20210130114135639.png" alt="image-20210130114135639" style="zoom:50%;"></p><ol start="2" type="1"><li><p>右键左栏<code>订阅</code>中出现的新项，选择<code>RSS下载器</code>，进入下载器详细配置：</p><p><img src="/2021/02/01/seed-anime-system/image-20210130114636832.png" alt="image-20210130114636832" style="zoom:50%;"></p><p>这里我们需要修改的只有<code>保存在</code>和<code>订阅</code>两项，前者是这个订阅中所有文件的保存位置，订阅是这套设置将应用于哪个订阅。需要注意的是，文件夹最好为这个番剧的名字，且命名最好遵守以下规则，方便后续匹配：</p><ul><li>只有一个主名字。如果一个番剧既有中文名，又有日文名，还有罗马字或英语名，请只使用一个名字。使用哪个看你自己，最好是广泛接受的名字。其他语言的名字，其他信息，如字幕组，简体繁体，清晰度等，请分别使用一个中括号括住。整理串流软件由于主要遵循西方影视的命名规则，会直接忽略方括号内的所有内容，所以在方括号里面你可以放入任何你想备注的信息。</li><li>一套规则对应一个订阅。整个链条的对应关系是：<code>一个番剧名字-&gt;一条RSS链接-&gt;utorrent中一项RSS订阅-&gt;RSS下载器中一套规则-&gt;一个独立文件夹</code></li><li>过滤器如果没有需求，可以留空或<code>*</code>，表示匹配所有。若是在这个订阅链接中还没有完全筛选，那你可以着这里进行另外一波过滤。</li></ul></li></ol><h2 id="重命名">重命名</h2><p>首先再解释一下为何需要重命名。正如前文所说，Plex等媒体管理软件主要还是针对欧美剧集的命名方法设计的，且会忽略方括号中所有内容。但国内，至少是番剧的命名，对于集数的命名，恰恰大多会放到方括号中，如<code>[c.c動漫][1月新番][工作細胞 第二季][04][BIG5][1080P][MP4]</code>里面的这个<code>[04]</code>就是集数。另外还有的命名是<code>第04话</code>，<code>[第04话]</code>，<code>- 04 -</code>等。另外，由于某些技术性错误，有些发布组会在某集发布后重新发布修复版本，此时的命名为<code>04v2</code>等。还有很多番剧的最后一集会被添加上END字样表示完结。而这些内容都会对后续的解析造成困难，最好可以将集数这个变量转为某种固定格式。</p><p>Python可以很好的完成这项任务。通过正则匹配上述内容，并重新组合，可以轻易的得到想要的结果。写好的脚本如下：</p><h3 id="reg_match.py"><code>reg_match.py</code>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> pathlib2 <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">log_name=op.join(op.dirname(op.realpath(__file__)), <span class="string">&#x27;log.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Episode Regular Expression Matching Rules</span></span><br><span class="line">episode_rules = [<span class="string">r&#x27;(.*)\[(\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[E(\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[第(\d*\.*\d*)话(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)\[第(\d*\.*\d*)話(?:END)?\](.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)第(\d*\.*\d*)话(?:END)?(.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)第(\d*\.*\d*)話(?:END)?(.*)&#x27;</span>,</span><br><span class="line">                 <span class="string">r&#x27;(.*)- (\d&#123;1,3&#125;|\d&#123;1,3&#125;\.\d&#123;1,2&#125;)(?:v\d&#123;1,2&#125;)?(?:END)? (.*)&#x27;</span>]</span><br><span class="line"><span class="comment"># Suffixs of files we are going to rename</span></span><br><span class="line">suffixs = [<span class="string">&#x27;mp4&#x27;</span>, <span class="string">&#x27;mkv&#x27;</span>, <span class="string">&#x27;avi&#x27;</span>, <span class="string">&#x27;mov&#x27;</span>, <span class="string">&#x27;flv&#x27;</span>, <span class="string">&#x27;rmvb&#x27;</span>, <span class="string">&#x27;ass&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>]</span><br><span class="line">sys.stdout = io.TextIOWrapper(buffer=sys.stdout.buffer,encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parse the input arguments. You can whether input only root, or only path, or both root and name.</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Regular Expression Match&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--root&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The root directory of the input file.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The file name of the input file.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--path&#x27;</span>, default=<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;The file full path of the input file.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rename</span>(<span class="params">root, name</span>):</span></span><br><span class="line">    root = Path(root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> episode_rules:</span><br><span class="line">        matchObj = re.match(rule, name, re.I)</span><br><span class="line">        <span class="keyword">if</span> matchObj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            new_name = <span class="string">f&#x27;<span class="subst">&#123;matchObj.group(<span class="number">1</span>)&#125;</span> E<span class="subst">&#123;matchObj.group(<span class="number">2</span>)&#125;</span> <span class="subst">&#123;matchObj.group(<span class="number">3</span>)&#125;</span>&#x27;</span></span><br><span class="line">            <span class="comment"># print(matchObj.group())</span></span><br><span class="line">            <span class="comment"># print(new_name)</span></span><br><span class="line">            print(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="keyword">with</span> codecs.<span class="built_in">open</span>(log_name, <span class="string">&#x27;a+&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="comment"># f.writelines(f&#x27;&#123;name&#125; -&gt; &#123;new_name&#125;&#x27;)</span></span><br><span class="line">                print(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>, file=f)</span><br><span class="line"></span><br><span class="line">            os.rename(<span class="built_in">str</span>(root/name), <span class="built_in">str</span>(root/new_name))</span><br><span class="line">            general_check(root, new_name)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    general_check(root, name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">general_check</span>(<span class="params">root, name</span>):</span></span><br><span class="line">    new_name = <span class="string">&#x27; &#x27;</span>.join(name.split())</span><br><span class="line">    <span class="keyword">if</span> new_name != name:</span><br><span class="line">        print(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">with</span> codecs.<span class="built_in">open</span>(log_name, <span class="string">&#x27;a+&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            print(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> -&gt; <span class="subst">&#123;new_name&#125;</span>&#x27;</span>, file=f)</span><br><span class="line">        os.rename(<span class="built_in">str</span>(root/name), <span class="built_in">str</span>(root/new_name))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">if</span> op.isdir(args.path):</span><br><span class="line">        args.root = args.path</span><br><span class="line">        args.path = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name != <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        temp = <span class="built_in">str</span>(args.root/args.name)</span><br><span class="line">        <span class="keyword">if</span> op.isdir(temp):</span><br><span class="line">            args.root = temp</span><br><span class="line">            args.name = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.path != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        root, name = op.split(args.path)</span><br><span class="line">        rename(root, name)</span><br><span class="line">    <span class="keyword">elif</span> args.name != <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        rename(args.root, args.name)</span><br><span class="line">    <span class="keyword">elif</span> args.root != <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">        files = []</span><br><span class="line">        <span class="keyword">for</span> suffix <span class="keyword">in</span> suffixs:</span><br><span class="line">            files.extend(Path(args.root).rglob(<span class="string">&#x27;*.&#x27;</span>+suffix))</span><br><span class="line">            files.extend(Path(args.root).rglob(<span class="string">&#x27;*.&#x27;</span>+suffix.upper()))</span><br><span class="line">        print(<span class="string">f&#x27;Total Files Number: <span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> path <span class="keyword">in</span> files:</span><br><span class="line">            root, name = op.split(path)</span><br><span class="line">            rename(root, name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;Please input whether only root, or only path, or both root and name&#x27;</span>)</span><br></pre></td></tr></table></figure><p>使用的话直接将该文件存到一个新建的<code>reg_match.py</code>文件中即可。使用Python3.6编写，理论支持所有Python3版本。</p><p>但是，仅用Python无法直接被utorrent调用，它想要的是像<code>exe</code>或<code>cmd</code>之类的文件，并想传递<strong>下载好的文件所在目录</strong>和<strong>下载好的文件名</strong>进去。这样我们最好创建一个<code>cmd</code>文件，用于接受参数，并传递给Python脚本。</p><h3 id="run_after_done.cmd"><code>run_after_done.cmd</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set file_name=%1%</span><br><span class="line">set directory=%2%</span><br><span class="line"></span><br><span class="line">python &lt;YOUR_SCRIPT_PATH&gt;\reg_match.py --name=%file_name% --root=%directory%</span><br></pre></td></tr></table></figure><p>其中，<code>&lt;YOUR_SCRIPT_PATH&gt;</code>是你存放上面提到的Python脚本的目录位置。</p><h3 id="utorrent配置">utorrent配置</h3><p>完成了上面的两个脚本，让我们将其配置到utorrent的<code>当下载完成时运行此程序</code>中。效果就是，每当一个文件下载完成，utorrent都会自动调用这个<code>cmd</code>文件，并且将文件目录和名字都作为参数传递进去。</p><p><img src="/2021/02/01/seed-anime-system/image-20210130122024176.png" alt="image-20210130122024176" style="zoom:50%;"></p><p>命令为：<code>&lt;YOUR_CMD_PATH&gt;\run_after_done.cmd "%F" "%D"</code></p><p>其中，<code>&lt;YOUR_CMD_PATH&gt;</code>为你存放脚本<code>run_after_done.cmd</code>的目录。推荐将它和Python脚本放在同一个目录下。</p><h2 id="plex">Plex</h2><p>引用一段Wiki，以方便不熟悉的朋友们了解：</p><blockquote><p><strong>Plex</strong>是一套<a href="https://zh.wikipedia.org/wiki/媒体播放器">媒体播放器</a>及<a href="https://zh.wikipedia.org/w/index.php?title=媒體伺服器&amp;action=edit&amp;redlink=1">媒体服务器</a>软件，让用户整理在设备上的<a href="https://zh.wikipedia.org/wiki/有聲書">有声书</a>、<a href="https://zh.wikipedia.org/wiki/音樂">音乐</a>、<a href="https://zh.wikipedia.org/wiki/播客">播客</a>、<a href="https://zh.wikipedia.org/wiki/圖片">图片</a>和<a href="https://zh.wikipedia.org/wiki/影片">视频</a>文件，以供<a href="https://zh.wikipedia.org/wiki/串流">流</a>至<a href="https://zh.wikipedia.org/wiki/流動裝置">移动设备</a>、<a href="https://zh.wikipedia.org/wiki/智能電視">智能电视</a>和<a href="https://zh.wikipedia.org/w/index.php?title=電子媒體播放器&amp;action=edit&amp;redlink=1">电子媒体播放器</a>上。Plex可用于<a href="https://zh.wikipedia.org/wiki/Microsoft_Windows">Windows</a>、<a href="https://zh.wikipedia.org/wiki/Android">Android</a>、<a href="https://zh.wikipedia.org/wiki/Linux">Linux</a>、<a href="https://zh.wikipedia.org/wiki/OS_X">OS X</a>和<a href="https://zh.wikipedia.org/wiki/FreeBSD">FreeBSD</a>[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-Gigaom-2">2]</a>。另外，Plex亦让用户透过该平台观看来自<a href="https://zh.wikipedia.org/wiki/YouTube">YouTube</a>、<a href="https://zh.wikipedia.org/wiki/Vimeo">Vimeo</a>和<a href="https://zh.wikipedia.org/wiki/TED大會">TED</a>等内容提供商的视频。Plex亦与<a href="https://zh.wikipedia.org/w/index.php?title=Bitcasa&amp;action=edit&amp;redlink=1">Bitcasa</a>、<a href="https://zh.wikipedia.org/wiki/Box公司">Box</a>和<a href="https://zh.wikipedia.org/wiki/Dropbox">Dropbox</a>等云端服务兼容[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-3">3]</a>[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-4">4]</a>。</p><p>用户可透过Plex<a href="https://zh.wikipedia.org/wiki/前端和后端">前端</a>媒体播放器“Plex Media Player”管理及播放在一台运行“Plex Media Server”的远程电脑上的多媒体文件。另外，用户可使用“Plex Online”服务以社区开发的插件收看<a href="https://zh.wikipedia.org/wiki/Netflix">Netflix</a>、<a href="https://zh.wikipedia.org/wiki/Hulu">Hulu</a>和<a href="https://zh.wikipedia.org/wiki/CNN">CNN</a>的视频。[<a href="https://zh.wikipedia.org/wiki/Plex#cite_note-CrunchGear_Interview-5">5]</a></p></blockquote><p>简单说，就是Plex可以管理你的连续剧集（如番剧）和电影，解析出它们的元数据（如海报，封面，演职员表，简介，分集标题等等），在你的电脑上自动搭建一个<strong>服务器</strong>，向你在其他设备上的Plex软件提供像是Bilibili或Netflix一样的串流服务。形象点说就是，你自己搭建了一个简单的视频网站，数据库是你电脑上的内容，供你自己使用。</p><p>另外，Plex不仅支持内网访问，在外网访问也是可以的。内网访问要求终端设备连接到和你的电脑同一个Wi-Fi，而外网则是只要有网络连接就行。当然，内网的速度和清晰度都是更高的。</p><p>Plex支持在Windows，Mac，NAS，Docker等位置安装其server，安装过程十分简单，基本一路Next即可。</p><p>有了前面的设置，这里就很简单了。假设你的文件结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Anime</span><br><span class="line">|- 工作细胞</span><br><span class="line">|- Season 1</span><br><span class="line">        |- 01.mp4</span><br><span class="line">        |- 02.mp4</span><br><span class="line">        |...</span><br><span class="line">        |- 12.mp4</span><br><span class="line">        |- Season 2</span><br><span class="line">        |- 01.mp4</span><br><span class="line">        |- ...</span><br><span class="line">|- 偶像荣耀</span><br><span class="line">|- 01.mp4</span><br><span class="line">        |- ...</span><br></pre></td></tr></table></figure><p>那么，你所需要做的只是：创建一个账户，然后启动你的服务器页面（地址栏输入<a href="https://www.plex.tv/">plex.tv</a>，右上角Launch即可，或是直接点击<a href="https://app.plex.tv/desktop#">Link</a>）：</p><p><img src="/2021/02/01/seed-anime-system/image-20210130123759950.png" alt="image-20210130123759950" style="zoom:50%;"></p><p>点击<code>+</code>号，新建一个库，</p><p><img src="/2021/02/01/seed-anime-system/image-20210130123947605.png" alt="image-20210130123947605" style="zoom:50%;"></p><p>简单两步，完成！</p><p>当然我们这里介绍的仅仅是添加剧集的方法，电影库、音乐库和图片库的添加方法类似。你完全可以把你的照片文件夹放到Plex上，这样你就可以在任何地方看到你的照片啦！另外，由于<strong>服务器</strong>是你自己的电脑，它并不会上传你的文件到某个云端，所以安全性也是较有保障的。</p><p>其中电影和电视节目库的最大区别是是否分集和分季。电影的解析针对的是单个的视频文件，而剧集的解析针对的是某个子文件夹。</p><p>之后，你可以进入你的库，查看效果。很大一部分内容会直接成功抓取元数据得到漂亮的封面等内容，另一些则会失败。对于失败的内容，如果你不在意其实也不影响观看，只是没有封面和介绍等。如果你介意，可以选择手动协助匹配：</p><figure><img src="/2021/02/01/seed-anime-system/image-20210130125127348.png" alt="image-20210130125127348"><figcaption aria-hidden="true">image-20210130125127348</figcaption></figure><p>匹配失败的主要原因是命名问题。在<code>修复匹配-搜索选项</code>中可以更改用于搜索的命名。另外，有的时候是因为搜索的数据库中没有这个数据，此时，你可以切换<code>代理</code>，很多时候是可以找到匹配的。如果你的命名是日语或英语，那么把搜索的语言相应修改。之后点击搜索即可。</p><h2 id="其他">其他</h2><h3 id="注意">注意</h3><ul><li>该方法适用但不仅适用于动漫。其他类型影视剧集也可，只要你能找到合适的订阅源。</li><li>本文涉及的所有脚本都可以在该<a href="https://github.com/miracleyoo/anime_renamer">GitHub</a>库上找到。</li><li>动漫正版化事业不易，各大网站都投入了巨大的资源来建设更全的正版资源库，请尽最大可能支持正版。请将本方法主要用于观看网盘见类剧集和存在删减的剧集。</li></ul><h3 id="trouble-shooting">Trouble Shooting</h3><p>需要注意的是，由于utorrent在上传时会占用文件，所以有一定概率重命名会失败。解决办法也很简单，如果你觉得有问题的时候，关闭utorrent解除占用，往往需要等上几秒，然后直接将整个<code>Anime</code>文件夹（存放动漫的根文件夹）拖放到下面这个<code>cmd</code>文件上即可：</p><p><code>rename_episodes.cmd</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> %%i <span class="keyword">in</span> (%*) <span class="keyword">do</span> python &lt;YOUR_SCRIPT_PATH&gt;\reg_match.py --path=%%i</span><br></pre></td></tr></table></figure><p>该文件会调用前面提到的Python脚本，并对整个文件夹中的所有有问题的文件名进行重命名。你也可以将有问题的子目录或特定文件拖放上去，或是同时拖放多个文件/文件夹。Python脚本中有着相应的适配。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;：你喜欢的番剧更新了。是你喜欢的字幕组的高清资源。里面没有放肆的圣光和暗牧。尽管它也许没在国内放送。你可以在任何设备上观看它，并且可以无缝切换。电视，手机，iPad，电脑都没有问题。它很快。&lt;/p&gt;
&lt;p&gt;这将是本篇介绍的全自动全平台订阅追番系统。&lt;/p&gt;
&lt;p&gt;出于各种原因，有许多番剧在B站并找不到。即使开了大会员，从国外驾梯回国，还是无法看到一些喜欢的内容。但同时，各大动漫种子站却提供了几乎无所不包的资源，其中包括新番。但依靠种子追番最常见的问题就是难以track，经常会忘记更新，而且每次都需要经过一个&lt;code&gt;搜索-下载种子-下载-整理-观看&lt;/code&gt;的过程，确实还是很劝退的。如何搭建一个易于配置、足够简单、全自动化抓取、下载、串流的追番系统，便成为了一个追番人的核心需求。&lt;/p&gt;
&lt;p&gt;下面本文将会带领你认识整套流程，如果你足够经常折腾，大致一个小时之内就可以搭建完成。祝你好运:)&lt;/p&gt;
&lt;p&gt;PS: 本文涉及的所有脚本都可以在该&lt;a href=&quot;https://github.com/miracleyoo/anime_renamer&quot;&gt;GitHub&lt;/a&gt;库上找到。&lt;/p&gt;</summary>
    
    
    
    
    <category term="acg" scheme="https://www.miracleyoo.com/tags/acg/"/>
    
    <category term="anime" scheme="https://www.miracleyoo.com/tags/anime/"/>
    
    <category term="system" scheme="https://www.miracleyoo.com/tags/system/"/>
    
    <category term="torrent" scheme="https://www.miracleyoo.com/tags/torrent/"/>
    
  </entry>
  
  <entry>
    <title>Git Submodule 攻略</title>
    <link href="https://www.miracleyoo.com/2020/10/22/git-sub-module/"/>
    <id>https://www.miracleyoo.com/2020/10/22/git-sub-module/</id>
    <published>2020-10-23T02:26:00.000Z</published>
    <updated>2021-03-12T22:26:24.061Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tldr">TL;DR</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add submodule</span></span><br><span class="line">git submodule add</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clone a project with submodules</span></span><br><span class="line">git <span class="built_in">clone</span> --recursive</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update when submodeule remote repo changed</span></span><br><span class="line">git submodule update --remote</span><br><span class="line"></span><br><span class="line"><span class="comment"># When cloned without recursive</span></span><br><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br><span class="line"></span><br><span class="line"><span class="comment"># Push submodule change to its remote origin master</span></span><br><span class="line"><span class="built_in">cd</span> &lt;submodule_name&gt;</span><br><span class="line">git add -A .</span><br><span class="line">git commit -m <span class="string">&quot;xxx&quot;</span></span><br><span class="line">git checkout &lt;detached branch name/number&gt;</span><br><span class="line">git merge master</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure><span id="more"></span><h2 id="定义">定义</h2><p><code>git submodule</code>允许用户将一个 Git 仓库作为另一个 Git 仓库的子目录。 它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立性。</p><h2 id="作用">作用</h2><p>在我这里，它的作用非常明确，即给在各个项目中都会用到的代码段一个公共栖息地，做到“一处改，处处改”。</p><h2 id="常用命令">常用命令</h2><h3 id="添加">添加</h3><p><code>git submodule add</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接clone，会在当前目录生成一个someSubmodule目录存放仓库内容</span></span><br><span class="line">git submodule add https://github.com/miracleyoo/someSubmodule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定文件目录</span></span><br><span class="line">git submodule add https://github.com/miracleyoo/someSubmodule  src/submodulePath</span><br></pre></td></tr></table></figure><p>添加完之后，子模块目录还是空的（似乎新版不会了），此时需要执行：</p><p><code>git submodule update --init --recursive</code></p><p>来真正将子模块中的内容clone下来。同时，如果你的主目录在其他机器也有了一份clone，它们也需要执行上面的命令来把远端关于子模块的更改实际应用。</p><h3 id="clone时子模块初始化">Clone时子模块初始化</h3><p><code>clone</code>父仓库的时候加上<code>--recursive</code>，会自动初始化并更新仓库中的每一个子模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recursive</span><br></pre></td></tr></table></figure><p>或：</p><p>如果已经正常的<code>clone</code>了，那也可以做以下补救：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br></pre></td></tr></table></figure><p>正常<code>clone</code>包含子模块的函数之后，由于.submodule文件的存在<code>someSubmodule</code>已经自动生成，但是里面是空的。上面的两条命令分别：</p><ol type="1"><li>初始化的本地配置文件</li><li>从该项目中抓取所有数据并检出到主项目中。</li></ol><h3 id="更新">更新</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule update --remote</span><br></pre></td></tr></table></figure><p>Git 将会进入所有子模块，分别抓取并更新，默认更新master分支。</p><p>不带<code>--remote</code>的<code>update</code>只会在本地没有子模块或它是空的的时候才会有效果。</p><h3 id="推送子模块修改">推送子模块修改</h3><p>这里有一个概念，就是主repo中的子模块被拉到本地时默认是一个子模块远程仓库master分支的<code>detached branch</code>。这个分支是master的拷贝，但它不会被推送到远端。如果在子模块中做了修改，并且已经<code>add</code>，<code>commit</code>，那你会发现当你想要<code>push</code>的时候会报错：<code>Updates were rejected because a pushed branch tip is behind its remote</code>。这便是所谓的<code>detached branch</code>的最直接的体现。</p><p>解决方法是：在子模块中先<code>git checkout master</code>，然后在<code>git merge &lt;detached branch name/number&gt;</code>，最后<code>git push -u origin master</code>即可。</p><p>这里解释一下<code>&lt;detached branch name/number&gt;</code>这个东西可以使用<code>git branch</code>命令查看。如果你使用的是<code>zsh</code>，那么问题就更简单了，直接在命令提示符处就可以找到。</p><figure><img src="/2020/10/22/git-sub-module/image-20200704184550817.png" alt="image-20200704184550817"><figcaption aria-hidden="true">image-20200704184550817</figcaption></figure><figure><img src="/2020/10/22/git-sub-module/image-20200704184656815.png" alt="image-20200704184656815"><figcaption aria-hidden="true">image-20200704184656815</figcaption></figure><h2 id="参考">参考</h2><ol type="1"><li><p><a href="https://juejin.im/post/5d5ca6e06fb9a06b1a568e32">来说说坑爹的 git submodule</a></p></li><li><p><a href="https://juejin.im/post/5ca47a84e51d4565372e46e0">Git submodule使用指南（一）</a></p></li><li><p><a href="https://github.blog/author/jaw6/">Working with submodules</a></p></li><li><p><a href="https://stackoverflow.com/questions/18770545/why-is-my-git-submodule-head-detached-from-master">Why is my Git Submodule HEAD detached from master?</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Add submodule&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule add&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Clone a project with submodules&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; --recursive&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Update when submodeule remote repo changed&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule update --remote&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# When cloned without recursive&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule init&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git submodule update&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Push submodule change to its remote origin master&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; &amp;lt;submodule_name&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git add -A .&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git commit -m &lt;span class=&quot;string&quot;&gt;&amp;quot;xxx&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git checkout &amp;lt;detached branch name/number&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git merge master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git push -u origin master&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="git" scheme="https://www.miracleyoo.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Spectral Imaging 数据采集前期调研</title>
    <link href="https://www.miracleyoo.com/2020/10/16/hsi-pre-data-collection/"/>
    <id>https://www.miracleyoo.com/2020/10/16/hsi-pre-data-collection/</id>
    <published>2020-10-16T23:57:04.000Z</published>
    <updated>2021-03-12T22:04:46.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="equipment">Equipment</h2><h3 id="flir-blackfly-s-rgb-camera">FLIR Blackfly S RGB Camera</h3><ol type="1"><li><p>Spectral Range:</p><ul><li>Blue: 460 nm</li><li>Green: 530 nm</li><li>Red: 625 nm</li></ul></li><li><p>Resolution: 720 × 540</p></li><li><p>FPS: 522</p><span id="more"></span></li><li><p>Dimensions [W x H x L]: 29 mm × 29 mm × 30 mm</p></li><li><p>Official Link: <a href="https://www.flir.com/products/blackfly-s-usb3/">Link</a></p></li></ol><h3 id="ximea-mq022hg-im-sm5x5-nir-multispectral-camera">XIMEA MQ022HG-IM-SM5X5-NIR Multispectral Camera</h3><ol type="1"><li>Spectral Range: 665~975nm</li><li>Resolution:<ul><li>Original: 2048 × 1088</li><li>Spatial: 409 × 217</li></ul></li><li>FPS: up to 170 cubes/sec</li><li>Sensor size: 2/3"</li><li>Dimensions WxHxD: 26 x 26 x 31 mm</li><li>Pixel size: 5.5 µm</li><li>Python multispectral processing lib: <a href="http://www.spectralpython.net/#documentation">Link</a></li><li>Camera control official python lib: <a href="https://www.ximea.com/support/wiki/apis/Python">Link</a></li><li>Official brief specification: <a href="https://www.ximea.com/files/brochures/xiSpec-Hyperspectral-cameras-2015-brochure.pdf">Link</a></li><li>Official Page: <a href="https://www.ximea.com/en/products/hyperspectral-cameras-based-on-usb3-xispec/mq022hg-im-sm5x5-nir">Link</a></li></ol><table><thead><tr class="header"><th>Full Specifications:</th><th></th></tr></thead><tbody><tr class="odd"><td><strong>Part Number</strong></td><td>MQ022HG-IM-SM5X5-NIR</td></tr><tr class="even"><td><strong>Resolution</strong></td><td>Original: 2048 × 1088 Spatial: 409 × 217</td></tr><tr class="odd"><td><strong>Frame rates</strong></td><td>up to 170 cubes/sec</td></tr><tr class="even"><td><strong>Sensor type</strong></td><td>CMOS, Hyperspectral filters added at wafer-level</td></tr><tr class="odd"><td><strong>Sensor model</strong></td><td>IMEC SNm5x5</td></tr><tr class="even"><td><strong>Sensor size</strong></td><td>2/3"</td></tr><tr class="odd"><td><strong>Sensor active area</strong></td><td>25 Bands</td></tr><tr class="even"><td><strong>Readout Method</strong></td><td>Snapshot Mosaic</td></tr><tr class="odd"><td><strong>Pixel size</strong></td><td>5.5 µm</td></tr><tr class="even"><td><strong>ADC -Bits per pixel</strong></td><td>8, 10 bit RAW pixel data</td></tr><tr class="odd"><td><strong>Data interface</strong></td><td>USB 3.1 Gen1 or PCI Express (xiX camera model)</td></tr><tr class="even"><td><strong>Data I/O</strong></td><td>GPIO IN, OUT</td></tr><tr class="odd"><td><strong>Power consumption</strong></td><td>1.6 Watt</td></tr><tr class="even"><td><strong>Lens mount</strong></td><td>C or CS Mount</td></tr><tr class="odd"><td><strong>Weight</strong></td><td>32 grams</td></tr><tr class="even"><td><strong>Dimensions WxHxD</strong></td><td>26 x 26 x 31 mm</td></tr><tr class="odd"><td><strong>Operating temperature</strong></td><td>50 °C</td></tr><tr class="even"><td><strong>Spectral range</strong></td><td>665-975 nm</td></tr><tr class="odd"><td><strong>Customs tariff code</strong></td><td>8525.80 30 (EU) / 8525.80 40 (USA)</td></tr><tr class="even"><td><strong>ECCN</strong></td><td>EAR99</td></tr></tbody></table><h3 id="seek-compact-pro-thermal-camera">Seek Compact Pro Thermal Camera</h3><ol type="1"><li>Seek Compact Pro: 7500~14000 nm</li><li>Resolution: <strong>320 x 240</strong></li><li>Field of view: <strong>32°</strong></li><li>Frame rate: <strong>&lt; 9 Hz</strong></li><li>Focusable lens</li><li>Platform: Android or iOS (Linux 3rd-party binary library)</li><li>Specification Sheet: <a href="https://www.thermal.com/uploads/1/0/1/3/101388544/compactpro-sellsheet-website.pdf">Link</a></li></ol><h2 id="introduction">Introduction</h2><p>Full spectrum:</p><figure><img src="/2020/10/16/hsi-pre-data-collection/2880px-EM_spectrum.svg.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/2880px-EM_Spectrum_Properties_edit_zh.svg.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/Atmospheric_electromagnetic_opacity.svg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/200px-Light_spectrum.svg.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h3 id="different-infrared"><strong>Different Infrared:</strong></h3><table><colgroup><col style="width: 14%"><col style="width: 15%"><col style="width: 11%"><col style="width: 15%"><col style="width: 12%"><col style="width: 15%"><col style="width: 15%"></colgroup><thead><tr class="header"><th style="text-align: center;">Division name</th><th style="text-align: center;">Abbreviation</th><th style="text-align: center;">Wavelength</th><th style="text-align: center;">Frequency</th><th style="text-align: center;">Photon energy</th><th style="text-align: center;">Temperature[<a href="https://en.wikipedia.org/wiki/Infrared#cite_note-†-15">i]</a></th><th style="text-align: center;">Characteristics</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Near-infrared</td><td style="text-align: center;">NIR, IR-A <em><a href="https://en.wikipedia.org/wiki/DIN">DIN</a></em></td><td style="text-align: center;">0.75–1.4 <a href="https://en.wikipedia.org/wiki/Μm">μm</a></td><td style="text-align: center;">214–400 <a href="https://en.wikipedia.org/wiki/Terahertz_(unit)">THz</a></td><td style="text-align: center;">886–1653 <a href="https://en.wikipedia.org/wiki/MeV">meV</a></td><td style="text-align: center;">3,864–2,070 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (3,591–1,797 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align: center;">Defined by water absorption,[<em><a href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify">clarification needed</a></em>] and commonly used in <a href="https://en.wikipedia.org/wiki/Fiber_optic">fiber optic</a> telecommunication because of low attenuation losses in the SiO2 glass (<a href="https://en.wikipedia.org/wiki/Silica">silica</a>) medium. <a href="https://en.wikipedia.org/wiki/Image_intensifier">Image intensifiers</a> are sensitive to this area of the spectrum; examples include <a href="https://en.wikipedia.org/wiki/Night_vision">night vision</a> devices such as night vision goggles. <a href="https://en.wikipedia.org/wiki/Near-infrared_spectroscopy">Near-infrared spectroscopy</a> is another common application.</td></tr><tr class="even"><td style="text-align: center;">Short-wavelength infrared</td><td style="text-align: center;">SWIR, IR-B <em>DIN</em></td><td style="text-align: center;">1.4–3 μm</td><td style="text-align: center;">100–214 THz</td><td style="text-align: center;">413–886 meV</td><td style="text-align: center;">2,070–966 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (1,797–693 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align: center;">Water absorption increases significantly at 1450 nm. The 1530 to 1560 nm range is the dominant spectral region for long-distance telecommunications.</td></tr><tr class="odd"><td style="text-align: center;">Mid-wavelength infrared</td><td style="text-align: center;">MWIR, IR-C <em>DIN</em>; MidIR.[<a href="https://en.wikipedia.org/wiki/Infrared#cite_note-rdmag20120908-16">15]</a> Also called intermediate infrared (IIR)</td><td style="text-align: center;">3–8 μm</td><td style="text-align: center;">37–100 THz</td><td style="text-align: center;">155–413 meV</td><td style="text-align: center;">966–362 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (693–89 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align: center;">In guided missile technology the 3–5 μm portion of this band is the atmospheric window in which the homing heads of passive IR 'heat seeking' missiles are designed to work, homing on to the <a href="https://en.wikipedia.org/wiki/Infrared_signature">Infrared signature</a> of the target aircraft, typically the jet engine exhaust plume. This region is also known as thermal infrared.</td></tr><tr class="even"><td style="text-align: center;">Long-wavelength infrared</td><td style="text-align: center;">LWIR, IR-C <em>DIN</em></td><td style="text-align: center;">8–15 μm</td><td style="text-align: center;">20–37 THz</td><td style="text-align: center;">83–155 meV</td><td style="text-align: center;">362–193 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (89 – −80 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align: center;">The "thermal imaging" region, in which sensors can obtain a completely passive image of objects only slightly higher in temperature than room temperature - for example, the human body - based on thermal emissions only and requiring no illumination such as the sun, moon, or infrared illuminator. This region is also called the "thermal infrared".</td></tr><tr class="odd"><td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Far_infrared">Far infrared</a></td><td style="text-align: center;">FIR</td><td style="text-align: center;">15–1000 μm</td><td style="text-align: center;">0.3–20 THz</td><td style="text-align: center;">1.2–83 meV</td><td style="text-align: center;">193–3 <a href="https://en.wikipedia.org/wiki/Kelvin">K</a> (−80.15 – −270.15 <a href="https://en.wikipedia.org/wiki/Celsius">°C</a>)</td><td style="text-align: center;">(see also <a href="https://en.wikipedia.org/wiki/Far-infrared_laser">far-infrared laser</a> and <a href="https://en.wikipedia.org/wiki/Far_infrared">far infrared</a>)</td></tr></tbody></table><h2 id="thermal">Thermal</h2><h3 id="dataset">Dataset</h3><ol type="1"><li><p><a href="https://www.flir.com/oem/adas/adas-dataset-form/">FREE FLIR Thermal Dataset for Algorithm Training</a></p><figure><img src="/2020/10/16/hsi-pre-data-collection/image-20200716161001770.png" alt="image-20200716161001770"><figcaption aria-hidden="true">image-20200716161001770</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/image-20200716161013251.png" alt="image-20200716161013251"><figcaption aria-hidden="true">image-20200716161013251</figcaption></figure></li><li><p><a href="https://soonminhwang.github.io/rgbt-ped-detection/">KAIST Multispectral Pedestrian Detection Benchmark</a> [2018] <a href="https://www-users.cs.umn.edu/~jsyoon/JaeShin_homepage/kaist_multispectral.pdf">Paper</a></p><p>Contain day and night scenarios. Human with bounding box. RGB-Thermal pair.</p><p>The KAIST Multispectral Pedestrian Dataset consists of 95k color-thermal pairs (640x480, 20Hz) taken from a vehicle. All the pairs are manually annotated (person, people, cyclist) for the total of 103,128 dense annotations and 1,182 unique pedestrians.</p><figure><img src="/2020/10/16/hsi-pre-data-collection/teaser.png" alt="teaserImage"><figcaption aria-hidden="true">teaserImage</figcaption></figure></li></ol><h2 id="real-multispectral">Real-Multispectral</h2><ol type="1"><li><p><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset">Hyperspectral Images Database</a> [2017]</p><p><strong>Visible Range MSI</strong></p><p>NUS hyperspectral images database: 52 Outdoor Scene, 35 Indoor Scene, 33 Individual Fruit Scene, 11 Group Fruit Scene, 13 Real vs Fake Fruit Scene, 44 color Charts &amp; Patches Scene.</p></li></ol><p>It consists of various indoor and outdoor scenes taken with a SPECIM hyperspectral camera and multiple consumer cameras. For consumer cameras, camera-specific RAW format that is free of any manipulation, is available. For easier classification, this hyperspectral camera dataset has been categorized into the following categories:</p><ul><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/general-scenes">General Scenes (Outdoor &amp; Indoor)</a></li><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/fruits">Fruits</a></li><li><a href="https://sites.google.com/site/hyperspectralcolorimaging/dataset/color-patches">Color Charts and Patches</a></li></ul><p>Additionally, our spectral data can be visualized using the professional software by <a href="http://scyllarus.research.nicta.com.au/">Scyllarus Matlab/C++ toolbox</a>.</p><p>Relevant Code <a href="https://github.com/trangreyle/gene-color-mapping">GitHub</a></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729094343425.png" alt="image-20200729094343425" style="zoom: 50%;"></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729094355733.png" alt="image-20200729094355733" style="zoom:50%;"></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729094407105.png" alt="image-20200729094407105" style="zoom:50%;"></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729094457881.png" alt="image-20200729094457881" style="zoom:50%;"></p><ol start="2" type="1"><li><p><a href="https://biic.wvu.edu/data-sets/multispectral-dataset">Multispectral Dataset from west virginia university</a></p><ol type="1"><li>SWIR Biometrics Dataset: <strong>SWIR</strong></li><li>WVU Multispectral Face Database: Three types of camera are used: <strong>RGB, Multi(RGB+NIR), SWIR</strong></li><li>Multispectral Imaging (Iris) Database:</li></ol></li><li><p><a href="https://www.mi.t.u-tokyo.ac.jp/static/projects/mil_multispectral/">Multispectral Image Recognition</a></p><ol type="1"><li>Multi-spectral Object Detection</li></ol><p><strong>RGB, Near-infrared (NIR), Mid-wavelength infrared (MIR), and Far infrared (FIR)</strong> from the left. Objects are labeled and bounding box predicted.</p><figure><img src="/2020/10/16/hsi-pre-data-collection/det_result.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><ol start="2" type="1"><li>Multi-spectral Semantic Segmentation</li></ol><p>RGB-Thermal dataset with semantic segmentation</p><figure><img src="/2020/10/16/hsi-pre-data-collection/predictionExamples_good.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure></li><li><p><a href="https://projects.ics.forth.gr/cvrl/msi/">Multispectral Imaging (MSI) datasets</a>: Painting multispectral images. Not paired. Not ordinary objects.</p><figure><img src="/2020/10/16/hsi-pre-data-collection/image-20200727175214182.png" alt="image-20200727175214182"><figcaption aria-hidden="true">image-20200727175214182</figcaption></figure></li><li><p><a href="https://www.cs.columbia.edu/CAVE/databases/multispectral/">CAVE Multispectral Image Database</a></p><p><strong>Visible Range MSI:</strong> <strong>400nm to 700nm</strong></p><p>It only has 32 multispectral &amp; RGB image pairs... Be careful to use it. Each image has 31 bands, and they are separated.</p><table><thead><tr class="header"><th>Camera</th><th><a href="http://www.ccd.com/alta_u260.html">Cooled CCD camera (Apogee Alta U260)</a></th></tr></thead><tbody><tr class="odd"><td>Resolution</td><td>512 x 512 pixel</td></tr><tr class="even"><td>Filter</td><td><a href="http://www.cri-inc.com/products/varispec.asp">VariSpec liquid crystal tunable filter</a></td></tr><tr class="odd"><td>Illuminant</td><td>CIE Standard Illuminant D65</td></tr><tr class="even"><td>Range of wevelength</td><td>400nm - 700nm</td></tr><tr class="odd"><td>Steps</td><td>10nm</td></tr><tr class="even"><td>Number of band</td><td>31 band</td></tr><tr class="odd"><td>Focal length</td><td>f/1.4</td></tr><tr class="even"><td>Focus</td><td>Fixed (focused using 550nm image)</td></tr><tr class="odd"><td>Image format</td><td>PNG (16bit)</td></tr></tbody></table><figure><img src="/2020/10/16/hsi-pre-data-collection/teaser-20200727174750119.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure></li><li><p><a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/">Bristol Hyperspectral Images Database</a> [1995]</p><p><strong>Visible Range MSI</strong></p><p>The database consists of <strong>29 scenes</strong>, each composed by <strong>31 spectrally filtered images</strong> (256 x 256 x 256 grey levels). Each scene has been compressed (zipped) and can be downloaded separately by clicking on the corresponding picture. Please bear in mind that all individual images have a 32 bytes header. To download the whole database at once, just click <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/brelstaff.tar.gz">here</a>.</p><p>There is some code and miscellaneous files <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/src/Src.zip">here</a> (these need to be run in order to make use of the images as physical measurements). A more complete description on how the images were gathered and some issues on the camera's technicalities can be found <a href="http://www.cvc.uab.es/color_calibration/Bristol_Hyper/2-TECH.pdf">here</a>.</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729095320320.png" alt="image-20200729095320320" style="zoom:50%;"></p></li><li><p><a href="http://vision.seas.harvard.edu/hyperspec/">Harvard Real-World Hyperspectral Images</a> [2011]</p><p><strong>Visible Range MSI:</strong> <strong>420nm to 720nm</strong></p><p>The camera uses an integrated liquid crystal tunable filter and is capable of acquiring a hyperspectral image by sequentially tuning the filter through a series of <strong>31 narrow wavelength bands</strong>, each with approximately 10nm bandwidth and centered at steps of 10nm from <strong>420nm to 720nm</strong>.</p><p>The captured dataset includes images of both indoor and outdoor scenes featuring a diversity of objects, materials and scale.</p><p>This is a database of <strong>50</strong> hyperspectral images of indoor and outdoor scenes under daylight illumination, and an additional <strong>25</strong> images under artificial and mixed illumination. The images were captured using a commercial hyperspectral camera (Nuance FX, CRI Inc) with an integrated liquid crystal tunable filter capable of acquiring a hyperspectral image by sequentially tuning the filter through a series of thirty-one narrow wavelength bands, each with approximately 10nm bandwidth and centered at steps of 10nm from 420nm to 720nm. The camera is equipped with an apo-chromatic lens and the images were captured with the smallest viable aperture setting, thus largely avoiding chromatic aberration. All the images are of static scenes, with labels to mask out regions with movement during exposure.</p><p>This database is available for non-commercial research use. The data is available as a series of MATLAB .mat files (one for each image) containing both the images data and masks. Since the size of the download is large (around 5.5 + 2.2 GB), we ask that you send an e-mail to the authors at <strong>ayanc[at]eecs[dot]harvard[dot]edu</strong> for the download link. If you use this data in an academic publication, kindly cite the following paper:</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729100019504.png" alt="image-20200729100019504" style="zoom:50%;"></p></li><li><p><a href="http://colour.cmp.uea.ac.uk/datasets/multispectral.html">UAE multispectral image database</a></p><p><strong>Visible Range MSI:</strong> <strong>400nm to 700nm</strong></p><p>Wavelength range from 400nm to 700nm at 10nm steps (31 samples). The image matrix for each object is 31xWIDTHxHEIGHT. The images have been captured in a VeriVide viewing booth with a black cloth background under CIE illuminant D75. Each image has been captured twice: once with a white tile and once without. The illuminant has been estimated from the white tile and the spectral data divided by this estimate, in order to arrive at reflectance measurements. The images below are displayed sRGB values rendered under a neutral daylight (D65).</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729100842353.png" alt="image-20200729100842353" style="zoom:33%;"></p></li><li><p><a href="http://personalpages.manchester.ac.uk/staff/david.foster/default.html">Manchester hyperspectral images Dataset<strong>s</strong></a></p><p><strong>Visible Range MSI:</strong> 400, 410, ..., 720 nm</p><p>Multiple MSI datasets included:</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729101027937.png" alt="image-20200729101027937" style="zoom:50%;"></p><ul><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html">Time-Lapse Hyperspectral Radiance Images of Natural Scenes 2015</a></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729101135099.png" alt="image-20200729101135099" style="zoom:33%;"></p></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Local_Illumination_HSIs/Local_Illumination_HSIs_2015.html">Hyperspectral Images for Local Illumination in Natural Scenes 2015</a></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729101232518.png" alt="image-20200729101232518" style="zoom:50%;"></p></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_02.html">Hyperspectral Images of Natural Scenes 2002</a></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729101343922.png" alt="image-20200729101343922" style="zoom:50%;"></p></li><li><p><a href="https://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_04.html">Hyperspectral Images of Natural Scenes 2004</a></p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729101441578.png" alt="image-20200729101441578" style="zoom:50%;"></p></li></ul></li><li><p><a href="https://pythonhosted.org/bob.db.cbsr_nir_vis_2/">BOB NIR+VIS Face Database</a> [2013]</p><p>It consists of 725 subjects in total. There are [1-22] VIS and [5-50] NIR face images per subject. The eyes positions are also distributed with the images.</p><p><img src="/2020/10/16/hsi-pre-data-collection/database.png" alt="_images/database.png" style="zoom:33%;"></p></li><li><p><a href="http://icvl.cs.bgu.ac.il/hyperspectral/">ICVL hyperspectral database</a></p></li></ol><p><strong>RGB+NIR Range MSI:</strong> Images were collected at 1392×1300 spatial resolution over 519 spectral bands (<strong>400-1,000nm</strong> at roughly 1.25nm increments)</p><p>The database images were acquired using a Specim PS Kappa DX4 hyperspectral camera and a rotary stage for spatial scanning. At this time it contains 201 images and will continue to grow progressively. For your convenience, <strong>.mat</strong> files are provided, downsampled to 31 spectral channels from 400nm to 700nm at 10nm increments.</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729102126434.png" alt="image-20200729102126434" style="zoom:50%;"></p><ol start="11" type="1"><li><p><a href="http://colorimaginglab.ugr.es/pages/Data">University of Granada hyperspectral image database</a></p><p><strong>RGB+NIR Range MSI:</strong> Most of the images have spatial resolution of 1000 × 900 pixels. The spectral range is from <strong>400 nm to 1000</strong> nm in 10 nm intervals, resulting in total 61 channels.</p><p><img src="/2020/10/16/hsi-pre-data-collection/image-20200729102332515.png" alt="image-20200729102332515" style="zoom:50%;"></p></li><li><p><a href="http://www.cs.cmu.edu/~ILIM/projects/IM/MSPowder/">SWIRPowder</a>: A 400-1700nm Multispectral Dataset with 100 Powders on Complex Backgrounds</p><p><strong>SWIR(Multi)+RGB+NIR</strong></p><figure><img src="/2020/10/16/hsi-pre-data-collection/result.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/illustration_1.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure></li><li><p><a href="http://www.ok.sc.e.titech.ac.jp/res/MSI/MSIdata31.html">TokyoTech 31-band Hyperspectral Image Dataset</a> [2015]</p><p><strong>Visible Range MSI:</strong> <strong>420nm to 720nm</strong></p><p>Colorful objects with rich textures 30 scenes from 420nm to 720nm at 10nm intervals</p><figure><img src="/2020/10/16/hsi-pre-data-collection/MSimage.png" alt="MSimage"><figcaption aria-hidden="true">MSimage</figcaption></figure><figure><img src="/2020/10/16/hsi-pre-data-collection/image-20200729094232057.png" alt="image-20200729094232057"><figcaption aria-hidden="true">image-20200729094232057</figcaption></figure></li></ol><h2 id="reference">Reference</h2><ul><li><a href="https://jbthomas.org/TechReport/CIC-shortcourseSFA-2017.pdf">Spectral Filter Arrays Technology</a></li><li><a href="https://en.wikipedia.org/wiki/Infrared">Infrared WiKi</a></li></ul><h2 id="electromagnetic-wave-classification">Electromagnetic Wave Classification</h2><p>γ = <a href="https://zh.wikipedia.org/wiki/伽馬射線">伽马射线</a> <strong><a href="https://zh.wikipedia.org/wiki/X射線">X射线</a>：</strong> HX = 硬<a href="https://zh.wikipedia.org/wiki/X射線">X射线</a> SX = 软X射线 <strong><a href="https://zh.wikipedia.org/wiki/紫外線">紫外线</a>：</strong> EUV = 极端<a href="https://zh.wikipedia.org/wiki/紫外線">紫外线</a> NUV = 近紫外线 <strong><a href="https://zh.wikipedia.org/wiki/紅外線">红外线</a>：</strong> NIR = 近<a href="https://zh.wikipedia.org/wiki/紅外線">红外线</a> MIR =中红外线 FIR = <a href="https://zh.wikipedia.org/wiki/遠紅外線">远红外线</a></p><p>Typically we define near infrared (<em>NIR</em>) from 780 nm to 1400 nm and shortwave infrared (<em>SWIR</em>) from 1400 nm to 3000 nm.</p><p><strong><a href="https://zh.wikipedia.org/wiki/微波">微波</a>：</strong> EHF = <a href="https://zh.wikipedia.org/wiki/極高頻">极高频</a> SHF = <a href="https://zh.wikipedia.org/wiki/超高頻">超高频</a> UHF = <a href="https://zh.wikipedia.org/wiki/特高頻">特高频</a> <strong><a href="https://zh.wikipedia.org/wiki/無線電波">无线电波</a>：</strong> VHF = <a href="https://zh.wikipedia.org/wiki/甚高頻">甚高频</a> HF = <a href="https://zh.wikipedia.org/wiki/高頻">高频</a> MF = <a href="https://zh.wikipedia.org/wiki/中頻">中频</a> LF = <a href="https://zh.wikipedia.org/wiki/低頻">低频</a> VLF = <a href="https://zh.wikipedia.org/wiki/甚低频">甚低频</a> ULF = <a href="https://zh.wikipedia.org/wiki/特低頻">特低频</a> ELF = <a href="https://zh.wikipedia.org/wiki/極低頻">极低频</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;equipment&quot;&gt;Equipment&lt;/h2&gt;
&lt;h3 id=&quot;flir-blackfly-s-rgb-camera&quot;&gt;FLIR Blackfly S RGB Camera&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Spectral Range:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blue: 460 nm&lt;/li&gt;
&lt;li&gt;Green: 530 nm&lt;/li&gt;
&lt;li&gt;Red: 625 nm&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Resolution: 720 × 540&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FPS: 522&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="hsi" scheme="https://www.miracleyoo.com/tags/hsi/"/>
    
    <category term="data" scheme="https://www.miracleyoo.com/tags/data/"/>
    
    <category term="camera" scheme="https://www.miracleyoo.com/tags/camera/"/>
    
    <category term="optics" scheme="https://www.miracleyoo.com/tags/optics/"/>
    
  </entry>
  
  <entry>
    <title>日系绘画构图</title>
    <link href="https://www.miracleyoo.com/2020/09/12/illustration-composition/"/>
    <id>https://www.miracleyoo.com/2020/09/12/illustration-composition/</id>
    <published>2020-09-13T00:48:14.000Z</published>
    <updated>2021-03-12T23:12:02.117Z</updated>
    
    <content type="html"><![CDATA[<p><strong>构图法+出镜比例+人物动态=好的构图</strong></p><h2 id="常见单人构图">常见单人构图</h2><h3 id="对称构图">对称构图</h3><p>主要包含：左右对称，对角线对称</p><p>相对比较中规中矩，但是同样的，限制会比较大，也需要更多细节上的<strong>不对称</strong>来中和构图上的对称。这些不对称元素往往来自于：人物动态、道具、背景元素。</p><p><img src="/2020/09/12/illustration-composition/image-20200912154217731.png" alt="image-20200912154217731" style="zoom:50%;"></p><span id="more"></span><h3 id="九宫格构图">九宫格构图</h3><p>将重要的点置于九宫格的焦点上，起到强化关键要素的作用，如脸部、关节等。</p><figure><img src="/2020/09/12/illustration-composition/image-20200912153443124.png" alt="image-20200912153443124"><figcaption aria-hidden="true">image-20200912153443124</figcaption></figure><h3 id="黄金比例构图">黄金比例构图</h3><p>B格较高，人物的人体曲线和黄金分割线契合，螺旋正中央缩于人物脸部。该构图较为有特色，有着不对称的美感。</p><p><img src="/2020/09/12/illustration-composition/image-20200912153516507.png" alt="image-20200912153516507" style="zoom:50%;"></p><h3 id="好用技巧">好用技巧</h3><ol type="1"><li><p>人物动态和道具都可以被用来表达动态。如果人体动态比较复杂灵动，衣物可以相对简单点；而如果人物动态想画的简单保守一点，可以使用道具（长而飘逸的头发、大而舞动的裙摆、长长的布料...）</p><p><img src="/2020/09/12/illustration-composition/image-20200912172118092.png" alt="image-20200912172118092" style="zoom:50%;"></p></li><li><p>利用人物动作带来动态感（风）-&gt; 头发和裙摆随风飘动，人物跳起来或浮于空中。这样可以让画面看起来更有活力和生命力。</p></li></ol><p><img src="/2020/09/12/illustration-composition/image-20200912172201727.png" alt="image-20200912172201727" style="zoom:50%;"></p><ol type="1"><li></li></ol><h2 id="出镜比例">出镜比例</h2><p>一般的人物在插画中的出镜比例为：</p><ol type="1"><li><p>腰部以上：刻画部分更少，但是相对的，对头部和上身衣物的刻画就会要求更高。</p><p><img src="/2020/09/12/illustration-composition/image-20200912153749506.png" alt="image-20200912153749506" style="zoom:50%;"></p></li><li><p>大腿以上：可以刻画几乎全部的人物动态。</p><p>特点：方便表现躯干动态，方便表达透视。</p><p><img src="/2020/09/12/illustration-composition/image-20200912153853701.png" alt="image-20200912153853701" style="zoom:50%;"></p></li><li><p>全身：由于人体是一个细长的物体，如果要展现全身并主要由人物填满画面的话，需要作出很大的动作（弯曲）和大角度的透视。</p><p><img src="/2020/09/12/illustration-composition/image-20200912153932298.png" alt="image-20200912153932298" style="zoom:50%;"></p></li></ol><p>竖版插图（A4）三种比例都很多，而横版插图（1080p）则偏全身。</p><h2 id="常见双人构图">常见双人构图</h2><p>无论使用什么构图，两个人之间一定是有某种较为亲近的关系的。由于有不止一个人，此时的人物动态自由度会相对降低，而对构图法的依赖更高，同时也需要更加灵活准确的透视。相比单人，画面会更加丰富，也会侧重边线人物之间的联系、互动。</p><h3 id="平分型">平分型</h3><p>偏实用型。</p><p>往往会有某种形式的肢体接触，这样让全图显得更加自然。如果两个人站在一起又没有任何互动，画面会比价奇怪。</p><p>如果两个人的身高体型都近似，那自然是最好；如果身高体型有别，那么则需要通过某些部分（探出的头部，飘逸的头发、突出的膝盖、背包等道具）来平衡画面。</p><p>特点：画面易于平衡、容易理解好上手。对动态要求不高。</p><p><img src="/2020/09/12/illustration-composition/image-20200912155936519.png" alt="image-20200912155936519" style="zoom:50%;"></p><p><img src="/2020/09/12/illustration-composition/image-20200912161613370.png" alt="image-20200912161613370" style="zoom:50%;"></p><h3 id="八卦型">八卦型</h3><p>偏静谧美型，强调双人关系。</p><p>特点：两人之间往往会有交叉关系。构图有特征，画面不容易单调。</p><p>要求：对人物动态要求比较高。</p><p><img src="/2020/09/12/illustration-composition/image-20200912161749570.png" alt="image-20200912161749570" style="zoom:50%;"></p><p><img src="/2020/09/12/illustration-composition/image-20200912161941253.png" alt="image-20200912161941253" style="zoom:50%;"></p><h3 id="空间构图">空间构图</h3><p>偏氛围。</p><p>特点：两个人物由与摄像机镜头的远近产生了远近、大小的区别。让画面的空间感更足，从而对动态要求不高。</p><p><img src="/2020/09/12/illustration-composition/image-20200912162117364.png" alt="image-20200912162117364" style="zoom:50%;"></p><h2 id="常见多人构图">常见多人构图</h2><p>普通插画中使用相对较少，多用于动画海报、游戏登录界面。</p><h3 id="没有绝对主角的时候">没有绝对主角的时候</h3><h4 id="并排构图及其变形">并排构图及其变形</h4><p>人物基本分布在同一距离，处于同一直/曲线上。</p><p>特点：</p><ol type="1"><li>有着明显走势（为了让画面有序）</li><li>集中</li></ol><figure><img src="/2020/09/12/illustration-composition/image-20200912162754564.png" alt="image-20200912162754564"><figcaption aria-hidden="true">image-20200912162754564</figcaption></figure><p><img src="/2020/09/12/illustration-composition/image-20200912162706717.png" alt="image-20200912162706717" style="zoom:80%;"></p><h3 id="有绝对主角">有绝对主角</h3><p>以主角为中心扩散。</p><p><img src="/2020/09/12/illustration-composition/image-20200912163421619.png" alt="image-20200912163421619" style="zoom:50%;"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;构图法+出镜比例+人物动态=好的构图&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;常见单人构图&quot;&gt;常见单人构图&lt;/h2&gt;
&lt;h3 id=&quot;对称构图&quot;&gt;对称构图&lt;/h3&gt;
&lt;p&gt;主要包含：左右对称，对角线对称&lt;/p&gt;
&lt;p&gt;相对比较中规中矩，但是同样的，限制会比较大，也需要更多细节上的&lt;strong&gt;不对称&lt;/strong&gt;来中和构图上的对称。这些不对称元素往往来自于：人物动态、道具、背景元素。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/09/12/illustration-composition/image-20200912154217731.png&quot; alt=&quot;image-20200912154217731&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
  </entry>
  
  <entry>
    <title>日系人体绘画总结</title>
    <link href="https://www.miracleyoo.com/2020/08/21/illustration-human-body/"/>
    <id>https://www.miracleyoo.com/2020/08/21/illustration-human-body/</id>
    <published>2020-08-22T00:50:24.000Z</published>
    <updated>2021-03-12T22:51:22.670Z</updated>
    
    <content type="html"><![CDATA[<h2 id="日系画风与现实的区别">日系画风与现实的区别</h2><ul><li>真实的“<strong>三庭五眼</strong>”：<ul><li>三庭：<strong>d(发际线, 眉骨) = d(眉骨, 鼻底) = d(鼻底, 下巴)</strong></li><li>五眼：<strong>眼宽=眼间距=眼睛到面部轮廓的距离</strong></li></ul></li><li>眼睛的长度相对真实比例更长的，并不符合三庭五眼中的五眼比例。而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。</li></ul><p><img src="/2020/08/21/illustration-human-body/Screenshot%2520-%25202020-06-12%252022.46.34.png" alt="Screenshot - 2020-06-12 22.46.34" style="zoom:50%;"></p><span id="more"></span><h2 id="日系男生与女生刻画的特点与区别">日系男生与女生刻画的特点与区别</h2><h3 id="女生">女生</h3><ul><li>头部的高宽比偏小，脸部偏短。</li><li>面部相对较圆，使用更多的曲线。</li><li>五官相对下移，留出更高的额头放刘海。</li><li>眼睛是刻画的重点，它很大。</li><li>眼型偏圆的居多。</li><li>弱化鼻子、嘴巴、<strong>眉毛</strong>等的刻画。</li><li>眉毛和眼睛间距较大。</li><li>表情方面更多的表现出萌、可爱、甜美，总体偏美型。</li></ul><h3 id="男生">男生</h3><ul><li>头部的高宽比更大，脸部较长，或是更偏向真实比例。</li><li>面部外轮廓使用更多的直线、锐利的线条。</li><li>脸型更方正/直。</li><li>眼睛相对女生来说较小，或说较细，高度较低，但长度还是相对真实比例更长的，并不符合三庭五眼中的五眼比例，而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。</li><li>眼型偏平行四边形的居多。</li><li>同样眉毛也被着重刻画，意在表现男生的英气、帅气等。</li><li>眉毛和眼睛间距小。</li><li>弱化鼻子和嘴巴的刻画，但又比女生强一点，女生往往可以归结为一个点，而男生还是要刻画的。中老年男人鼻子甚至非常明显。</li><li>表情方面更偏向于酷、帅、冷、不耐烦的感觉。</li></ul><h2 id="日系年龄对面部画法的影响">日系年龄对面部画法的影响</h2><h3 id="男生-1">男生</h3><ul><li>年轻人的用线会相对圆润，越年长线条越锋利。</li><li>年长者的面部转折点会更向上部靠拢，五官也相对向上。</li><li>年长者的眼睛更窄，同样是更加锐利的感觉；而年轻人可以稍微眼睛大一点，高一点，圆一点。</li><li>年长者的眉毛会更尖锐有力。</li><li>年长者的下巴会更宽一些。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200613182651960.png" alt="image-20200613182651960" style="zoom:50%;"></p><h3 id="女生-1">女生</h3><ul><li>同样，年长者最重要的是脸部转折点上移。</li><li>年长者五官上移。</li><li>年长者眼睛变窄。</li><li>曲线更加有力。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200613184807549.png" alt="image-20200613184807549" style="zoom:50%;"></p><h3 id="总结">总结</h3><ul><li>在年龄变化的刻画上，更加着重地去刻画性别的特点，卡点和转折更重。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200613185226577.png" alt="image-20200613185226577" style="zoom:50%;"></p><ul><li>角色设定是一个把文字设定转为标签化的预设人体特征的过程。比如不同的眼型、不同的睫毛表现、不同的眉毛表现、不同的面部外轮廓等。</li></ul><h2 id="眉眼结构">眉眼结构</h2><p><img src="/2020/08/21/illustration-human-body/image-20200614093652461.png" alt="image-20200614093652461" style="zoom:50%;"></p><ul><li>男生眉毛粗，离眼睛近。画的时候要压低、往下放。</li><li>女生眉毛细，离眼睛远。画的时候要提高、往上走。女生眉毛很多时候更像是一条线。</li><li>眉眼的侧视图大致是正视图宽度的<span class="math inline">\(\frac{1}{2}\)</span>。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200614094501368.png" alt="image-20200614094501368" style="zoom:50%;"></p><h2 id="口鼻的画法">口鼻的画法</h2><ul><li>嘴角处要卡点（加重），无论嘴多么小，改卡点的都要卡。嘴的中间要相对画的较虚，若隐若现感觉。</li><li>与真实的人不同，日系画法中往往不刻画上嘴唇，但有时还是会把下嘴唇用一根线带过。</li><li>张开的嘴最好画出牙齿和舌头。</li><li>微笑最好画出左右斜向下箭头的形状。同理，微微噘嘴要画出左右斜向上箭头的形状。</li><li>侧面张开的嘴是梯形的。</li><li>鼻尖也需要卡点。</li><li>男性的鼻子长，嘴巴宽；女性的鼻子小、短，嘴巴窄。</li><li>虽然女生鼻子很多都是一个点，但这个点是有方向和轻重的。方向是面部朝向，而轻重则是点的起笔重（上方），落笔轻（下方）。</li></ul><h3 id="男性口鼻示例">男性口鼻示例</h3><p><img src="/2020/08/21/illustration-human-body/image-20200614150816122.png" alt="image-20200614150816122" style="zoom:50%;"></p><h3 id="女性口鼻示例">女性口鼻示例</h3><p><img src="/2020/08/21/illustration-human-body/image-20200614150929840.png" alt="image-20200614150929840" style="zoom:50%;"></p><h2 id="面部示例">面部示例</h2><h3 id="女生-2">女生</h3><p><img src="/2020/08/21/illustration-human-body/image-20200614204900408.png" alt="image-20200614204900408" style="zoom:50%;"></p><h3 id="男生-2">男生</h3><p><img src="/2020/08/21/illustration-human-body/image-20200615124628061.png" alt="image-20200615124628061" style="zoom:50%;"></p><h2 id="整体注意事项">整体注意事项</h2><ol type="1"><li>绘制步骤：找参考-&gt;打型-&gt;细化-&gt;调整</li><li>画的过程中要不断重复一下几个检查：<ul><li>头部的角度</li><li>角色性格相对应的五官形状</li><li>不断放大缩小去看位置和透视</li></ul></li></ol><h2 id="头发的绘制">头发的绘制</h2><ol type="1"><li><p>绘制的顺序：</p><ol type="1"><li>观察参考</li><li>确定发心（刘海中心点）或发中线位置</li><li>分块画草图</li><li>细化</li></ol></li><li><p>头发多样性的体现：</p><ol type="1"><li>粗细变换</li><li>不能过于对称（粗细、位置高低、头发的根数）</li><li>头发的方向（发旋、贴头弯曲、呆毛和不规则发）</li><li>动态变化</li></ol><p><img src="/2020/08/21/illustration-human-body/Screenshot%2520-%25202020-07-08%252023.03.30.png" alt="Screenshot - 2020-07-08 23.03.30" style="zoom:50%;"></p></li><li><p>头发的最大分块可分为：前发（刘海）、中发（一般到脸或肩的侧面头发）和后发。前中都可以没有，但后发一定有。</p></li><li><p>各种发型可以看做各种前中后发的组合。</p></li><li><p>后发不能紧贴头皮，要预留一定距离、体现蓬松感。</p></li><li><p>好的发型的绘制其剪影也是很好看的。</p></li><li><p>注意发梢在脖子周围行程一个圆，而不是直线。这点在短发情况下尤其明显。</p></li><li><p>颈部后面会有短小的头发。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709105324243.png" alt="image-20200709105324243" style="zoom:33%;"></p></li><li><p>发梢是头发的灵魂。不同种类的头发其发梢差距由其之大，比如直线型、尖锐型、折叠型等。</p></li><li><p>发梢部分，尤其是后发，尽量往里收，除非发型就是翘的。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709095740444.png" alt="image-20200709095740444" style="zoom:50%;"></p></li><li><p>长卷发会偏向成熟魅力的气质、长直发偏向冷酷高贵、短直发更多偏向清纯感和学生感、短卷发偏可爱活力。</p></li><li><p>卷发的基础是一撮头发，每一撮头发的基础是S型线，分清楚里外。先画大的S，最后画装饰线。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709102107872.png" alt="image-20200709102107872" style="zoom:50%;"></p></li><li><p>画长发，尤其是长卷发，最重要的是即使线条乱，也不能被线条带节奏，要搞清楚每一片头发的从属关系。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709102245876.png" alt="image-20200709102245876" style="zoom:33%;"></p></li></ol><h3 id="辫子">辫子</h3><ol type="1"><li><p>绘制辫子的时候要尤其注意线条的虚实，营造穿插感。加细节的时候一定要注意碎发也要顺着辫子的大体走势绘制。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709104609283.png" alt="image-20200709104609283" style="zoom:50%;"></p></li><li><p>马尾辫：年龄越大、马尾越低。马尾朝向斜向上的辫子要先上升后下降。</p></li><li><p>相较于披散发，辫子的翻转更多。</p></li><li><p>辫子中，总会有一些短的、碎的头发丝无法被扎到辫子主流中，他们构成了有效的细节。</p><figure><img src="/2020/08/21/illustration-human-body/image-20200709105414092.png" alt="image-20200709105414092"><figcaption aria-hidden="true">image-20200709105414092</figcaption></figure></li></ol><h3 id="男性头发">男性头发</h3><ol type="1"><li><p>男性头发如果是向外翘，显得有攻气；向内卷，显得温柔和受气。</p><p><img src="/2020/08/21/illustration-human-body/image-20200709121121242.png" alt="image-20200709121121242" style="zoom:50%;"></p></li><li><p>长发的男性角色，可以直接参照女性头发画法。</p></li></ol><h3 id="发型设计">发型设计</h3><ol type="1"><li><p>设计流程：</p><ol type="1"><li>分块</li><li>分组（使用不同的元素）</li><li>组合设计</li><li>不同的发型出来上がり～</li></ol><p><img src="/2020/08/21/illustration-human-body/image-20200709113650623.png" alt="image-20200709113650623" style="zoom:50%;"></p></li></ol><h2 id="发饰与其他装饰道具的绘制">发饰与其他装饰道具的绘制</h2><h3 id="蝴蝶结和蝴蝶结类似物分类">蝴蝶结和蝴蝶结类似物分类</h3><ol type="1"><li>固定形状的，即卖的时候就是打好的那种蝴蝶结。实心、较大，常用作头饰和领结。</li><li>手打的空心蝴蝶结。同样较大，除了头饰和领结，还可以用作围裙后面等。</li><li>飘带。由一根细长扁平的带子打出来的蝴蝶结。易随风飘动，且由于体积较小，需仔细控制。</li><li>小型点缀式的蝴蝶结。用作小型装饰物，出现在头上（发卡）、衣服上、口袋上等等。</li><li>以上的组合和变形。</li></ol><p><img src="/2020/08/21/illustration-human-body/image-20200719014529861.png" alt="image-20200719014529861" style="zoom: 60%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200719014444973.png" alt="image-20200719014444973" style="zoom:50%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200719014416132.png" alt="image-20200719014416132" style="zoom:50%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200719014348797.png" alt="image-20200719014348797" style="zoom:50%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200719014322885.png" alt="image-20200719014322885" style="zoom:68%;"></p><h3 id="发库">发库：</h3><ol type="1"><li>注意不能沿着头部轮廓画。</li><li>应该先找出来面部竖直轮廓线，然后保证发库与它垂直。</li><li>尤其注意透视现象，发库的末端结束于耳朵后面。</li></ol><figure><img src="/2020/08/21/illustration-human-body/image-20200719103005141.png" alt="image-20200719103005141"><figcaption aria-hidden="true">image-20200719103005141</figcaption></figure><h3 id="帽子">帽子</h3><ol type="1"><li><p>偏头顶（正）：棒球帽、礼帽、田园风</p></li><li><p>后脑勺：贝雷帽、太阳帽</p></li><li><p>棒球帽的画法：</p><ul><li>帽子的中线与头部中线平齐</li><li>注意帽子的外形</li><li>适当的给帽檐增加厚度</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200719104600298.png" alt="image-20200719104600298" style="zoom:33%;"></p></li><li><p>太阳帽的画法：</p><ul><li>位置偏脑后</li><li>*帽檐的形状</li><li>适当的装饰：缎带、蝴蝶结、鲜花等</li><li>画的时候即使后面的帽檐部分被脑袋遮挡，还是要先画出来、确保左右是连着的线。</li></ul></li></ol><h3 id="眼镜">眼镜</h3><ol type="1"><li><p>可爱风：镜框偏大、偏圆、可以抹除上眼镜框。这样的好处是可以避免眼镜挡住眼睛。</p><figure><img src="/2020/08/21/illustration-human-body/image-20200719110203120.png" alt="image-20200719110203120"><figcaption aria-hidden="true">image-20200719110203120</figcaption></figure></li><li><p>成熟风：眼型本来就偏小，眼镜也就偏窄。</p><figure><img src="/2020/08/21/illustration-human-body/image-20200719110140108.png" alt="image-20200719110140108"><figcaption aria-hidden="true">image-20200719110140108</figcaption></figure></li><li><p>性感风：无镜框、偏窄。</p><figure><img src="/2020/08/21/illustration-human-body/image-20200719110242408.png" alt="image-20200719110242408"><figcaption aria-hidden="true">image-20200719110242408</figcaption></figure></li><li><p>搞事派：镜片反光。。。如柯南开始推理、坂本大佬等。</p><figure><img src="/2020/08/21/illustration-human-body/image-20200719110419553.png" alt="image-20200719110419553"><figcaption aria-hidden="true">image-20200719110419553</figcaption></figure></li><li><p>画法要点：</p><ul><li>两个关键支点：鼻梁、耳朵</li><li>眼镜中线呈直线型：镜片透视保持一致</li><li>在画眼睛之前先画出来一个能包裹住两个镜片的矩形外框</li></ul></li></ol><h2 id="表情">表情</h2><h3 id="分类">分类</h3><ol type="1"><li><p>微笑（尴尬而不失礼貌的）</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113228668.png" alt="image-20200724113228668" style="zoom:50%;"></p><ol type="1"><li>口型就外八字点两点就好，注意卡点</li></ol></li><li><p>开口笑</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113253860.png" alt="image-20200724113253860" style="zoom:50%;"></p><ol type="1"><li>开口是梯形类似物</li><li>开口的大小要多尝试，因为不同人物适合的大小并不相同</li></ol></li><li><p>狂笑</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113443615.png" alt="image-20200724113443615" style="zoom:50%;"></p></li><li><p>闭眼笑</p><ol type="1"><li>眼睛即使闭上了也不是一条单调的线，而是有形状有细节的</li><li>那条线在之前上下眼睑中间</li></ol></li><li><p>怒（攻气、傲慢、盛气凌人）</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113521388.png" alt="image-20200724113521388" style="zoom:50%;"></p><ol type="1"><li>眼角上扬</li><li>眉毛外八字，也是上扬</li><li>嘴中间向上弯曲（即曲线向上凸）</li></ol></li><li><p>哀（受气、可怜）</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113548199.png" alt="image-20200724113548199" style="zoom:50%;"></p><ol type="1"><li>眼角向下撇</li><li>眉毛正八字</li><li>嘴会很小</li></ol></li><li><p>三无少女</p><p><img src="/2020/08/21/illustration-human-body/image-20200724113639102.png" alt="image-20200724113639102" style="zoom:50%;"></p><ol type="1"><li>无口无心无表情</li><li>嘴部很小很小，一般就是一个点</li><li>由于本来角色就不怎么有表情，所以眼睛和眉毛移一直都是放松状态</li></ol></li><li><p>邪魅一笑（色气）</p><p><img src="/2020/08/21/illustration-human-body/image-20200724111805983.png" alt="image-20200724111805983" style="zoom:42%;"></p><ol type="1"><li>面部红晕</li><li>特制笑容</li></ol></li><li><p>微微哭泣</p><p><img src="/2020/08/21/illustration-human-body/image-20200724112336854.png" alt="image-20200724112336854" style="zoom:33%;"></p><ol type="1"><li>委屈+抿嘴</li><li>少量眼角泪花</li><li>面颊红晕</li></ol></li><li><p>大哭</p><p><img src="/2020/08/21/illustration-human-body/image-20200724110336117.png" alt="image-20200724110336117" style="zoom:33%;"></p><ol type="1"><li>委屈加强版，眉毛继续八字</li><li>眼睛高度减小，下眼睑向上挤压</li><li>有眼泪元素</li><li>嘴张开，露牙齿</li><li>面颊红晕</li></ol></li><li><p>害羞</p><ol type="1"><li>大片的面部红晕，可以覆盖眼睛下面一长条包括鼻子位置</li></ol></li><li><p>傲娇</p><p><img src="/2020/08/21/illustration-human-body/image-20200724112224823.png" alt="image-20200724112224823" style="zoom:33%;"></p><ol type="1"><li>害羞+怒</li><li>可以用微汗点缀</li><li>小虎牙</li></ol></li><li><p>病娇（诡异）</p><p><img src="/2020/08/21/illustration-human-body/image-20200724110015171.png" alt="image-20200724110015171" style="zoom:33%;"></p><ol type="1"><li>眼球和上眼睑不是连着的，之间隔着一点眼白</li><li>嘴部有不对称元素</li><li>视角可偏仰视</li></ol></li></ol><h3 id="复杂表情构成">复杂表情构成</h3><ol type="1"><li>基础表情（不同程度的笑、哭、哀）</li><li>不同角度、身体转向（比如表现委屈、惹人怜爱、哭泣可以采用俯视，而表现惊悚和恶役则可使用仰视，元气可爱灵动可用各种侧视图等）</li><li>肢体动作（如手部动作）</li></ol><h3 id="漫画与插画的区别">漫画与插画的区别</h3><ul><li>相较于漫画，插画更偏向于整体人物的唯美性，表情一般不会很夸张，笑脸会占据大半。</li><li>漫画则相对束缚较小，可以画各种各样奇奇怪怪的夸张表情来凸显人物心情和推动剧情。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200724120144027.png" alt="image-20200724120144027" style="zoom:50%;"></p><h2 id="衣物的画法">衣物的画法</h2><h3 id="衣服的分类">衣服的分类</h3><p><strong>女生：</strong> 制服（JK）、女仆装、泳装、和风服饰（和服、浴衣）、LOLITA、idol（舞台装）</p><p><strong>男生：</strong> 衬衫、西服、卫衣、T恤</p><h3 id="画法要点">画法要点</h3><ol type="1"><li>注意与人物形体的关系，不要直接画衣服</li><li>受力情况</li><li>搭配、元素组合</li><li>设计（饰品）</li><li>参与构图</li></ol><h3 id="衣褶">衣褶</h3><h4 id="衣褶的成因">衣褶的成因</h4><ul><li>衣褶是受支撑点和地心引力的拉力影响而形成的褶皱。</li><li>如果没有内部结构就没有布纹衣褶的存在。</li><li>拉伸的力抹平衣褶，收缩的力形成衣褶。</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200726104737094.png" alt="image-20200726104737094" style="zoom:33%;"></p><h4 id="衣褶多的地方">衣褶多的地方</h4><ul><li>和内部支撑物之间空间较大的地方（宽松领口、古人衣服、宽松袖子、卫衣腰部）（相反：紧身衣、内衣、丝袜）</li><li>受到收缩外力明显的地方（腰带处、袖口、撸起来的袖子、关节内缩侧）（相反：关节绷紧侧）</li></ul><p><img src="/2020/08/21/illustration-human-body/Screenshot%2520-%25202020-07-26%252010.48.59.png" alt="Screenshot - 2020-07-26 10.48.59" style="zoom:50%;"></p><ul><li><p>人体中衣褶多的位置列举：</p><ul><li>领口</li><li>腋下</li><li>弯曲的胳膊腿的内侧</li><li>腰部</li><li>裆部（裤褶、裙褶）</li><li>裤脚</li><li>其他所有的有转折的地方</li><li>妹子胸部下部</li><li>腰带、绳带处</li><li>扣子处</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200726110314572.png" alt="image-20200726110314572" style="zoom:33%;"></p></li></ul><h3 id="褶皱理论">褶皱理论</h3><h4 id="一点支撑">一点支撑</h4><p>褶皱形状：放射状</p><figure><img src="/2020/08/21/illustration-human-body/image-20200726111529235.png" alt="image-20200726111529235"><figcaption aria-hidden="true">image-20200726111529235</figcaption></figure><h5 id="常见位置">常见位置</h5><ul><li>肩部</li><li>胸部中下侧</li><li>绳带</li><li>膝盖</li><li>手肘</li></ul><h4 id="两点支撑">两点支撑</h4><p>两点支撑可以看做两个一点支撑去掉内侧的线然后连起来。</p><p><img src="/2020/08/21/illustration-human-body/image-20200726112319409.png" alt="image-20200726112319409" style="zoom:40%;"></p><h5 id="常见位置-1">常见位置：</h5><ul><li>领口</li><li>腰部（胯）</li><li>宽松的XX（袖子、裤子、腰）</li></ul><p><img src="/2020/08/21/illustration-human-body/image-20200726112701126.png" alt="image-20200726112701126" style="zoom:33%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200726112803185.png" alt="image-20200726112803185" style="zoom:33%;"></p><h3 id="褶皱画法">褶皱画法</h3><h4 id="穿插">穿插</h4><p>简单的说，穿插就是A插到了B里面。由于褶皱弯曲，有时候会鼓起来突出一块，其效果就是下游的布料插进了上游的布料所形成的褶子里面。</p><p><img src="/2020/08/21/illustration-human-body/image-20200726120758174.png" alt="image-20200726120758174" style="zoom:50%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200726120727471.png" alt="image-20200726120727471" style="zoom:50%;"></p><p>由于视角的不同，让穿插呈现出2中绘制结构。</p><p>运用范围：衣袖、腰部、手肘（弯曲）、腿部（弯曲）。。。</p><h4 id="堆叠">堆叠</h4><p>堆叠，即把过长的部分堆起来。其形如阶梯，但各阶形异。论其走线，莫有完全平行者，目之所及，皆为交错向下；间有内陷，若小水洼。</p><p><img src="/2020/08/21/illustration-human-body/image-20200726141313049.png" alt="image-20200726141313049" style="zoom:50%;"></p><h4 id="裁缝线">裁缝线</h4><ul><li><p>作用：分摊拉力。假设如果没有裁缝线时只有几个大的衣褶纹路，此时加上一根裁缝线，则在原来的褶皱纹路基础上会出现许多细小的褶子，此即为所谓“分摊拉力”。</p><p><img src="/2020/08/21/illustration-human-body/image-20200730133455227.png" alt="image-20200730133455227" style="zoom:33%;"></p></li></ul><h4 id="雷区">雷区</h4><ol type="1"><li>褶皱太少，状似铁皮（布的质感）</li><li>衣褶过多，如着破布（不美观）</li><li>衣褶位置不对</li><li>线条太实或太虚</li></ol><h4 id="服装的质感软与硬">服装的质感：软与硬</h4><ul><li>软的衣服（如水手服、T恤）：<ul><li>褶皱相对偏多</li><li>褶皱规律性弱</li><li>褶皱线条柔软</li><li>布纹偏多</li></ul></li><li>硬的衣服（如西服、制服）：<ul><li>褶皱相对较少</li><li>褶皱规律相对较强</li><li>褶皱线条偏硬</li><li>布纹偏少</li></ul></li><li>实际区分方法：线条</li></ul><h3 id="衣物绘制注意事项">衣物绘制注意事项</h3><h4 id="厚度">厚度</h4><p>布是有厚度的，我们画布的时候要注意在垂下的边角处线与线的交汇处不要接死，留出一定的空隙以体现布料的厚度。</p><p><img src="/2020/08/21/illustration-human-body/image-20200730134510170.png" alt="image-20200730134510170" style="zoom:33%;"></p><h4 id="虚实">虚实</h4><p>我们在画衣褶的时候要注意线的虚实变化。交代清楚线的虚实，衣纹会更有活力，更生动。</p><p>外粗里细，主要褶皱粗、次要衣纹细。</p><h4 id="大小对比">大小对比</h4><p>我们画衣纹的时候也要注意有大小对比，不要把每一个纹路画的一样大，这样会显得呆板无趣。</p><p><img src="/2020/08/21/illustration-human-body/image-20200730135011004.png" alt="image-20200730135011004" style="zoom:33%;"></p><h3 id="jk制服的画法">JK制服的画法</h3><h4 id="分类-1">分类</h4><p><strong>上衣</strong>：水手服、衬衫+（∅、针织衫、西服、马甲、背心）</p><p><strong>裙子</strong>：格子裙、净色裙、净色+条纹</p><p><strong>领饰</strong>：领绳、领带、领结、领巾</p><h4 id="变化">变化</h4><p><strong>领口</strong>：圆领、尖领；长领口、短领口（向下延伸长度）；领子上带条纹、不带条纹</p><p><img src="/2020/08/21/illustration-human-body/image-20200811011515263.png" alt="image-20200811011515263" style="zoom:50%;"></p><p><img src="/2020/08/21/illustration-human-body/image-20200811011529584.png" alt="image-20200811011529584" style="zoom: 33%;"></p><p><strong>百褶裙堆叠方式</strong>：上下上下式、阶梯式</p><p><img src="/2020/08/21/illustration-human-body/image-20200811011403507.png" alt="image-20200811011403507" style="zoom: 33%;"></p><h4 id="特点">特点</h4><ol type="1"><li>水手服并不是收腰紧身的，而是使用了相对较硬的材质、在胸部以下更偏向线性延伸的。</li></ol><h4 id="示例">示例</h4><p><img src="/2020/08/21/illustration-human-body/image-20200811013940137.png" alt="image-20200811013940137" style="zoom:33%;"></p><h3 id="花边的画法">花边的画法</h3><p><img src="/2020/08/21/illustration-human-body/image-20200811112029242.png" alt="image-20200811112029242" style="zoom:33%;"></p><h3 id="服装的设计">服装的设计</h3><ol type="1"><li>元素的组合——主元素、辅元素。如以和风为主，但辅以花边和宽松裙摆等Lolita的设计。</li><li>变形：打破传统，求异。如传统的和服袖子都很长，领结都在领子下面，而求异的设计就可以把和服的袖子缩短、领结直接绑在颈部。</li><li>切割：衣服的虚与实。这里的实指的是有布料覆盖的地方，而虚则是指切开露出、没有布料覆盖的位置。虚的位置往往可以出现在肩部、腰部、腿部、胳膊等。</li><li>对比：<ol type="1"><li>体积的对比（大、小）</li><li>长度的对比（长、短）</li><li>娇小的角色穿宽松较大的服装（如埃罗芒阿老师、点兔、小埋）。宽大的衣服更显得角色本身的娇小。</li><li>性感的角色产布料较少的衣服，突出其身材</li></ol></li><li>剪影：整体上突出主元素</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;日系画风与现实的区别&quot;&gt;日系画风与现实的区别&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;真实的“&lt;strong&gt;三庭五眼&lt;/strong&gt;”：
&lt;ul&gt;
&lt;li&gt;三庭：&lt;strong&gt;d(发际线, 眉骨) = d(眉骨, 鼻底) = d(鼻底, 下巴)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;五眼：&lt;strong&gt;眼宽=眼间距=眼睛到面部轮廓的距离&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;眼睛的长度相对真实比例更长的，并不符合三庭五眼中的五眼比例。而是眼睛宽度和眼间距大体相同且较长，而眼睛两边到脸部轮廓的距离更短。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/2020/08/21/illustration-human-body/Screenshot%2520-%25202020-06-12%252022.46.34.png&quot; alt=&quot;Screenshot - 2020-06-12 22.46.34&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
    <category term="human-body" scheme="https://www.miracleyoo.com/tags/human-body/"/>
    
  </entry>
  
  <entry>
    <title>Cascade, Recursive, Residual and Dense 辨析</title>
    <link href="https://www.miracleyoo.com/2020/08/05/cascade-recursive/"/>
    <id>https://www.miracleyoo.com/2020/08/05/cascade-recursive/</id>
    <published>2020-08-06T00:00:45.000Z</published>
    <updated>2021-03-12T22:03:25.619Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cascade">Cascade</h2><p>相当于Progressive Optimization，每个阶段都会输出一个和最终结果形状相同的Matrix，如目标分布的图像、BBox等，然后下一个block的作用则是输入这个Matrix和前面提取的Features，输出Refined后的Matrix，该步骤不断重复。核心是逐步优化。</p><span id="more"></span><h2 id="recursive">Recursive</h2><p>相当于把一块Conv block重复了好多次，每次的权重是共享的。核心作用是节省内存和参数量、节省运算时间。同时它也含有时域特征。</p><h2 id="residual">Residual</h2><p>保留一条“信息高速公路”，使得前一轮的输出可以直接点加到经过了新一轮的Block卷积过后的结果上。核心作用是解决梯度消失问题，同时在网络的各层保留了不同层级的信息。变形有如Residual in Residual。</p><figure><img src="/2020/08/05/cascade-recursive/v2-862e1c2dcb24f10d264544190ad38142_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>ResNet网络的短路连接机制（其中+代表的是元素级相加操作）</p></blockquote><h2 id="dense">Dense</h2><p>每个Conv Block的输出会在Channel维上和后面所有Conv Block的输出Concate到一起。注意和Residual结构的区别，前者是直接逐点相加，而Dense则是并到Channel维度上。</p><figure><img src="/2020/08/05/cascade-recursive/v2-2cb01c1c9a217e56c72f4c24096fe3fe_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）</p></blockquote><figure><img src="/2020/08/05/cascade-recursive/v2-0a9db078f505b469973974aee9c27605_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>DenseNet的前向过程</p></blockquote><figure><img src="/2020/08/05/cascade-recursive/v2-c81da515c8fa9796601fde82e4d36f61_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>原图</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;cascade&quot;&gt;Cascade&lt;/h2&gt;
&lt;p&gt;相当于Progressive Optimization，每个阶段都会输出一个和最终结果形状相同的Matrix，如目标分布的图像、BBox等，然后下一个block的作用则是输入这个Matrix和前面提取的Features，输出Refined后的Matrix，该步骤不断重复。核心是逐步优化。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>IoU, AP, mAP等对比</title>
    <link href="https://www.miracleyoo.com/2020/07/14/iou-ap-map/"/>
    <id>https://www.miracleyoo.com/2020/07/14/iou-ap-map/</id>
    <published>2020-07-15T01:01:42.000Z</published>
    <updated>2021-03-12T22:15:03.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="iou-ap-map-map0.5-map0.5-0.95-average-map">IoU, AP, mAP, mAP@0.5, mAP@[0.5: 0.95], Average mAP</h1><h2 id="tldr">TL;DR</h2><ol type="1"><li>IoU：两个框框重叠部分面积/两个框框合并后的总面积​</li><li>AP：绘制Recall-Precision图，经过平滑后曲线的下面全面积。这个图的绘制方法是：按照每个预测结果的Confidence从上往下排列，先只取一个画出图上左上角第一个点，然后是只取前两个，直到取完。</li><li>mAP：AP是针对某一个类的，而mAP是把各个类的AP做一个平均。</li><li>mAP@0.5：当IoU阈值为0.5时的mAP。</li><li>mAP@[0.5:0.95]：COCO要求IoU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均。</li></ol><span id="more"></span><h2 id="查准率precision和查全率recall"><strong>查准率（Precision）和查全率（recall）</strong></h2><p>查准率（Precision）是指在所有预测为正例中真正例的比率，也即预测的准确性。</p><p>查全率（Recall）是指在所有正例中被正确预测的比率，也即预测正确的覆盖率。</p><p>一个样本模型预测按正确与否分类如下：</p><p>真正例： <span class="math inline">\(TP=True\space Positive\)</span></p><p>真反例： <span class="math inline">\(TN=True\space Negative\)</span></p><p>假正例：<span class="math inline">\(FP=False\space Positive\)</span></p><p>假反例：<span class="math inline">\(FN=False\space Negative\)</span></p><p><strong>则，查准率和查全率计算公式：</strong></p><p><strong>查准率</strong>：<span class="math inline">\(Precision=\frac{TP}{TP+FP}\)</span></p><p><strong>查全率</strong>：<span class="math inline">\(Recall=\frac{TP}{TP+FN}\)</span></p><h2 id="交并比iouintersection-over-union"><strong>交并比IoU(Intersection over union)</strong></h2><p>交并比IoU衡量的是两个区域的重叠程度，是两个区域重叠部分面积占二者总面积（重叠部分只计算一次）的比例。如下图，两个矩形框的IoU是交叉面积（中间图片红色部分）与合并面积（右图红色部分）面积之比。</p><p><img src="/2020/07/14/iou-ap-map/v2-11ed1bf4a882ee38f9ea1f73a2593472_1440w.jpg" alt="img">IoU计算重叠度</p><p>这里需要注意的是IoU=0.5，并不意味着每个框刚好有50%与另外一个框交叉部分，而是每个框大约有2/3被交叉。有点反直觉。</p><p>我当初看到IoU，非常疑惑为啥不按交叉面积占每个框的比例（IoA 也即Intersection over Area）取大值计算重叠度，更符合直觉。其实这种算法只反应小图片的被遮盖度，并不能反映互相之间的重叠度，一般情况下不可取。如下图，橙色部分较小，IoA很大，但对于蓝色部分，IoA就很小，只按橙色取IoA显然有失偏驳。</p><p><img src="/2020/07/14/iou-ap-map/v2-284022eaa7bbb8dd7b4f8488e0495fcd_1440w.jpg" alt="img">IoA计算重叠度</p><h2 id="单类别apaverage-precision的计算"><strong>单类别AP(Average Precision)的计算</strong></h2><p>物体检测中的每一个预测结果包含两部分，预测框（bounding box）和置信概率（Pc）。bounding box通常以矩形预测框的左上角和右下角的坐标表示，即x_min, y_min, x_max, y_max，如下图。置信概率Pc有两层意思，一是所预测bounding box的类别，二是这个类别的置信概率，如下图中的P_dog=0.88，代表预测绿色框为dog，并且置信概率为88%。</p><figure><img src="/2020/07/14/iou-ap-map/v2-8e1e070d1a59043a349eb1f921ea1e1c_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>那么，怎么才叫预测正确呢？显而易见的，必须满足两个条件：</p><ol type="1"><li>类别正确且置信度大于一定阀值（P_threshold）</li><li>预测框与真实框（ground truth）的IoU大于一定阀值（IoU_threshold）</li></ol><p>如下图，假如P_threshold=0.6，IoU_threshold=0.5，则绿色框预测正确，记为True Positive。</p><figure><img src="/2020/07/14/iou-ap-map/v2-fa34f541cee564e83435562297e768ab_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>而在衡量模型性能时，IoU_threshold先取一个定值，然后综合考虑各种P_threshold取值时的性能，进而得到一个与P_threshold选定无关的模型性能衡量标准。</p><p><strong>AP是计算单类别的模型平均准确度。</strong></p><p>假如目标类别为Dog，有5张照片，共包含7只Dog，也即GT（Ground Truth）数量为7，经模型预测，得到了Dog的10个预测结果，选定IoU_threshold=0.5，然后按confidence从高到低排序，如下图。其中，BB表示Bounding Box序号，GT=1表示有GT与所预测的Bounding Box的IoU&gt;=IoU_threshold，Bounding Box序号相同代表所对应的GT是同一个。</p><table><thead><tr class="header"><th>Rank</th><th>BB</th><th>confidence</th><th>GT</th></tr></thead><tbody><tr class="odd"><td>1</td><td>BB1</td><td>0.9</td><td>1</td></tr><tr class="even"><td>2</td><td>BB2</td><td>0.8</td><td>1</td></tr><tr class="odd"><td>3</td><td>BB1</td><td>0.8</td><td>1</td></tr><tr class="even"><td>4</td><td>BB3</td><td>0.5</td><td>0</td></tr><tr class="odd"><td>5</td><td>BB4</td><td>0.4</td><td>0</td></tr><tr class="even"><td>6</td><td>BB5</td><td>0.4</td><td>1</td></tr><tr class="odd"><td>7</td><td>BB6</td><td>0.3</td><td>0</td></tr><tr class="even"><td>8</td><td>BB7</td><td>0.2</td><td>0</td></tr><tr class="odd"><td>9</td><td>BB8</td><td>0.1</td><td>1</td></tr><tr class="even"><td>10</td><td>BB9</td><td>0.1</td><td>1</td></tr></tbody></table><p>因此，如果设置P_threshold=0，则有 TP=5 (BB1, BB2, BB5, BB8, BB9)，FP=5 (重复检测到的BB1也算FP)。除了表里检测到的5个GT以外，我们还有2个GT没被检测到，因此: FN = 2.</p><p>然后依次从上到下设定对应的rank为正反分界线，此rank之前（包含此rank）的预测为正，此rank之后的预测为反，然后计算对应的Precision和Recall：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">rank=1  precision=1.00 and recall=0.14</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=2  precision=1.00 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=3  precision=0.66 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=4  precision=0.50 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=5  precision=0.40 and recall=0.29</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=6  precision=0.50 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=7  precision=0.43 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=8  precision=0.38 and recall=0.43</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=9  precision=0.44 and recall=0.57</span><br><span class="line">--------------------------------------</span><br><span class="line">rank=10 precision=0.50 and recall=0.71</span><br><span class="line">--------------------------------------</span><br></pre></td></tr></table></figure><p>比如rank=4时，TP=2 (BB1, BB2)，则</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Precision=2/4=0.5，Recall=TP/GT=2/7=0.29</span><br></pre></td></tr></table></figure><p>可以看出，随着预测正反分割线的向下移动，Recall稳步变大，Precision整体减小，局部上下跳动，PR曲线如下图：</p><figure><img src="/2020/07/14/iou-ap-map/v2-0a899369aeab8824dc3dd3e4fe572cd3_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>AP(Average Precision)的计算基本等同于计算PR曲线下的面积，但略有不同。需要先将PR曲线平滑化。</p><p>方法是，查全率r对应的查准率p，取查全率大于等于r时最大的查准率p。即， <span class="math display">\[tex=p(r)=\max_{\tilde{r}\geq r}{p(\tilde{r})}\]</span> 平滑后的曲线如下图中的绿色曲线：</p><figure><img src="/2020/07/14/iou-ap-map/v2-666e46a022e32981aeb07b85958803cc_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>对于AP(Average Precision)的计算有两种方法：</p><p><strong>1. VOC2010之前的方法</strong></p><p>AP =（平滑后PR曲线上，Recall分别等于0，0.1，0.2，… , 1.0等11处Precision的平均值）。 <span class="math display">\[AP=\frac{1}{11}\sum_{r\subseteq\left\{0,0.1,..,1.0\right\}}{p\left(r\right)}\]</span></p><p>这里则有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AP = (1 + 1 + 1 + 0.5 + 0.5 + 0.5 + 0.5 + 0.5 + 0 + 0 + 0) / 11 = 0.5</span><br></pre></td></tr></table></figure><p><strong>2. VOC2010及以后的方法</strong></p><p>AP=平滑后PR曲线下包围的面积</p><p>这里则有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AP = (0.14-0) * 1 + (0.29-0.14) * 1 + (0.43-0.29) * 0.5 + (0.57-0.43) * 0.5 + (0.71-0.57) * 0.5 + (1-0.71) * 0 = 0.5</span><br></pre></td></tr></table></figure><p>这里两种方案得出的AP值相同，但通常是不同的。</p><p>需要注意的是上述AP的计算并没有显式设定<code>P_threshold</code>，而是通过从上到下依次指定每一个rank为正反分界线来变相的反映<code>P_threshold</code>不同取值。</p><h2 id="map的计算"><strong>mAP的计算</strong></h2><p>上述计算的AP只是针对dog这个类别，物体检测通常有多个类别，模型性能肯定是多个类别准度的综合度量。</p><p><strong>1. VOC数据集中的mAP</strong></p><p>VOC数据集中的mAP计算的是<code>IoU_threshold=0.5</code>时各个类别AP的均值。</p><p><strong>2. COCO数据集中的mAP</strong></p><p>检测是否正确有两个超参数，<code>P_threshold</code>和<code>IoU_threshold</code>。AP是固定了<code>IoU_threshold</code>，再综合考虑各个<code>P_threshold</code>下的模型平均准确度。</p><p>VOC认为<code>IoU_threshold</code>固定一个单值0.5即可，COCO则认为固定了<code>IoU_threshold</code>的取值，无法衡量<code>IoU_threshold</code>对模型性能的影响。</p><p>比如，</p><p>A模型在<code>IoU_threshold=0.5</code>时，<code>mAP=0.4</code>。</p><p>B模型在<code>IoU_threshold=0.7</code>时，<code>mAP</code>同样为0.4。</p><p>依据VOC的标准，AB模型的性能一样，但显然B模型的框更准，性能更优。</p><p>COCO在VOC标准的基础上，取<code>IoU_threshold=0.5，0.55， 0.6，… , 0.95</code>时各个mAP的均值。</p><h2 id="reference">Reference</h2><ol type="1"><li><a href="https://arleyzhang.github.io/articles/c521a01c/">目标检测评价标准-AP mAP</a></li><li><a href="http://blog.sina.com.cn/s/blog_9db078090102whzw.html">多标签图像分类任务的评价方法-mAP</a></li><li><a href="https://zhuanlan.zhihu.com/p/56961620">详解object detection中的mAP</a></li><li><a href="https://blog.csdn.net/luke_sanjayzzzhong/article/details/89851944">对于目标检测中mAP@0.5的理解</a></li><li><a href="https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean">What does the notation mAP@[.5:.95] mean?</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;iou-ap-map-map0.5-map0.5-0.95-average-map&quot;&gt;IoU, AP, mAP, mAP@0.5, mAP@[0.5: 0.95], Average mAP&lt;/h1&gt;
&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;IoU：两个框框重叠部分面积/两个框框合并后的总面积​&lt;/li&gt;
&lt;li&gt;AP：绘制Recall-Precision图，经过平滑后曲线的下面全面积。这个图的绘制方法是：按照每个预测结果的Confidence从上往下排列，先只取一个画出图上左上角第一个点，然后是只取前两个，直到取完。&lt;/li&gt;
&lt;li&gt;mAP：AP是针对某一个类的，而mAP是把各个类的AP做一个平均。&lt;/li&gt;
&lt;li&gt;mAP@0.5：当IoU阈值为0.5时的mAP。&lt;/li&gt;
&lt;li&gt;mAP@[0.5:0.95]：COCO要求IoU阈值在[0.5, 0.95]区间内每隔0.05取一次，这样就可以计算出10个类似于PASCAL的mAP，然后这10个还要再做平均。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch官方模型实现分析</title>
    <link href="https://www.miracleyoo.com/2020/07/11/pytorch-official-network-analysis/"/>
    <id>https://www.miracleyoo.com/2020/07/11/pytorch-official-network-analysis/</id>
    <published>2020-07-11T20:08:17.000Z</published>
    <updated>2021-03-12T22:08:45.249Z</updated>
    
    <content type="html"><![CDATA[<h2 id="resnet">Resnet</h2><h3 id="如何应对不同尺寸输入">如何应对不同尺寸输入</h3><p>在网络最后添加一个<code>AdaptiveAvgPool2d(output_size)</code>函数，它的作用是无论输入图片形状如何，最终都会转换为给定输出尺寸。而Resnet中，这个<code>output_size</code>被设置为了<code>(1,1)</code>，即无论输入的图片尺寸为多少，只要其大小足以扛得住网络前面的一系列pooling layers，到最后的输出尺寸大于等于<code>(1,1)</code>，其每个Channel就会被这一层压缩成一个点，即最后只会得到一个与Channel数目相等的向量。这个向量被送到了FC层。</p><span id="more"></span><h3 id="如何应对channel数目不合适问题">如何应对Channel数目不合适问题</h3><p>由于兼容了各种大小和尺寸的模型，所以有时难免会出现如Channel数目无法被4整除（BottleNeck Layer要求）的情况。这里，它使用了<code>1x1 conv</code>的方法。由于每个block的输出都要加到输入上，如果这个整除不了，结果就是Channel数目不匹配无法做Residual。这里的操作是，如果输出<code>Channel数*4 != 输入Channel数</code>，那么就直接让输入先用<code>1x1 conv</code>改变维度到<code>输出Channel数*4</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">    downsample = nn.Sequential(</span><br><span class="line">        conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">        norm_layer(planes * block.expansion),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="如何应对不同总layer数的resnet使用不同的layer-block的问题">如何应对不同总Layer数的Resnet使用不同的Layer Block的问题</h3><p>在Resnet中，resnet18, 34使用的是双层<code>3x3 conv</code>的<code>Basic Block</code>，而resnet50, 101, 152则使用的是<code>1x1conv -&gt; 3x3 conv -&gt; 1x1 conv</code>的结构。为了获得最大的兼容性，这里官方模型将<code>Basic Block</code>和<code>Bottleneck Block</code>分别定义为两个class，即子模块，然后对于不同尺寸的resnet分别输入不同的模块。</p><h3 id="为什么定义了conv3x3和conv1x1两个函数">为什么定义了conv3x3和conv1x1两个函数</h3><p>这两个函数看似画蛇添足多此一举，但实际上在我的理解中，他们避免了一些重复变量的输入，更直观地反映了该层的功能：<code>1x1</code>或<code>3x3</code>，也潜在地避免了一些错误，并优化了理解。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;resnet&quot;&gt;Resnet&lt;/h2&gt;
&lt;h3 id=&quot;如何应对不同尺寸输入&quot;&gt;如何应对不同尺寸输入&lt;/h3&gt;
&lt;p&gt;在网络最后添加一个&lt;code&gt;AdaptiveAvgPool2d(output_size)&lt;/code&gt;函数，它的作用是无论输入图片形状如何，最终都会转换为给定输出尺寸。而Resnet中，这个&lt;code&gt;output_size&lt;/code&gt;被设置为了&lt;code&gt;(1,1)&lt;/code&gt;，即无论输入的图片尺寸为多少，只要其大小足以扛得住网络前面的一系列pooling layers，到最后的输出尺寸大于等于&lt;code&gt;(1,1)&lt;/code&gt;，其每个Channel就会被这一层压缩成一个点，即最后只会得到一个与Channel数目相等的向量。这个向量被送到了FC层。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="pytorch" scheme="https://www.miracleyoo.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Attention 从分类到实现细节</title>
    <link href="https://www.miracleyoo.com/2020/07/06/attention-wiki/"/>
    <id>https://www.miracleyoo.com/2020/07/06/attention-wiki/</id>
    <published>2020-07-07T00:13:30.000Z</published>
    <updated>2021-03-12T22:16:17.487Z</updated>
    
    <content type="html"><![CDATA[<p>Attention的本质可以看做加权求和。</p><h2 id="attention-的-n-种类型">Attention 的 N 种类型</h2><figure><img src="/2020/07/06/attention-wiki/image-20200705230846450.png" alt="image-20200705230846450"><figcaption aria-hidden="true">image-20200705230846450</figcaption></figure><p>Attention 有很多种不同的类型：Soft Attention、Hard Attention、静态 Attention、动态 Attention、Self Attention 等等。上图为各种Attention的分类，下面是这些不同的 Attention 的解释。</p><p>由于这篇文章《<a href="https://zhuanlan.zhihu.com/p/35739040">Attention 用于 NLP 的一些小结</a>》已经总结的很好的，下面就直接引用了：</p><p>本节从计算区域、所用信息、结构层次和模型等方面对 Attention 的形式进行归类。</p><span id="more"></span><h3 id="计算区域"><strong>1. 计算区域</strong></h3><p>根据 Attention 的计算区域，可以分成以下几种：</p><p>1）<strong>Soft</strong> Attention，这是比较常见的 Attention 方式，对所有 key 求权重概率，每个 key 都有一个对应的权重，是一种全局的计算方式（也可以叫 Global Attention）。这种方式比较理性，参考了所有 key 的内容，再进行加权。但是计算量可能会比较大一些。</p><p>2）<strong>Hard</strong> Attention，这种方式是直接精准定位到某个 key，其余 key 就都不管了，相当于这个 key 的概率是 1，其余 key 的概率全部是 0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用 gumbel softmax 之类的）</p><p>3）<strong>Local</strong> Attention，这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用 Hard 方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用 Soft 方式来算 Attention。</p><h3 id="所用信息"><strong>2. 所用信息</strong></h3><p>假设我们要对一段原文计算 Attention，这里原文指的是我们要做 attention 的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息。</p><p>1）<strong>General</strong> Attention，这种方式利用到了外部信息，常用于需要构建两段文本关系的任务，query 一般包含了额外信息，根据外部 query 对原文进行对齐。</p><p>简单判定依据：<strong>计算Attention时，有没有用到除了被Attention向量以外的向量。</strong></p><p>比如在阅读理解任务中，需要构建问题和文章的关联，假设现在 baseline 是，对问题计算出一个问题向量 q，把这个 q 和所有的文章词向量拼接起来，输入到 LSTM中进行建模。那么在这个模型中，文章所有词向量共享同一个问题向量，现在我们想让文章每一步的词向量都有一个不同的问题向量，也就是，在每一步使用文章在该步下的词向量对问题来算 attention，这里问题属于原文，文章词向量就属于外部信息。</p><figure><img src="/2020/07/06/attention-wiki/v2-1e08348b226f4e2f08c89ae6e5d7fcda_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>2）<strong>Self</strong> Attention，这种方式只使用内部信息，key 和 value 以及 query 只和输入原文有关，在 self attention 中，key=value=query。既然没有外部信息，那么在原文中的每个词可以跟该句子中的所有词进行 Attention 计算，相当于寻找原文内部的关系。</p><p>还是举阅读理解任务的例子，上面的 baseline 中提到，对问题计算出一个向量 q，那么这里也可以用上 attention，只用问题自身的信息去做 attention，而不引入文章信息。</p><p>同样是在Encoder-Decoder模型中，它的实现方法是Encoder部分堆叠了两层。</p><p><img src="/2020/07/06/attention-wiki/v2-2c7e48868e98202b6c3af3a7aa4ea987_1440w.jpg" alt="img" style="zoom:50%;"></p><h3 id="结构层次"><strong>3. 结构层次</strong></h3><p>结构方面根据是否划分层次关系，分为单层 attention，多层 attention 和多头 attention：</p><p>1）单层 Attention，这是比较普遍的做法，用一个 query 对一段原文进行一次 attention。</p><p>2）多层 Attention，一般用于文本具有层次关系的模型，假设我们把一个 document 划分成多个句子，在第一层，我们分别对每个句子使用 attention 计算出一个句向量（也就是单层 attention）；在第二层，我们对所有句向量再做 attention 计算出一个文档向量（也是一个单层 attention），最后再用这个文档向量去做任务。</p><p>3）多头 Attention，这是 Attention is All You Need 中提到的 multi-head attention，用到了多个 query 对一段原文进行了多次 attention，每个 query 都关注到原文的不同部分，相当于重复做多次单层 attention：<span class="math display">\[head_i=Attention(q_i,K,V)\]</span></p><p>最后再把这些结果拼接起来：<span class="math inline">\(MultiHead(Q, K, V)=Concat(head_1,head_2,head_3,...,head_n)W^O\)</span></p><figure><img src="/2020/07/06/attention-wiki/v2-3cd76d3e0d8a20d87dfa586b56cc1ad3_1440w.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h3 id="模型方面"><strong>4. 模型方面</strong></h3><p>从模型上看，Attention 一般用在 CNN 和 LSTM 上，也可以直接进行纯 Attention 计算。</p><h4 id="cnnattention"><strong>1）CNN+Attention</strong></h4><p>CNN 的卷积操作可以提取重要特征，我觉得这也算是 Attention 的思想，但是 CNN 的卷积感受视野是局部的，需要通过叠加多层卷积区去扩大视野。另外，Max Pooling 直接提取数值最大的特征，也像是 hard attention 的思想，直接选中某个特征。</p><p>CNN 上加 Attention 可以加在这几方面：</p><ol type="a"><li><p>在卷积操作前做 attention，比如 Attention-Based BCNN-1，这个任务是文本蕴含任务需要处理两段文本，同时对两段输入的序列向量进行 attention，计算出特征向量，再拼接到原始向量中，作为卷积层的输入。</p></li><li><p>在卷积操作后做 attention，比如 Attention-Based BCNN-2，对两段文本的卷积层的输出做 attention，作为 pooling 层的输入。</p></li><li><p>在 pooling 层做 attention，代替 max pooling。比如 Attention pooling，首先我们用 LSTM 学到一个比较好的句向量，作为 query，然后用 CNN 先学习到一个特征矩阵作为 key，再用 query 对 key 产生权重，进行 attention，得到最后的句向量。</p></li></ol><h4 id="lstmattention"><strong>2）LSTM+Attention</strong></h4><p>LSTM 内部有 Gate 机制，其中 input gate 选择哪些当前信息进行输入，forget gate 选择遗忘哪些过去信息，我觉得这算是一定程度的 Attention 了，而且号称可以解决长期依赖问题，实际上 LSTM 需要一步一步去捕捉序列信息，在长文本上的表现是会随着 step 增加而慢慢衰减，难以保留全部的有用信息。</p><p>LSTM 通常需要得到一个向量，再去做任务，常用方式有：</p><ol type="a"><li><p>直接使用最后的 hidden state（可能会损失一定的前文信息，难以表达全文）</p></li><li><p>对所有 step 下的 hidden state 进行等权平均（对所有 step 一视同仁）。</p></li><li><p>Attention 机制，对所有 step 的 hidden state 进行加权，把注意力集中到整段文本中比较重要的 hidden state 信息。性能比前面两种要好一点，而方便可视化观察哪些 step 是重要的，但是要小心过拟合，而且也增加了计算量。</p></li></ol><h4 id="纯-attention"><strong>3）纯 Attention</strong></h4><p>Attention is all you need，没有用到 CNN/RNN，乍一听也是一股清流了，但是仔细一看，本质上还是一堆向量去计算 attention。本文提出了Transformer。Transformer 也可以视为一种自带Attention机制的RNN。它使用了问题、键、值三个向量，让权重的计算变得更加细致。</p><p>Transformer可以说是集近些年的研究之于大成。里面涉及到很多很多技术点，包括：</p><ul><li>Feed Forward Network</li><li>ResNet的思想</li><li>Positional Embedding 解决输入时序问题</li><li>Layer Normalization</li><li>Decoder中的Masked Self-Attention</li></ul><figure><img src="/2020/07/06/attention-wiki/transformer_resideual_layer_norm_3.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p><img src="/2020/07/06/attention-wiki/self-attention-output.png" alt="img" style="zoom:50%;"></p><p><img src="/2020/07/06/attention-wiki/self-attention-matrix-calculation.png" alt="img" style="zoom:50%;"></p><p><img src="/2020/07/06/attention-wiki/self-attention-matrix-calculation-2.png" alt="img" style="zoom:50%;"></p><h3 id="相似度计算方式"><strong>5. 相似度计算方式</strong></h3><p>在做 attention 的时候，我们需要计算 query 和某个 key 的分数（相似度），常用方法有：</p><p>1）点乘：最简单的方法， <span class="math inline">\(s(q,k)=q^Tk\)</span></p><p>2）矩阵相乘： <span class="math inline">\(s(q,k)=q^TWk\)</span></p><p>3）cos 相似度： <span class="math display">\[s(q,k)=\frac{q^T}{||q||·||k||}\]</span></p><p>4）串联方式：把 q 和 k 拼接起来， <span class="math display">\[s(q,k)=W[q;k]\]</span></p><p>5）用多层感知机也可以： <span class="math display">\[s(q,k)=v^T_atanh(Wq+Uk)\]</span></p><h2 id="encoder-decoder模型结构">Encoder-Decoder模型结构</h2><p>以机器翻译模型为例：<a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">原文</a></p><figure><img src="/2020/07/06/attention-wiki/seq2seq.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2020/07/06/attention-wiki/image-20200705230609732.png" alt="image-20200705230609732"><figcaption aria-hidden="true">image-20200705230609732</figcaption></figure><ul><li>Encoder的输入是句子中每个词对应的数字编号的序列，输出的是RNN每一个Cell的相应Output Vector（或是一次只输入一个词的编号，然后每次得到一个Output，最后拼成一个List）。其RNN的初始Hidden Layer是随机初始化（Random、全0）的。</li><li>Decoder的第一个输入是句子起始符<SOS>，数字编码可以为0，然后Hidden Layer使用Encoder的最后一个Hidden Layer Value初始化。每次只输入一个Input数字编号，然后下一次的Input或使用上次的预测结果，或是使用Output Label的相应值。前者被称作<em>Teacher Forcing</em>，后者是<em>Without Teacher Forcing</em>。</SOS></li><li>Attention是加在Decoder上的。Decoder 的input会先做embedding，之后么embedding和Hidden Layer参数Concate到一起，再过一个FC（<strong>过FC就相当于乘上了一个变换矩阵了</strong>）就得到了Attention。这个算出来的Attention再和Encoder的output做一个<code>torch.bmm</code>矩阵乘法（<span class="math inline">\([Batch\_Size*1*Length]*[Batch\_Size*Length*Hidden\_Size]\)</span>），得到一个向量([<span class="math inline">\(Batch\_Size*1*Hidden\_Size\)</span>])，这个数值即为该input过了Attention后的值。该值再和embedding后的值([<span class="math inline">\(Batch\_Size*1*Hidden\_Size\)</span>]) 使用Concate拼到一起，过一个FC，输出一个([<span class="math inline">\(Batch\_Size*1*Hidden\_Size\)</span>])的向量，它便是施加了Attention后的Input。这里本质上使用了前面提到的权值计算方法中的 <strong>串联方式</strong>。</li><li>简单说，decoder的input和hidden layer只是用来计算encoder各个cell的output的权重的。每个output有着hidden_size维度，他们最终按照attention作为加权求和。或是说：<strong>Decoder中每一个Cell，去Encoder中寻找最相关的记忆。</strong></li></ul><h2 id="网络结构">网络结构</h2><h3 id="embedding"><code>Embedding</code></h3><p><code>torch.nn.Embedding(*num_embeddings: int*, *embedding_dim: int*)</code></p><blockquote><p>To summarize <code>num_embeddings</code> is total number of unique elements in the vocabulary, and <code>embedding_dim</code> is the size of each embedded vector once passed through the embedding layer. Therefore, you can have a tensor of 10+ elements, as long as each element in the tensor is in the range <code>[0, 9]</code>, because you defined a vocabulary size of 10 elements.</p></blockquote><p>即第一个数<code>num_embedding</code>指的是你的输入中有多少可能的值，或者说语料库的大小；第二个数<code>embedding_dim</code>指的是给定一个input（一个digit），输出几个digit。</p><h3 id="bmm"><code>BMM</code></h3><p><code>torch.bmm(*input*, *mat2*, *deterministic=False*, *out=None*) → Tensor</code></p><blockquote><p>Performs a batch matrix-matrix product of matrices stored in <code>input</code> and <code>mat2</code>.</p><p><code>input</code> and <code>mat2</code> must be 3-D tensors each containing the same number of matrices.</p><p>If <code>input</code> is a <span class="math inline">\((b \times n \times m)\)</span> tensor, <code>mat2</code> is a <span class="math inline">\((b \times m \times p)\)</span> tensor, <code>out</code> will be a <span class="math inline">\((b \times n \times p)\)</span>tensor.</p></blockquote><p>简单说，这个函数的作用是在不动batch维度的情况下对其他维度执行矩阵乘法。</p><h2 id="self-attention-与-general-attention在实现上的区别">Self-Attention 与 General Attention在实现上的区别</h2><p>Self Attention 本质上是乘上一个和输入向量需要加Attention的维度等长的向量（<code>nn.Parameter(torch.Tensor(1, D), requires_grad=True)</code>），并做矩阵相乘。如输入是一个长为L的句子，句子中每个词的Embedding长度是D， batch为B，即(B, L, D)， 那么若是要对句子的长度维度做Attention，则需要乘一个shape为(B, D, 1)的向量，得到的Output shape为(B, L, 1)。之后还需做softmax，句子长度mask，结果除以单词个数保证所有weight加起来等于一，然后再使用这个output点乘input，即可得到和input shape相同，但被加了Attention后的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Pytorch self-attention layer code inspired from:</span></span><br><span class="line"><span class="string">    Link:</span></span><br><span class="line"><span class="string">        https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/4</span></span><br><span class="line"><span class="string">    Original Web Page:</span></span><br><span class="line"><span class="string">        https://www.kaggle.com/dannykliu/lstm-with-attention-clr-in-pytorch</span></span><br><span class="line"><span class="string">    Usage:</span></span><br><span class="line"><span class="string">        In __init__():</span></span><br><span class="line"><span class="string">            self.atten1 = Attention(hidden_dim*2, batch_first=True) # 2 is bidrectional</span></span><br><span class="line"><span class="string">        In forward():</span></span><br><span class="line"><span class="string">            x, _ = self.atten1(x, lengths)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, batch_first=<span class="literal">True</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.device = device</span><br><span class="line"></span><br><span class="line">        self.att_weights = nn.Parameter(</span><br><span class="line">            torch.Tensor(<span class="number">1</span>, hidden_size), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        stdv = <span class="number">1.0</span> / np.sqrt(self.hidden_size)</span><br><span class="line">        <span class="keyword">for</span> weight <span class="keyword">in</span> self.att_weights:</span><br><span class="line">            nn.init.uniform_(weight, -stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_mask</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, lengths</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            batch_size, max_len = inputs.size()[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            max_len, batch_size = inputs.size()[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply attention layer</span></span><br><span class="line">        weights = torch.bmm(inputs,</span><br><span class="line">                            self.att_weights  <span class="comment"># (1, hidden_size)</span></span><br><span class="line">                            .permute(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># (hidden_size, 1)</span></span><br><span class="line">                            .unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, hidden_size, 1)</span></span><br><span class="line">                            <span class="comment"># (batch_size, hidden_size, 1)</span></span><br><span class="line">                            .repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                            )</span><br><span class="line"></span><br><span class="line">        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create mask based on the sentence lengths</span></span><br><span class="line">        mask = torch.ones(attentions.size(), requires_grad=<span class="literal">True</span>).to(self.device)</span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(lengths):  <span class="comment"># skip the first sentence</span></span><br><span class="line">            <span class="keyword">if</span> l &lt; max_len:</span><br><span class="line">                mask[i, l:] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply mask and renormalize attention scores (weights)</span></span><br><span class="line">        masked = attentions * mask</span><br><span class="line">        _sums = masked.<span class="built_in">sum</span>(-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)  <span class="comment"># sums per row</span></span><br><span class="line"></span><br><span class="line">        attentions = masked.div(_sums)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply attention weights</span></span><br><span class="line">        weighted = torch.mul(</span><br><span class="line">            inputs, attentions.unsqueeze(-<span class="number">1</span>).expand_as(inputs))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the final fixed vector representations of the sentences</span></span><br><span class="line">        representations = weighted.<span class="built_in">sum</span>(<span class="number">1</span>).squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> representations, attentions</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>而General Attention的本质则是一个FC（即一个映射矩阵）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span></span><br><span class="line">        embedded = self.embedding(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        output = embedded</span><br><span class="line">        output, hidden = self.gru(output, hidden)</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnDecoderRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, output_size, dropout_p=<span class="number">0.1</span>, max_length=MAX_LENGTH</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AttnDecoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.dropout_p = dropout_p</span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, self.max_length)</span><br><span class="line">        self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">        self.dropout = nn.Dropout(self.dropout_p)</span><br><span class="line">        self.gru = nn.GRU(self.hidden_size, self.hidden_size)</span><br><span class="line">        self.out = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input: (1)</span></span><br><span class="line">    <span class="comment"># hidden: (1, D)</span></span><br><span class="line">    <span class="comment"># encoder_outputs: (L, D)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden, encoder_outputs</span>):</span></span><br><span class="line">        embedded = self.embedding(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># (1, 1, D)</span></span><br><span class="line">        embedded = self.dropout(embedded) <span class="comment"># (1, 1, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (1, 2D) -&gt; (1, L)</span></span><br><span class="line">        attn_weights = F.softmax(</span><br><span class="line">            self.attn(torch.cat((embedded[<span class="number">0</span>], hidden[<span class="number">0</span>]), <span class="number">1</span>)), dim=<span class="number">1</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (1, 1, L) x (1, L, D) -&gt; (1, 1, D)</span></span><br><span class="line">        attn_applied = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                                 encoder_outputs.unsqueeze(<span class="number">0</span>)) </span><br><span class="line"></span><br><span class="line">        output = torch.cat((embedded[<span class="number">0</span>], attn_applied[<span class="number">0</span>]), <span class="number">1</span>) <span class="comment"># (1, 2D)</span></span><br><span class="line">        output = self.attn_combine(output).unsqueeze(<span class="number">0</span>) <span class="comment"># (1, 1, D)</span></span><br><span class="line"></span><br><span class="line">        output = F.relu(output) <span class="comment"># (1, 1, D)</span></span><br><span class="line">        output, hidden = self.gru(output, hidden) <span class="comment"># (1, D)</span></span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output[<span class="number">0</span>]), dim=<span class="number">1</span>) <span class="comment"># (1, output_size)</span></span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size, device=device)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH</span>):</span></span><br><span class="line">    encoder_hidden = encoder.initHidden()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.zero_grad()</span><br><span class="line">    decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    input_length = input_tensor.size(<span class="number">0</span>)</span><br><span class="line">    target_length = target_tensor.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ei <span class="keyword">in</span> <span class="built_in">range</span>(input_length):</span><br><span class="line">        encoder_output, encoder_hidden = encoder(</span><br><span class="line">            input_tensor[ei], encoder_hidden)</span><br><span class="line">        encoder_outputs[ei] = encoder_output[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    decoder_input = torch.tensor([[SOS_token]], device=device)</span><br><span class="line"></span><br><span class="line">    decoder_hidden = encoder_hidden</span><br><span class="line"></span><br><span class="line">    use_teacher_forcing = <span class="literal">True</span> <span class="keyword">if</span> random.random() &lt; teacher_forcing_ratio <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">        <span class="comment"># Teacher forcing: Feed the target as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            decoder_input = target_tensor[di]  <span class="comment"># Teacher forcing</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Without teacher forcing: use its own predictions as the next input</span></span><br><span class="line">        <span class="keyword">for</span> di <span class="keyword">in</span> <span class="built_in">range</span>(target_length):</span><br><span class="line">            decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">                decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">            topv, topi = decoder_output.topk(<span class="number">1</span>)</span><br><span class="line">            decoder_input = topi.squeeze().detach()  <span class="comment"># detach from history as input</span></span><br><span class="line"></span><br><span class="line">            loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">            <span class="keyword">if</span> decoder_input.item() == EOS_token:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    encoder_optimizer.step()</span><br><span class="line">    decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item() / target_length</span><br></pre></td></tr></table></figure><h2 id="reference">Reference</h2><ol type="1"><li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION</a></li><li><a href="https://easyai.tech/ai-definition/attention/">Attention 机制 -- EasyAI</a></li><li><a href="https://zhuanlan.zhihu.com/p/35739040">Attention用于NLP的一些小结</a></li><li><a href="https://github.com/EvilPsyCHo/Attention-PyTorch">Attention-PyTorch</a></li><li><a href="https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq/blob/master/attentionRNN.py">Pytorch Batch Attention Seq-2-Seq</a></li><li><a href="https://zhuanlan.zhihu.com/p/47282410">Attention机制详解（二）——Self-Attention与Transformer</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li><li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li><li><a href="https://www.zhihu.com/question/68482809">知乎：目前主流的attention方法都有哪些？</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Attention的本质可以看做加权求和。&lt;/p&gt;
&lt;h2 id=&quot;attention-的-n-种类型&quot;&gt;Attention 的 N 种类型&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/2020/07/06/attention-wiki/image-20200705230846450.png&quot; alt=&quot;image-20200705230846450&quot;&gt;&lt;figcaption aria-hidden=&quot;true&quot;&gt;image-20200705230846450&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Attention 有很多种不同的类型：Soft Attention、Hard Attention、静态 Attention、动态 Attention、Self Attention 等等。上图为各种Attention的分类，下面是这些不同的 Attention 的解释。&lt;/p&gt;
&lt;p&gt;由于这篇文章《&lt;a href=&quot;https://zhuanlan.zhihu.com/p/35739040&quot;&gt;Attention 用于 NLP 的一些小结&lt;/a&gt;》已经总结的很好的，下面就直接引用了：&lt;/p&gt;
&lt;p&gt;本节从计算区域、所用信息、结构层次和模型等方面对 Attention 的形式进行归类。&lt;/p&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="attention" scheme="https://www.miracleyoo.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>ACG相关AI项目-论文-代码-数据-资源</title>
    <link href="https://www.miracleyoo.com/2020/07/06/acg-dl/"/>
    <id>https://www.miracleyoo.com/2020/07/06/acg-dl/</id>
    <published>2020-07-06T18:19:16.000Z</published>
    <updated>2021-03-12T22:19:50.299Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文">论文</h2><p>[<a href>Paper</a>] [<a href>Code</a>] [20XX]</p><h3 id="自动勾线-线稿">自动勾线-线稿</h3><ol type="1"><li><p>Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup [<a href="http://www.f.waseda.jp/hfs/SimoSerraSIGGRAPH2016.pdf">Paper</a>] [<a href="https://github.com/bobbens/sketch_simplification">Code</a>] [<a href="https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9">Blog</a>] [2015]</p><ul><li><p>早稻田大学15年经典论文。</p></li><li><p>粗糙手稿映射到精描线稿。</p></li><li><p>使用的是自建数据集，给定精致线稿让画师去照着描粗稿，这样避免了从线稿描到精稿时候添加和大改了很多线条。数据集没有开源......不过似乎作者给了训练好的权重。</p><span id="more"></span></li><li><p>We found that the standard approach, which we denote as direct dataset construction, of asking artists to draw a rough sketch and then produce a clean version of the sketch ended up with a lot of changes in the figure, i.e., output lines are greatly changed with respect to their input lines, or new lines are added in the output. This results in very noisy training data that does not perform well. In order to avoid this issue, we found that the best approach is the inverse dataset construction approach, that is, given a clean simplified sketch drawing, the artist is asked to make a rough version of that sketch.</p></li></ul><p><img src="/2020/07/06/acg-dl/image-20200705025359787.png" alt="image-20200705025359787" style="zoom:50%;"></p><p><img src="/2020/07/06/acg-dl/image-20200705025440435.png" alt="image-20200705025440435" style="zoom:50%;"></p></li></ol><h3 id="自动线稿上色">自动线稿上色</h3><ol type="1"><li><p>Scribbler: Controlling Deep Image Synthesis with Sketch and Color [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.pdf">Paper</a>] [<a href>Code</a>] [2017]</p></li><li><p>User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks [<a href="https://arxiv.org/pdf/1808.03240.pdf">Paper</a>] [<a href="https://github.com/orashi/AlacGAN">Code</a>] [2018]</p><ul><li>既支持直接从线稿转换为色稿，也支持用户交互画点生成色稿。</li><li>生成器和检测器的输入进行了创新。生成器的输入是线稿、用户色点图、还有一个线稿的特征Map。检测器的输入是两对：（真实色稿，线稿特征）与（生成色稿，线稿特征）。这样做的好处是避免了直接让判别器看到原始线稿，从而一定程度上避免了过拟合。</li><li>线稿是通过XDoG转换色稿得来。</li><li>Comment from <a href="https://www.reddit.com/r/AnimeResearch/comments/962ziu/userguided_deep_anime_line_art_colorization_with/">reddit</a>: I don't have it locally but it should be easy to make an 'illustration dataset' simply by using a tag like <a href="https://danbooru.donmai.us/posts?utf8=✓&amp;tags=-monochrome+&amp;ms=1"><code>-monochrome</code></a> to filter out any line-art. It's easy to make a line-art dataset because that's also a tag: <code>lineart</code>. Finally, you don't need to half-ass the pairs dataset with fake pairs using XDoG because you can just use <a href="https://danbooru.donmai.us/wiki_pages/21859">parent-child relationships</a> to find all sets of related images, and then extract all pairs of monochrome vs non-monochrome, which will usually give you sketch to completed image. Or there are tags just for this, like <a href="https://danbooru.donmai.us/posts?utf8=✓&amp;tags=colored+&amp;ms=1"><code>colored</code></a>, for human colorization of BW images (and you can again use the parent/child relationships to filter more).</li></ul><p><img src="/2020/07/06/acg-dl/image-20200705104853930.png" alt="image-20200705104853930" style="zoom:33%;"></p><p><img src="/2020/07/06/acg-dl/image-20200705104922846.png" alt="image-20200705104922846" style="zoom:33%;"></p><p><img src="/2020/07/06/acg-dl/image-20200705105004118.png" alt="image-20200705105004118" style="zoom:33%;"></p></li><li><p>Line Art Correlation Matching Network for Automatic Animation Colorization</p></li></ol><h3 id="照片转动漫">照片转动漫</h3><ol type="1"><li>CartoonGAN: Generative Adversarial Networks for Photo Cartoonization [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf">Paper</a>] [<a href>Code</a>] [2018]<ul><li>Unpaired image dataset training.</li><li>Two loss: In generator, a semantic loss defined as an ℓ1 sparse regularization in the <strong>high-level feature maps</strong> of the VGG network. In discriminator, an edge-promoting adversarial loss for preserving <strong>clear edges</strong></li><li>Built a new set of data which is animation images with edge blurred. Traditional GAN discriminator only discriminate photo from carton(or real from false), but this discriminator has three classes: Normal Photo, Real Carton Image, and Blurred edge carton image.</li></ul></li></ol><h3 id="动漫图片embedding">动漫图片Embedding</h3><ol type="1"><li>illustration2vec (i2v)。[<a href="https://www.gwern.net/docs/anime/2015-saito.pdf">Paper</a>] [<a href="https://github.com/rezoo/illustration2vec">Code</a>] [2015]<ul><li>使用了VGG网络将任意动漫图片压缩为一个长度为4096的Vector。</li><li>同样，由于训练时Y为标签，这里可以使用该网络为动漫图片打标。</li><li>训练图片来自于Danbooru和Safebooru两个网站。共计使用了1,287,596张图片和它们的metadata。</li><li>标签分为四类：<code>General tags</code>, <code>copyright tags</code>,<code>character tags</code>和<code>rating tags</code>。第一个是图片本身的特征，如“武器”，“微笑”，第二个是版权方，如“VOCALOID”；第三个是角色名字，如“hatsune miku”最后一个类别表示是18禁、擦边球还是全年龄图片。</li></ul></li></ol><h3 id="简笔画转照片">简笔画转照片</h3><ol type="1"><li><p>Deep Learning for Free-Hand Sketch: A Survey and A Toolbox [<a href="https://arxiv.org/pdf/2001.02600.pdf">Paper</a>] [<a href="https://github.com/PengBoXiangShang/torchsketch/">Code</a>] [2020]</p><figure><img src="/2020/07/06/acg-dl/image-20200705105458250.png" alt="image-20200705105458250"><figcaption aria-hidden="true">image-20200705105458250</figcaption></figure><figure><img src="/2020/07/06/acg-dl/image-20200705105522568.png" alt="image-20200705105522568"><figcaption aria-hidden="true">image-20200705105522568</figcaption></figure></li></ol><h3 id="其他gan">其他GAN</h3><ol type="1"><li><p>Style GAN [<a href="https://arxiv.org/pdf/1812.04948.pdf">Paper</a>] [<a href="https://github.com/NVlabs/stylegan">Code</a>] [2019]</p><ul><li>NVIDIA出品</li><li>它可以连续控制生成图片的各种独立参数！</li><li>之前的GAN都是Input使用一个随机噪声，而NVIDIA这篇这是在许多层中间都添加一波噪声。层数越靠后，这些噪声控制的特征就越细节。</li><li>gwern训练好的二次元StyleGAN模型：<a href="https://drive.google.com/file/d/1z8N_-xZW9AU45rHYGj1_tDHkIkbnMW-R/view">Link</a></li></ul><p><img src="/2020/07/06/acg-dl/image-20200705105910579.png" alt="image-20200705105910579" style="zoom:33%;"></p><p><img src="/2020/07/06/acg-dl/v2-3fffd4e8d8e16d7da045de536f6d0e95_1440w.jpg" alt="img" style="zoom:33%;"></p><p><img src="/2020/07/06/acg-dl/image-20200705105951584.png" alt="image-20200705105951584" style="zoom:33%;"></p></li><li><p><a href="https://github.com/zhangqianhui/AdversarialNetsPapers">AdversarialNetsPapers 各种GAN的Papers</a></p></li><li><p><a href="https://github.com/tjwei/GANotebooks">GANotebooks 各种GAN的Jupyter Notebook教程</a></p></li><li><p><a href="https://github.com/jayleicn/animeGAN">animeGAN A simple PyTorch Implementation of GAN, focusing on anime face drawing.</a></p></li></ol><h2 id="数据集">数据集</h2><h3 id="纯动漫图片">纯动漫图片</h3><ol type="1"><li><p>Danbooru2019. [<a href="https://www.gwern.net/Danbooru2019">Release</a>] [<a href="https://github.com/fire-eggs/Danbooru2019">Code</a>] [2019]</p><ul><li>Original or 3x512x512</li><li>~3TB or 295GB</li><li>3.69M or 2828400 images</li><li>108M tag instances (of 392k defined tags, ~29/image).</li><li>Covering Danbooru from 24 May 2005 through 31 December 2019 (final ID: #3,734,659).</li><li>Image files &amp; a JSON export of the metadata.</li></ul></li><li><p>Anime Face Dataset [<a href="https://www.kaggle.com/splcher/animefacedataset">Link</a>][2019]</p><ul><li>数据来源：<a href="www.getchu.com">Getchu</a></li><li>包含图片数目： 63,632</li><li>只包含脸部截取图片</li><li>大小：395.95MB</li><li>每张图片分辨率：90 * 90 ~ 120 * 120</li></ul><p><img src="/2020/07/06/acg-dl/test.jpg" alt="anime girls" style="zoom:50%;"></p></li></ol><h3 id="线稿-色稿对">线稿-色稿对</h3><ol type="1"><li>Danbooru Sketch Pair 128x [<a href="https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x">Link</a>] [2019]<ul><li>9.58GB</li><li>647K images. 323K sketch-color pairs</li><li>Image size: 3x128x128</li><li>有的是插画，有的是漫画。</li><li>博客：Sketch to Color Anime: An application of Conditional GAN. [<a href="https://medium.com/@raviranjankr165/sketch-to-color-anime-an-application-of-conditional-gan-e40f59c66281">Link</a>] [<a href="https://github.com/ravi-1654003/Sketch2Color-conditional-GAN">Code</a>] [2020]</li></ul></li></ol><h3 id="推荐与评价">推荐与评价</h3><ol type="1"><li><p>Anime Recommendations Database [<a href="https://www.kaggle.com/CooperUnion/anime-recommendations-database">Link</a>] [2016]</p><ul><li>数据来源于：<a href="https://myanimelist.net/">Link</a></li><li>大小：107.14MB</li><li>This data set contains information on user preference data from 73,516 users on 12,294 anime.</li></ul><p><img src="/2020/07/06/acg-dl/image-20200705024244220.png" alt="image-20200705024244220" style="zoom: 33%;"></p></li></ol><h2 id="博客">博客</h2><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/24767059">GAN学习指南：从原理入门到制作生成Demo</a></li><li><a href="https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09">GAN — What is Generative Adversary Networks GAN?</a></li><li><a href="https://www.leiphone.com/news/201709/i9qlcvWrpitOacjf.html">可能是近期最好玩的深度学习模型：CycleGAN的原理与实验详解</a></li><li><a href="https://makegirlsmoe.github.io/main/2017/08/14/news-english.html">输入各种参数生成动漫人物头像官方博客</a></li><li><a href="https://www.jiqizhixin.com/articles/2017-08-20-4">宅男的福音：用GAN自动生成二次元萌妹子</a></li><li><a href="https://qiita.com/rezoolab/items/5cc96b6d31153e0c86bc">Chainerを使ってコンピュータにイラストを描かせる</a></li><li><a href="https://www.jqr.com/article/000215">旋转吧！换装少女：一种可生成高分辨率全身动画的GAN</a></li><li><a href="https://www.jianshu.com/p/f31d9fc1d677">不要怂，就是GAN</a></li><li><a href="https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900">GAN — Some cool applications of GANs</a></li><li><a href="https://zhuanlan.zhihu.com/p/30532830">眼见已不为实，迄今最真实的GAN：Progressive Growing of GANs</a></li><li><a href="https://zhuanlan.zhihu.com/p/33752313">通俗理解生成对抗网络GAN</a></li><li><a href="https://zhuanlan.zhihu.com/p/27012520">从头开始GAN</a></li><li><a href="https://junyanz.github.io/CycleGAN/">Cycle GAN 作者官网</a></li><li><a href="https://www.jiqizhixin.com/articles/2018-04-17-5">如何从零开始构建深度学习项目？这里有一份详细的教程</a></li><li><a href="https://zhuanlan.zhihu.com/p/27145954">带你理解CycleGAN，并用TensorFlow轻松实现</a></li></ol><h2 id="其他资源">其他资源</h2><h3 id="文字转语音">文字转语音</h3><ol type="1"><li><a href="http://sinsy.jp/">Sinsy</a>: アップロードされた楽譜(MusicXML)に基づいて任意の歌声を生成するHMM/DNN歌声合成システム，Sinsy（しぃんしぃ）です．<ul><li>支持日语、中文、英语。</li><li>输入特定格式的乐谱，输出相当不错的唱词音频文件。</li><li><a href="https://pypi.org/project/sinsy-cli/">sinsy-cli</a>: 使用命令行调用Sinsy进行合成。安装：<code>pip install sinsy-cli</code></li><li>介绍博客：<a href="http://blog.pcedev.com/2016/02/18/hands-sinsy-solution-song-vocal-synthesis-using-open-source-software/">Hands on Sinsy, a free software solution for song vocal synthesis</a></li></ul></li><li><a href="https://www.animenewsnetwork.com/interest/2013-10-22/vocaloducer-automatically-creates-songs-from-lyrics">Vocaloducer</a>。</li></ol><h3 id="动漫人脸检测切割">动漫人脸检测切割</h3><ol type="1"><li><a href="https://github.com/nagadomi/lbpcascade_animeface">自动化动漫人物脸部切割保存</a> <img src="/2020/07/06/acg-dl/43184241-ed3f1af8-9022-11e8-8800-468b002c73d9.png" alt="result"></li></ol><h3 id="头像生成">头像生成</h3><ol type="1"><li><a href="https://make.girls.moe/">输入各种参数生成动漫人物头像</a></li><li><a href="https://github.com/Aixile/chainer-cyclegan">Chainer-CycleGAN 动漫人物头发转银色</a></li></ol><h3 id="图像超分辨率">图像超分辨率</h3><ol type="1"><li><a href="http://waifu2x.udp.jp">Waifu2x 动漫图片无损放大</a></li></ol><h3 id="自动线稿上色-1">自动线稿上色</h3><ol type="1"><li><p><a href="https://github.com/pfnet/PaintsChainer">PaintsChainer</a>: Paints Chainer is a line drawing colorizer using chainer. Using CNN, you can colorize your sketch semi-automatically .</p><ul><li>作者提供了直接搭建网站server 的代码。</li><li>这里是搭建好的<a href="http://paintschainer.preferred.tech/">站点</a>。</li><li>该网站也提供草图或照片提取线稿功能。</li></ul><figure><img src="/2020/07/06/acg-dl/sample.png" alt="image"><figcaption aria-hidden="true">image</figcaption></figure></li></ol><h3 id="照片画风迁移">照片画风迁移</h3><ol type="1"><li><p><a href="https://deepart.io/">Repaint your picture in the style of your favorite artist</a></p><figure><img src="/2020/07/06/acg-dl/image-20200705105719731.png" alt="image-20200705105719731"><figcaption aria-hidden="true">image-20200705105719731</figcaption></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;论文&quot;&gt;论文&lt;/h2&gt;
&lt;p&gt;[&lt;a href&gt;Paper&lt;/a&gt;] [&lt;a href&gt;Code&lt;/a&gt;] [20XX]&lt;/p&gt;
&lt;h3 id=&quot;自动勾线-线稿&quot;&gt;自动勾线-线稿&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup [&lt;a href=&quot;http://www.f.waseda.jp/hfs/SimoSerraSIGGRAPH2016.pdf&quot;&gt;Paper&lt;/a&gt;] [&lt;a href=&quot;https://github.com/bobbens/sketch_simplification&quot;&gt;Code&lt;/a&gt;] [&lt;a href=&quot;https://medium.com/coinmonks/simplifying-rough-sketches-using-deep-learning-c404459622b9&quot;&gt;Blog&lt;/a&gt;] [2015]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;早稻田大学15年经典论文。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;粗糙手稿映射到精描线稿。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用的是自建数据集，给定精致线稿让画师去照着描粗稿，这样避免了从线稿描到精稿时候添加和大改了很多线条。数据集没有开源......不过似乎作者给了训练好的权重。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="acg" scheme="https://www.miracleyoo.com/tags/acg/"/>
    
    <category term="anime" scheme="https://www.miracleyoo.com/tags/anime/"/>
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>PS混合模式原理与示例</title>
    <link href="https://www.miracleyoo.com/2020/05/24/ps-blend/"/>
    <id>https://www.miracleyoo.com/2020/05/24/ps-blend/</id>
    <published>2020-05-24T22:39:00.000Z</published>
    <updated>2021-03-12T22:39:53.306Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模式官方介绍">模式官方介绍</h2><p><strong>正片叠底</strong></p><p>查看<strong>每个通道</strong>中的颜色信息，并将基色与混合色进行正片叠底。结果色总是较暗的颜色。任何颜色与黑色正片叠底产生黑色。任何颜色与白色正片叠底保持不变。当您用黑色或白色以外的颜色绘画时，绘画工具绘制的连续描边产生逐渐变暗的颜色。这与使用多个标记笔在图像上绘图的效果相似。</p><p><img src="/2020/05/24/ps-blend/image-20200519225738854.png" alt="image-20200519225738854" style="zoom:33%;"></p><p>如图，重点是<strong>分通道</strong>，所有的模式混合都是基于通道的。即红色的底色，往上加蓝色和绿色结果都是黑色。因为RGB通道相互间的暗色都是黑色。当然，它们与黑色叠加也是黑色的。</p><span id="more"></span><p><strong>滤色</strong></p><p>查看每个通道的颜色信息，并将混合色的互补色与基色进行正片叠底。结果色总是较亮的颜色。用黑色过滤时颜色保持不变。用白色过滤将产生白色。此效果类似于多个摄影幻灯片在彼此之上投影。</p><p><strong>正常</strong></p><p>编辑或绘制每个像素，使其成为结果色。这是默认模式。（在处理位图图像或索引颜色图像时，“正常”模式也称为<em>阈值</em>。）</p><p><strong>溶解</strong></p><p>编辑或绘制每个像素，使其成为结果色。但是，根据任何像素位置的不透明度，结果色由基色或混合色的像素随机替换。</p><p><strong>背后</strong></p><p>仅在图层的透明部分编辑或绘画。此模式仅在取消选择了“锁定透明区域”的图层中使用，类似于在透明纸的透明区域背面绘画。</p><p><strong>清除</strong></p><p>编辑或绘制每个像素，使其透明。此模式可用于形状工具（当选定填充区域 <img src="/2020/05/24/ps-blend/P_VectorShapeFilled_Md_N.png" alt="img"> 时）、油漆桶工具 <img src="/2020/05/24/ps-blend/P_Fill_Lg_N.png" alt="img">、画笔工具 <img src="/2020/05/24/ps-blend/P_Brush_Lg_N.png" alt="img">、铅笔工具 <img src="/2020/05/24/ps-blend/P_Draw_Lg_N.png" alt="img">、“填充”命令和“描边”命令。您必须位于取消选择了“锁定透明区域”的图层中才能使用此模式。</p><p><strong>变暗</strong></p><p>查看每个通道中的颜色信息，并选择基色或混合色中较暗的颜色作为结果色。将替换比混合色亮的像素，而比混合色暗的像素保持不变。</p><p><strong>颜色加深</strong></p><p>查看每个通道中的颜色信息，并通过增加二者之间的对比度使基色变暗以反映出混合色。与白色混合后不产生变化。</p><p><strong>线性加深</strong></p><p>查看每个通道中的颜色信息，并通过减小亮度使基色变暗以反映混合色。与白色混合后不产生变化。</p><p><strong>变亮</strong></p><p>查看每个通道中的颜色信息，并选择基色或混合色中较亮的颜色作为结果色。比混合色暗的像素被替换，比混合色亮的像素保持不变。</p><p><strong>颜色减淡</strong></p><p>查看每个通道中的颜色信息，并通过减小二者之间的对比度使基色变亮以反映出混合色。与黑色混合则不发生变化。</p><p><strong>线性减淡（添加）</strong></p><p>查看每个通道中的颜色信息，并通过增加亮度使基色变亮以反映混合色。与黑色混合则不发生变化。</p><p><strong>叠加</strong></p><p>对颜色进行正片叠底或过滤，具体取决于基色。图案或颜色在现有像素上叠加，同时保留基色的明暗对比。不替换基色，但基色与混合色相混以反映原色的亮度或暗度。</p><p><strong>柔光</strong></p><p>使颜色变暗或变亮，具体取决于混合色。此效果与发散的聚光灯照在图像上相似。如果混合色（光源）比 50% 灰色亮，则图像变亮，就像被减淡了一样。如果混合色（光源）比 50% 灰色暗，则图像变暗，就像被加深了一样。使用纯黑色或纯白色上色，可以产生明显变暗或变亮的区域，但不能生成纯黑色或纯白色。</p><p><strong>强光</strong></p><p>对颜色进行正片叠底或过滤，具体取决于混合色。此效果与耀眼的聚光灯照在图像上相似。如果混合色（光源）比 50% 灰色亮，则图像变亮，就像过滤后的效果。这对于向图像添加高光非常有用。如果混合色（光源）比 50% 灰色暗，则图像变暗，就像正片叠底后的效果。这对于向图像添加阴影非常有用。用纯黑色或纯白色上色会产生纯黑色或纯白色。</p><p><strong>亮光</strong></p><p>通过增加或减小对比度来加深或减淡颜色，具体取决于混合色。如果混合色（光源）比 50% 灰色亮，则通过减小对比度使图像变亮。如果混合色比 50% 灰色暗，则通过增加对比度使图像变暗。</p><p><strong>线性光</strong></p><p>通过减小或增加亮度来加深或减淡颜色，具体取决于混合色。如果混合色（光源）比 50% 灰色亮，则通过增加亮度使图像变亮。如果混合色比 50% 灰色暗，则通过减小亮度使图像变暗。</p><p><strong>点光</strong></p><p>根据混合色替换颜色。如果混合色（光源）比 50% 灰色亮，则替换比混合色暗的像素，而不改变比混合色亮的像素。如果混合色比 50% 灰色暗，则替换比混合色亮的像素，而比混合色暗的像素保持不变。这对于向图像添加特殊效果非常有用。</p><p><strong>实色混合</strong></p><p>将混合颜色的红色、绿色和蓝色通道值添加到基色的 RGB 值。如果通道的结果总和大于或等于 255，则值为 255；如果小于 255，则值为 0。因此，所有混合像素的红色、绿色和蓝色通道值要么是 0，要么是 255。此模式会将所有像素更改为主要的加色（红色、绿色或蓝色）、白色或黑色。</p><p><strong>差值</strong></p><p>查看每个通道中的颜色信息，并从基色中减去混合色，或从混合色中减去基色，具体取决于哪一个颜色的亮度值更大。与白色混合将反转基色值；与黑色混合则不产生变化。</p><p><strong>排除</strong></p><p>创建一种与“差值”模式相似但对比度更低的效果。与白色混合将反转基色值。与黑色混合则不发生变化。</p><p><strong>减去</strong></p><p>查看每个通道中的颜色信息，并从基色中减去混合色。在 8 位和 16 位图像中，任何生成的负片值都会剪切为零。</p><p><strong>划分</strong></p><p>查看每个通道中的颜色信息，并从基色中划分混合色。</p><p><strong>色相</strong></p><p>用基色的明亮度和饱和度以及混合色的色相创建结果色。</p><p><strong>饱和度</strong></p><p>用基色的明亮度和色相以及混合色的饱和度创建结果色。在无 (0) 饱和度（灰度）区域上用此模式绘画不会产生任何变化。</p><p><strong>颜色</strong></p><p>用基色的明亮度以及混合色的色相和饱和度创建结果色。这样可以保留图像中的灰阶，并且对于给单色图像上色和给彩色图像着色都会非常有用。</p><p><strong>明度</strong></p><p>用基色的色相和饱和度以及混合色的明亮度创建结果色。此模式创建与“颜色”模式相反的效果。</p><p><strong>浅色</strong></p><p>比较混合色和基色的所有通道值的总和并显示值较大的颜色。“浅色”不会生成第三种颜色（可以通过“变亮”混合获得），因为它将从基色和混合色中选取最大的通道值来创建结果色。</p><p><strong>深色</strong></p><p>比较混合色和基色的所有通道值的总和并显示值较小的颜色。“深色”不会生成第三种颜色（可以通过“变暗”混合获得），因为它将从基色和混合色中选取最小的通道值来创建结果色。</p><h2 id="公式">公式</h2><figure><img src="/2020/05/24/ps-blend/365c8ef854384f00b13c69e81eb2d41b.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>注释：</p><p>1.混合模式的数学计算公式，另外还介绍了不透明度。</p><p>2.这些公式仅适用于RGB图像，对于Lab颜色图像而言，这些公式将不再适用。</p><p>3.在公式中——</p><p>A 代表下面图层的颜色值，也称为基色；</p><p>B 代表上面图层的颜色值，也成为混合色；</p><p>C 代表混合图层的颜色值，也称为结果色；</p><p>D 表示该层的透明度。</p><h2 id="详细举例推理">详细举例推理</h2><p>我们先建立两个图层：</p><p>当前选中层（Active Layer）为<strong>A层</strong>，或称“<strong>混合色</strong> blend color”</p><p>下层（Background Layer）为<strong>B层</strong>，或称“<strong>基色</strong> base color”，两者混合得到“<strong>结果色</strong> Result color”</p><figure><img src="/2020/05/24/ps-blend/v2-49a088eef8b2851abc6bb5809dff666c_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="混合模式的基本原理"><strong>混合模式的基本原理</strong></h2><p>取A层任意一个像素a [R1, G1, B1]，与B层对应位置的像素b [R2, G2, B2] 进行数学运算，得到c [R3, G3, B3]</p><ul><li>R1 某种运算 R2 = R3</li><li>G1 某种运算 G2 = G3</li><li>B1 某种运算 B2 = B3</li></ul><p>A、B两层<strong>所有像素</strong>都独立进行同样的运算，即得到混合后的结果C，即新的A层（注意虽然A层缩略图没有变，但直方图已经变了），而B保持不变</p><p>举一个简单例子，PS默认的图层混合模式是“正常”，在不透明度为100%时，</p><ul><li>R1 正常 R2 = R1</li><li>G1 正常 G2 = G1</li><li>B1 正常 B2 = B1</li></ul><p>也就是说，“正常”模式下，A层在B层之上，看到的只有A层，看不到B层</p><h2 id="混合模式的分类"><strong>混合模式的分类</strong></h2><p>一般系：正常、溶解</p><p>变暗系：变暗、深色、正片叠底、颜色加深、线性加深</p><p>变亮系：变亮、浅色、滤色、颜色减淡、线性减淡</p><p>对比系：叠加、强光、柔光、亮光、线性光、点光、实色混合</p><ul><li><ul><li>该组特点是让亮的更亮，暗的更暗，每一个“对比系”的混合模式都可看作“变暗系”和“变亮系”的结合，如“叠加”是对较暗的像素进行“正片叠底”，对较亮的像素进行“滤色”</li></ul></li></ul><p>负片系：差值、排除</p><p>相消系：减去、划分</p><p>HSL系：色相、饱和度、颜色、明度</p><h2 id="混合模式彼此之间的联系"><strong>混合模式彼此之间的联系</strong></h2><ol type="1"><li>对于大多数混合模式，图层“不透明度”和“填充度”对混合效果影响是一样的，也就是说，60%的不透明度，与60%的填充度，得到的效果一样；但以下8种混合模式是例外<ul><li>颜色加深、颜色减淡</li><li>线性加深、线性减淡</li><li>亮光、线性光、实色混合</li><li>差值</li></ul></li><li>以下5组混合模式是相反对应关系（从后面的公式就可以看出）<ul><li>变暗 - 变亮</li><li>深色 - 浅色</li><li>正片叠底 - 滤色</li><li>颜色加深 - 颜色减淡</li><li>线性加深 - 线性减淡</li></ul></li><li>具有“互逆”关系的混合模式有两组，举例，A层在上，对A“叠加”；B层在上，对B“强光”：两种情况得到的效果是一样的<ul><li>叠加 <img src="/2020/05/24/ps-blend/equation.svg" alt="公式"> 强光</li><li>颜色 <img src="/2020/05/24/ps-blend/equation" alt="公式"> 明度</li></ul></li></ol><h2 id="每一种混合模式的运算方法"><strong>每一种混合模式的运算方法</strong></h2><p>当通道位深度为8位时，R、G、B三通道数值范围在0到255，我们除以255，得到0~1范围内的数值</p><p>我们用下图所示的两个100像素的图层做实验</p><p>A层有一个颜色， [R, G, B]值分别是 [102, 153, 204]，即[0.4, 0.6, 0.8]</p><p>B层有两个颜色，[0, 51, 102] 和 [255, 204, 153]，即 [0, 0.2, 0.4] 和 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-49a088eef8b2851abc6bb5809dff666c_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>选中A层，调节混合模式</p><h2 id="i-正常系">I 正常系</h2><blockquote><p><strong>正常 Normal</strong></p></blockquote><p>设不透明度为n</p><p>**<em>c = n*</em>*<em>*a + (1-n)*</em>***b***</p><p>当不透明度为100%</p><p><strong><em>c = a</em></strong></p><blockquote><p><strong>溶解 Dissolve</strong></p></blockquote><p>对每个像素而言，其结果色是基色或混合色的随机值，取决于其“不透明度”：“不透明度”高时，更多像素取自当前层；低时，更多像素取自背景层</p><h2 id="ii-变暗和变亮系">II 变暗和变亮系</h2><blockquote><p><strong>变暗 Darken</strong></p></blockquote><p><em><strong>c = min(a,b)</strong> 逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.2, 0.4] 颜色2 [0.4, 0.6, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-6cd964357c823d7704b86e4aa486b02b_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到变暗的效果</p><blockquote><p><strong>变亮</strong> <strong>Lighten</strong></p></blockquote><p><em><strong>c = max(a,b)*</strong> </em>逐通道进行运算*</p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.6, 0.8] 颜色2 [1, 0.8, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-c520d5c07364cbed9ce6b58faf3475c7_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到变亮的效果</p><blockquote><p><strong>深色 Darker Color</strong></p></blockquote><p><em><strong>c = min(a, b)</strong> 三通道整体进行运算，不会产生新的颜色</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.2, 0.4] 颜色2 [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-c520d5c07364cbed9ce6b58faf3475c7_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>该混合模式不会产生新的颜色，注意与“变暗”的区别</p><blockquote><p><strong>浅色 Lighter Color</strong></p></blockquote><p><strong><em>c = max(a, b)</em></strong> *三通道整体进行运算**，不会产生新的颜色*</p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.6, 0.8] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-0734309c8a2c9ef18543c4a3e89b765a_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>该混合模式不会产生新的颜色，注意与“变亮”的区别</p><blockquote><p><strong>正片叠底 Multiply</strong></p></blockquote><p><em><strong>c = a*b</strong> 逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.12, 0.32] 颜色2 [0.4, 0.48, 0.48]</p><figure><img src="/2020/05/24/ps-blend/v2-ef14152ab9c1ed17b69901e2b2f8fcaa_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>常用的加深模式，用于产生阴影、去除白色和其他浅色。如同将所有的图层都叠在一起，上方一束光投下来到屏幕上；“颜色加深”和“线性加深”比“正片叠底”效果更为强烈</p><p>任何颜色和黑色混合结果都是黑的，任何颜色跟白色混合结果都是原来的颜色</p><p>“正片叠底”即所谓“减色”或“CMYK”模式，现实中相当于用染料、水彩笔绘画效果，如图所示</p><figure><img src="/2020/05/24/ps-blend/v2-8f278da13cf831f924d4638abaf8f4b3_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p><strong>滤色 Screen</strong></p></blockquote><p><strong><em>c = 1 − (1−a)*(1−b)</em></strong><em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.68, 0.88] 颜色2 [1, 0.92, 0.92]</p><figure><img src="/2020/05/24/ps-blend/v2-3aa136ef959a58faeb862b5b51bdeae3_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>常用的减淡方法，产生发光效果。如同将所有图层分开摆放，各有一束光通过各图层，汇聚在一块屏幕上</p><p>基色或混合色为白色时，结果会是白色；任何颜色和黑色混合，结果仍原来的颜色</p><p>“滤色”即所谓“加色”模式，现实中相当于发光体发光的叠加效果，如图所示</p><figure><img src="/2020/05/24/ps-blend/v2-9e577680c0ad20d6d63b1864bfe391ec_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p><strong>颜色加深 Color Burn</strong></p></blockquote><p><strong><em>c = 1 − (1−b)/a</em></strong> <em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0, 0.25] 颜色2 [1, 0.67, 0.5]</p><figure><img src="/2020/05/24/ps-blend/v2-a9617aa81abdba4047fea16108777627_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到的效果比“正片叠底”更深，中调更高饱和，高光减弱</p><p>任何颜色跟白色（B=1）混合结果都是白色</p><blockquote><p><strong>颜色减淡 Color Dodge</strong></p></blockquote><p><strong><em>c = b / (1−a)</em></strong><em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.5, 1] 颜色2 [1, 1, 1]</p><figure><img src="/2020/05/24/ps-blend/v2-3ef666006d4285e56f9c3545cda3c15d_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到的效果比“滤色”更亮，色彩对比更加强烈，中调更高饱和，高光增强</p><p>任何颜色跟黑色（B=0）混合结果都是黑色</p><blockquote><p><strong>线性加深 Linear Burn</strong></p></blockquote><p><strong><em>c = a + b − 1</em></strong> <em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0, 0.2] 颜色2 [0.4, 0.4, 0.4]</p><figure><img src="/2020/05/24/ps-blend/v2-14a6ce57b48355c20df849b97d1e20f7_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到的效果比“颜色加深”更深，比颜色加深饱和度低</p><p>任何颜色跟白色（B=1）混合结果都是原来的颜色</p><blockquote><p><strong>线性减淡 Linear Dodge</strong></p></blockquote><p><strong><em>c = a+b</em></strong> <em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.8, 1] 颜色2 [1, 1, 1]</p><figure><img src="/2020/05/24/ps-blend/v2-1f74cf80b6e5cd9087bc3b6c944a904d_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>得到的效果比“颜色减淡”更亮，但对比稍弱</p><p>任何颜色跟黑色（B=0）混合结果都是原来的颜色</p><h2 id="iii-对比系">III 对比系</h2><p>该组特点是让亮的更亮，暗的更暗，每一个“对比系”的混合模式都可看作“变暗系”和“变亮系”的结合，如“叠加”是对较暗的像素进行“正片叠底”，对较亮的像素进行“滤色”</p><blockquote><p><strong>叠加</strong> <strong>Overlay</strong></p></blockquote><p><em>逐通道进行运算</em></p><p>*<strong>若 b &lt;= 0.5: c = 2ab*</strong></p><p>*<strong>若 b &gt; 0.5: c = 1 - 2(1-a)(1-b)*</strong></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.24, 0.64] 颜色2[1, 0.84, 0.84]</p><figure><img src="/2020/05/24/ps-blend/v2-24fbd55f5e94ed2b164d839d9d8ee8ab_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>类似于 “正片叠底”+“滤色”的结合，但效果更柔和</p><p>A层在上，对A“叠加”；B层在上，对B“强光”：两种情况得到的效果是一样的</p><blockquote><p><strong>强光</strong> <strong>Hard Light</strong></p></blockquote><p><em>逐通道进行运算</em></p><p>*<strong>若 a &lt;= 0.5: c = 2ab*</strong></p><p>*<strong>若 a &gt; 0.5: c = 1 - 2(1-a)(1-b)*</strong></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.36, 0.76] 颜色2 [0.8, 0.84, 0.84]</p><figure><img src="/2020/05/24/ps-blend/v2-f0b8f5ffd678a80c6712583b6598db8e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>类似于 “正片叠底”+“滤色”的结合，但效果更柔和</p><p>A层在上，对A“叠加”；B层在上，对B“强光”：两种情况得到的效果是一样的</p><blockquote><p><strong>柔光</strong> <strong>Soft Light</strong></p></blockquote><p><em>逐通道进行运算</em></p><p>*<strong>若 a &lt;= 0.5, c = 2ab + <img src="/2020/05/24/ps-blend/equation" alt="公式">(1-2a)*</strong></p><p>*<strong>若 a &gt; 0.5, c = 2b(1-a) + <img src="/2020/05/24/ps-blend/equation.svg" alt="公式">(2a-1)*</strong></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.25, 0.54] 颜色2 [1, 0.82, 0.71]</p><figure><img src="/2020/05/24/ps-blend/v2-78b198e1c2fe8f6c060753318fb849ee_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>效果类似于“叠加”，但效果更柔和，有透明的光线和阴影</p><p>类似于 “浅色”+“深色”的结合</p><blockquote><p><strong>亮光</strong> <strong>Vivid Light</strong></p></blockquote><p><em>逐通道进行运算，近似公式</em></p><p>*<strong>若 a &lt;= 0.5, c = 1 + (b-1)/2a*</strong></p><p>*<strong>若 a &gt; 0.5, c = b / 2(1-a)*</strong></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.25, 1] 颜色2 [1, 0.996, 1]</p><figure><img src="/2020/05/24/ps-blend/v2-177f7f9a060ef7ccf2938c6ef9120350_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>类似于“实色混合”，但效果通常更加剧烈</p><p>类似于 “颜色减淡”+“颜色加深”的结合</p><blockquote><p><strong>线性光</strong> <strong>Linear Light</strong></p></blockquote><p><em><strong>c = b + 2a -1*</strong> </em>逐通道进行运算，近似公式*</p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.4, 0.996] 颜色2 [0.8, 0.996, 1]</p><figure><img src="/2020/05/24/ps-blend/v2-3e6f6616a8249f8e78a8553edc728d63_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>类似于“亮光”，但效果通常更加剧烈</p><p>类似于 “线性加深”+“线性减淡”的结果</p><blockquote><p><strong>点光</strong> <strong>Pin Light</strong></p></blockquote><p><em>逐通道进行运算</em></p><p>*<strong>若 a &gt; 0.5, c = max ( 2(a-0.5) , b )*</strong></p><p>*<strong>若 a &lt;= 0.5, c = min ( 2a , b )*</strong></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.2, 0.6] 颜色2 [0.796, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-fa86d21780209cf5b58e727fd33ef18f_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>该混合模式较为强烈，容易形成色块色斑和噪点</p><p>类似于 “变亮”+“变暗”的结合</p><blockquote><p><strong>实色混合</strong> <strong>Hard Mix</strong></p></blockquote><p><em><strong>若 a+b &gt;= 1, c = 1; 否则 c=0*</strong> </em>逐通道进行运算*</p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0, 1] 颜色2 [1, 1, 1]</p><figure><img src="/2020/05/24/ps-blend/v2-ec227746d961f540cc8f70547614bffa_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>结果只有8个颜色：R、G、B、C、M、Y 、K、White</p><p>如果“填充度”不为100，结果色会多于8</p><h2 id="iv-负片系">IV 负片系</h2><blockquote><p><strong>差值 Difference</strong></p></blockquote><p><em><strong>c = |b - a|</strong> 逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.4, 0.4] 颜色2 [0.6, 0.2, 0.2]</p><figure><img src="/2020/05/24/ps-blend/v2-abd81b87e3014ae47f3deaea3958a5a1_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>会有一定负片效果</p><blockquote><p><strong>排除 Exclusion</strong></p></blockquote><p><strong><em>c = a + b - 2ab</em></strong> <em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0.4, 0.56, 0.56] 颜色2 [0.6, 0.44, 0.44]</p><figure><img src="/2020/05/24/ps-blend/v2-097f5e2b9c9646d69f6a07ce66783459_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>会有一定负片效果</p><h2 id="v-相消系">V 相消系</h2><blockquote><p><strong>减去Subtract</strong></p></blockquote><p><strong><em>c = b−a</em></strong><em>逐通道进行运算</em></p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0, 0] 颜色2 [0.6, 0.2, 0]</p><figure><img src="/2020/05/24/ps-blend/v2-897968ab0abc87cf22279786051c46ab_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>变深效果</p><p>常结合“应用图像”，用于高低频法调色</p><blockquote><p><strong>划分Divide</strong></p></blockquote><p><em><strong>c = b/a*</strong> </em>逐通道进行运算*</p><p>B: 颜色1 [0, 0.2, 0.4] 颜色2 [1, 0.8, 0.6]</p><figure><img src="/2020/05/24/ps-blend/v2-9d6d881e09d25ea68d31b4aeb0a42584_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>A: [0.4, 0.6, 0.8]</p><figure><img src="/2020/05/24/ps-blend/v2-a97a64c70b7213f2179afa2c4969eb7e_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>C: 颜色1 [0, 0.33, 0.5] 颜色2 [1, 1, 0.75]</p><figure><img src="/2020/05/24/ps-blend/v2-fc94414ecd4e5b95cb0ea5912858589c_720w.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>变亮效果</p><h2 id="汇总在一张a4纸上">汇总在一张A4纸上</h2><figure><img src="/2020/05/24/ps-blend/v2-9777e7ee9933926f8cd3a50d905a592f_r.jpg" alt="preview"><figcaption aria-hidden="true">preview</figcaption></figure><h2 id="参考文献">参考文献</h2><ul><li>Adobe中文官网</li><li><a href="https://zhuanlan.zhihu.com/p/94081709">Photoshop图层混合模式详解</a></li><li>另外一篇知乎文章，由于写的时候忘了，现在实在找不到了。。要是引用部分原作者来了请找我claim。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;模式官方介绍&quot;&gt;模式官方介绍&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;正片叠底&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;查看&lt;strong&gt;每个通道&lt;/strong&gt;中的颜色信息，并将基色与混合色进行正片叠底。结果色总是较暗的颜色。任何颜色与黑色正片叠底产生黑色。任何颜色与白色正片叠底保持不变。当您用黑色或白色以外的颜色绘画时，绘画工具绘制的连续描边产生逐渐变暗的颜色。这与使用多个标记笔在图像上绘图的效果相似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/05/24/ps-blend/image-20200519225738854.png&quot; alt=&quot;image-20200519225738854&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;如图，重点是&lt;strong&gt;分通道&lt;/strong&gt;，所有的模式混合都是基于通道的。即红色的底色，往上加蓝色和绿色结果都是黑色。因为RGB通道相互间的暗色都是黑色。当然，它们与黑色叠加也是黑色的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
    <category term="ps" scheme="https://www.miracleyoo.com/tags/ps/"/>
    
  </entry>
  
  <entry>
    <title>PS笔刷探究</title>
    <link href="https://www.miracleyoo.com/2020/05/20/ps-brush/"/>
    <id>https://www.miracleyoo.com/2020/05/20/ps-brush/</id>
    <published>2020-05-21T05:29:01.000Z</published>
    <updated>2021-03-12T22:45:09.885Z</updated>
    
    <content type="html"><![CDATA[<h2 id="选项研究">选项研究</h2><p>由于多次搜索和研究画笔设置和笔刷的制作方法，但均感觉网上所讲一知半解居多，这次特意找时间全部整理了一遍。难理解的、可能并不常用的放在前面着重讲，容易理解的、常用的则放在后面。</p><h3 id="杂色noise">杂色(Noise)</h3><p>为个别画笔笔尖增加额外的随机性。当应用于柔画笔笔尖（包含灰度值的画笔笔尖）时，此选项最有效。</p><span id="more"></span><h3 id="湿边wet-edges">湿边(Wet Edges)</h3><p>沿画笔描边的边缘增大油彩量，从而创建水彩效果。</p><h3 id="喷枪建立airbrushbuild-up">喷枪/建立(Airbrush/Build-up)</h3><p>将渐变色调应用于图像，同时模拟传统的喷枪技术。“画笔”面板中的“喷枪”选项与选项栏中的“喷枪”选项相对应。喷枪的本质特点是：当你的笔/鼠标在一个点按住停留的时候，颜色会不断加深，影响区域也越来越大。如果没有勾选喷枪/建立选项，按多久都是一样的颜色深度。从某种意义上讲，喷枪效果在“笔压”之外加入了一个“停留时间”维度。</p><h3 id="平滑smoothing">平滑(Smoothing)</h3><p>在画笔描边中生成更平滑的曲线。当使用光笔进行快速绘画时，此选项最有效；但是它在描边渲染中可能会导致轻微的滞后。</p><h3 id="保护纹理protect-texture">保护纹理(Protect Texture)</h3><p>将相同图案和缩放比例应用于具有纹理的所有画笔预设。选择此选项后，在使用多个纹理画笔笔尖绘画时，可以模拟出一致的画布纹理。</p><h3 id="画笔笔势选项">画笔笔势选项</h3><p>画笔笔势选项可让您获得类似光笔的效果，并可让您控制画笔的角度和位置。</p><p>这里说的光笔是指一种在板绘时可以感知笔的倾斜角度、旋转的笔。<strong>画笔笔势</strong>选项固定且剥夺了板绘笔旋转、倾斜、压力的能力，直接设定为了定值。</p><p><strong>倾斜 X</strong></p><p>确定画笔从左向右倾斜的角度。</p><p><strong>倾斜 Y</strong></p><p>确定画笔从前向后倾斜的角度。</p><p><strong>旋转</strong></p><p>确定硬毛刷的旋转角度。</p><p><strong>压力</strong></p><p>确定应用于画布上画笔的压力。</p><p>启用“覆盖”选项以维护静态画笔笔势。</p><h2 id="画笔散布">画笔散布</h2><p>“画笔散布”可确定描边中笔迹的数目和位置。</p><p>简单的说，开启了该选项后，画的时候笔迹并不会严格遵循你笔的下笔位置，而是会在x和y轴上有一个或多或少的偏移。</p><figure><img src="/2020/05/20/ps-brush/pa_14.png" alt="Photoshop 画笔散布"><figcaption aria-hidden="true">Photoshop 画笔散布</figcaption></figure><p>无散布的画笔描边（左图）和有散布的画笔描边（右图）</p><p><strong>散布和控制</strong></p><p>指定画笔笔迹在描边中的分布方式。当选择“两轴”时，画笔笔迹按径向分布。当取消选择“两轴”时，画笔笔迹垂直于描边路径分布。</p><p>要指定散布的最大百分比，请输入一个值。若要指定希望如何控制画笔笔迹的散布变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的散布变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长将画笔笔迹的散布从最大散布渐隐到无散布。</p><p><strong>钢笔压力、钢笔斜度、光笔轮、旋转</strong></p><p>依据钢笔压力、钢笔斜度、钢笔拇指轮位置或钢笔的旋转来改变画笔笔迹的散布。</p><p><strong>计数</strong></p><p>指定在每个间距间隔应用的画笔笔迹数量。</p><h3 id="颜色动态画笔选项">颜色动态画笔选项</h3><p>颜色动态决定描边路线中油彩颜色的变化方式。</p><figure><img src="/2020/05/20/ps-brush/pa_18.png" alt="Photoshop 有颜色动态和无颜色动态的画笔描边"><figcaption aria-hidden="true">Photoshop 有颜色动态和无颜色动态的画笔描边</figcaption></figure><p>无颜色动态的画笔描边（左图）和有颜色动态的画笔描边（右图）</p><p>它也可以用来制作如下的图像：</p><p><img src="/2020/05/20/ps-brush/image-20200519182011143.png" alt="image-20200519182011143" style="zoom:33%;"></p><p><strong>每笔尖应用</strong></p><p>指定为描边中每个不同的笔尖图章更改颜色。</p><p>如果取消选中，则在每个描边开始时即进行动态更改。您便可以更改不同描边的颜色，而不是对每个描边内部更改颜色。</p><p><strong>前景/背景抖动和控制</strong></p><p>指定前景色和背景色之间的油彩变化方式。</p><p>要指定油彩颜色可以改变的百分比，请键入数字或使用滑块来输入值。若要指定希望如何控制画笔笔迹的颜色变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的颜色变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长在前景色和背景色之间改变油彩颜色。</p><p><strong>钢笔压力、钢笔斜度、光笔轮、旋转</strong></p><p>依据钢笔压力、钢笔斜度、钢笔拇指轮位置或钢笔的旋转来改变前景色和背景色之间的油彩颜色。</p><p><strong>色相抖动</strong></p><p>指定描边中油彩色相可以改变的百分比。键入数字，或者使用滑块来输入值。较低的值在改变色相的同时保持接近前景色的色相. 较高的值增大色相间的差异。</p><p><strong>饱和度抖动</strong></p><p>指定描边中油彩饱和度可以改变的百分比。键入数字，或者使用滑块来输入值。较低的值在改变饱和度的同时保持接近前景色的饱和度。较高的值增大饱和度级别之间的差异。</p><p><strong>亮度抖动</strong></p><p>指定描边中油彩亮度可以改变的百分比。键入数字，或者使用滑块来输入值。较低的值在改变亮度的同时保持接近前景色的亮度。较高的值增大亮度级别之间的差异。</p><p><strong>纯度</strong></p><p>增大或减小颜色的饱和度。键入一个数字，或者使用滑块输入一个介于 -100 和 100 之间的百分比。如果该值为 -100，则颜色将完全去色；如果该值为 100，则颜色将完全饱和。</p><h2 id="双重画笔">双重画笔</h2><p>双重画笔组合两个笔尖来创建画笔笔迹。将在<strong>主画笔的画笔描边内</strong>应用第二个画笔纹理；仅绘制<strong>两个画笔描边的交叉区域</strong>。在“画笔”面板的“画笔笔尖形状”部分中设置主要笔尖的选项。从“画笔”面板的“双重画笔”部分选择另一个画笔笔尖，然后设置以下任意选项。</p><figure><img src="/2020/05/20/ps-brush/pa_16.png" alt="Photoshop 主、辅助、双重画笔描边"><figcaption aria-hidden="true">Photoshop 主、辅助、双重画笔描边</figcaption></figure><p><strong>A.</strong> 主画笔笔尖描边（尖角 55）。 <strong>B.</strong> 辅助画笔笔尖描边（草）。 <strong>C.</strong> 双重画笔描边（使用两者）。</p><p><strong>模式</strong></p><p>选择从主要笔尖和双重笔尖组合画笔笔迹时要使用的混合模式。（请参阅<a href="https://helpx.adobe.com/cn/photoshop/using/blending-modes.html">混合模式</a>。）</p><p><strong>直径</strong></p><p>控制双笔尖的大小。以像素为单位输入值，或者单击“使用取样大小”来使用画笔笔尖的原始直径。(只有当画笔笔尖形状是通过采集图像中的像素样本创建的时，“使用取样大小”选项才可用。)</p><p><strong>间距</strong></p><p>控制描边中双笔尖画笔笔迹之间的距离。若要更改间距，请键入数字，或使用滑块输入笔尖直径的百分比。</p><p><strong>散布</strong></p><p>指定描边中双笔尖画笔笔迹的分布方式。当选中“两轴”时，双笔尖画笔笔迹按径向分布。当取消选择“两轴”时，双笔尖画笔笔迹垂直于描边路径分布。若要指定散布的最大百分比，请键入数字或使用滑块来输入值。</p><p><strong>计数</strong></p><p>指定在每个间距间隔应用的双笔尖画笔笔迹的数量。键入数字，或者使用滑块来输入值。</p><h2 id="纹理画笔选项">纹理画笔选项</h2><p>纹理画笔利用<strong>图案使描边</strong>看起来像是在带纹理的画布上绘制的一样。</p><figure><img src="/2020/05/20/ps-brush/pa_15.png" alt="Photoshop 有纹理和无纹理的画笔描边"><figcaption aria-hidden="true">Photoshop 有纹理和无纹理的画笔描边</figcaption></figure><p>无纹理的画笔描边（左图）和有纹理的画笔描边（右图）</p><p>单击图案样本，然后从弹出式面板中选择图案。设置下面的一个或多个选项：</p><p><strong>反相</strong></p><p>基于图案中的色调反转纹理中的亮点和暗点。当选择“反相”时，图案中的最亮区域是纹理中的暗点，因此接收最少的油彩；图案中的最暗区域是纹理中的亮点，因此接收最多的油彩。当取消选择“反相”时，图案中的最亮区域接收最多的油彩；图案中的最暗区域接收最少的油彩。</p><p><strong>比例</strong></p><p>指定图案的缩放比例。键入数字，或者使用滑块来输入图案大小的百分比值。</p><p><strong>为每个笔尖设置纹理</strong></p><p>将选定的纹理单独应用于画笔描边中的每个画笔笔迹，而不是作为整体应用于画笔描边（画笔描边由拖动画笔时连续应用的许多画笔笔迹构成）。必须选择此选项，才能使用“深度”变化选项。</p><p><strong>模式</strong></p><p>指定用于组合画笔和图案的混合模式。（请参阅<a href="https://helpx.adobe.com/cn/photoshop/using/blending-modes.html">混合模式</a>。）</p><p><strong>深度</strong></p><p>指定油彩渗入纹理中的深度。键入数字，或者使用滑块来输入值。如果是 100%，则纹理中的暗点不接收任何油彩。如果是 0%，则纹理中的所有点都接收相同数量的油彩，从而隐藏图案。</p><p><strong>最小深度</strong></p><p>指定将“深度控制”设置为“渐隐”、“钢笔压力”、“钢笔斜度”或“光笔轮”并且选中“为每个笔尖设置纹理”时油彩可渗入的最小深度。</p><p><strong>深度抖动和控制</strong></p><p>指定当选中“为每个笔尖设置纹理”时深度的改变方式。若要指定抖动的最大百分比，请输入一个值。若要指定希望如何控制画笔笔迹的深度变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的深度变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长从“深度抖动”百分比渐隐到“最小深度”百分比。</p><p><strong>钢笔压力、钢笔斜度、光笔轮、旋转</strong></p><p>依据钢笔压力、钢笔斜度、钢笔拇指轮位置或钢笔旋转角度来改变深度。</p><h4 id="几个示例">几个示例：</h4><p><strong>减去</strong>模式：</p><p><img src="/2020/05/20/ps-brush/image-20200519185949006.png" alt="image-20200519185949006" style="zoom:50%;"></p><p><strong>正片叠底</strong>模式：</p><p><img src="/2020/05/20/ps-brush/image-20200519185914171.png" alt="image-20200519185914171" style="zoom:50%;"></p><p><strong>颜色减淡</strong>模式：</p><p><img src="/2020/05/20/ps-brush/image-20200519190128765.png" alt="image-20200519190128765" style="zoom:40%;"></p><p><strong>高度</strong>模式：</p><p><img src="/2020/05/20/ps-brush/image-20200519190234030.png" alt="image-20200519190234030" style="zoom:33%;"></p><h3 id="画笔形状动态">画笔形状动态</h3><p>形状动态决定描边中画笔笔迹的变化。</p><figure><img src="/2020/05/20/ps-brush/pa_13.png" alt="Photoshop 有形状动态和无形状动态的画笔描边"><figcaption aria-hidden="true">Photoshop 有形状动态和无形状动态的画笔描边</figcaption></figure><p>无形状动态（左图）和有形状动态（右图）的画笔描边</p><p><strong>大小抖动和控制</strong></p><p>指定描边中画笔笔迹大小的改变方式。有关更多信息，请参阅<a href="https://helpx.adobe.com/cn/photoshop/using/creating-modifying-brushes.html">创建和修改画笔</a>。</p><p>若要指定抖动的最大百分比，请通过键入数字或使用滑块来输入值。若要指定希望如何控制画笔笔迹的大小变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的大小变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长在初始直径和最小直径之间渐隐画笔笔迹的大小。每个步长等于画笔笔尖的一个笔迹。值的范围可以从 1 到 9999。例如，输入步长数 10 会产生 10 个增量的渐隐。</p><p><strong>钢笔压力、钢笔斜度或光笔轮</strong></p><p>可依据钢笔压力、钢笔斜度或钢笔拇指轮位置以在初始直径和最小直径之间改变画笔笔迹大小。</p><p><strong>最小直径</strong></p><p>指定当启用“大小抖动”或“大小控制”时画笔笔迹可以缩放的最小百分比。可通过键入数字或使用滑块来输入画笔笔尖直径的百分比值。</p><p><strong>倾斜缩放比例</strong></p><p>指定当“大小抖动”设置为“钢笔斜度”时，在旋转前应用于画笔高度的比例因子。键入数字，或者使用滑块输入画笔直径的百分比值。</p><p><strong>角度抖动和控制</strong></p><p>指定描边中画笔笔迹角度的改变方式。该选项启用时，每次你重新下笔，都会随机改变一个画笔角度，随机程度由你设置的值来决定。若要指定抖动的最大百分比，请输入一个 360 度的百分比值。若要指定希望如何控制画笔笔迹的角度变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的角度变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长在 0 和 360 度之间渐隐画笔笔迹角度。</p><p><strong>钢笔压力、钢笔斜度、光笔轮、旋转</strong></p><p>依据钢笔压力、钢笔斜度、钢笔拇指轮位置或钢笔的旋转在 0 到 360 度之间改变画笔笔迹的角度。</p><p><strong>初始方向</strong></p><p>使画笔笔迹的角度基于画笔描边的初始方向。</p><p><strong>方向</strong></p><p>使画笔笔迹的角度基于画笔描边的方向。</p><p><strong>圆度抖动和控制</strong></p><p>指定画笔笔迹的圆度在描边中的改变方式。若要指定抖动的最大百分比，请输入一个指明画笔长短轴之间的比率的百分比。若要指定希望如何控制画笔笔迹的圆度变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的圆度变化。</p><p><strong>渐隐</strong></p><p>按指定数量步长在 100% 和“最小圆度”值之间渐隐画笔笔迹的圆度。</p><p><strong>钢笔压力、钢笔斜度、光笔轮、旋转</strong></p><p>依据钢笔压力、钢笔斜度、钢笔拇指轮位置或钢笔的旋转在 100% 和“最小圆度”值之间改变画笔笔迹的圆度。</p><p><strong>最小圆度</strong></p><p>指定当“圆度抖动”或“圆度控制”启用时画笔笔迹的最小圆度。输入一个指明画笔长短轴之间的比率的百分比。</p><p><strong>画笔投影</strong></p><p>指定当您使用光笔绘画时，光笔将更改为倾斜状态并将旋转光笔以改变笔尖形状。</p><h3 id="传递转换画笔选项">传递/转换画笔选项</h3><p>转换画笔选项确定油彩在描边路线中的改变方式。</p><figure><img src="/2020/05/20/ps-brush/pa_19.png" alt="Photoshop 传递画笔选项"><figcaption aria-hidden="true">Photoshop 传递画笔选项</figcaption></figure><p>无动态绘画的画笔描边（左图）和有动态绘画的画笔描边（右图）</p><p><strong>不透明度抖动和控制</strong></p><p>指定画笔描边中油彩不透明度如何变化，最高值（但不超过）是选项栏中指定的不透明度值。要指定油彩不透明度可以改变的百分比，请键入数字或使用滑块来输入值。若要指定希望如何控制画笔笔迹的不透明度变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的不透明度变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长将油彩不透明度从选项栏中的不透明度值渐隐到 0。</p><p><strong>钢笔压力、钢笔斜度或光笔轮</strong></p><p>可依据钢笔压力、钢笔斜度或钢笔拇指轮的位置来改变颜料的不透明度。</p><p><strong>流量抖动和控制</strong></p><p>指定画笔描边中油彩流量如何变化，最高（但不超过）值是选项栏中指定的流量值。</p><p>要指定油彩流量可以改变的百分比，请键入数字或使用滑块来输入值。若要指定希望如何控制画笔笔迹的流量变化，请从“控制”弹出式菜单中选取一个选项：</p><p><strong>关</strong></p><p>指定不控制画笔笔迹的流量变化。</p><p><strong>渐隐</strong></p><p>按指定数量的步长将油彩流量从选项栏中的流量值渐隐到 0。</p><p><strong>钢笔压力、钢笔斜度或光笔轮</strong></p><p>可依据钢笔压力、钢笔斜度或钢笔拇指轮的位置来改变油彩的流量。</p><h3 id="标准画笔笔尖形状选项">标准画笔笔尖形状选项</h3><p>对于标准画笔笔尖，可设置“画笔设置”面板中的以下选项：</p><h4 id="大小">大小</h4><p>控制画笔大小。输入以像素为单位的值，或拖动滑块。</p><figure><img src="/2020/05/20/ps-brush/pa_06.png" alt="具有不同直径值的画笔描边"><figcaption aria-hidden="true">具有不同直径值的画笔描边</figcaption></figure><h4 id="使用取样大小">使用取样大小</h4><p>将画笔复位到它的原始直径。只有在画笔笔尖形状是通过采集图像中的像素样本创建的情况下，此选项才可用。</p><h4 id="翻转-x">翻转 X</h4><p>改变画笔笔尖在其 <em>x</em> 轴上的方向。</p><figure><img src="/2020/05/20/ps-brush/pa_07.png" alt="Photoshop 翻转 X"><figcaption aria-hidden="true">Photoshop 翻转 X</figcaption></figure><p>将画笔笔尖在其 x 轴上翻转 <strong>A.</strong> 处在默认位置的画笔笔尖 <strong>B.</strong> 选中了“翻转 X”时 <strong>C.</strong> 选中了“翻转 X”和“翻转 Y”时</p><h4 id="翻转-y">翻转 Y</h4><p>改变画笔笔尖在其 <em>y</em> 轴上的方向。</p><figure><img src="/2020/05/20/ps-brush/pa_08.png" alt="Photoshop 翻转 Y"><figcaption aria-hidden="true">Photoshop 翻转 Y</figcaption></figure><p>将画笔笔尖在其 y 轴上翻转 <strong>A.</strong> 处在默认位置的画笔笔尖 <strong>B.</strong> 选中了“翻转 Y”时 <strong>C.</strong> 选中了“翻转 Y”和“翻转 X”时</p><h4 id="角度">角度</h4><p>指定椭圆画笔或样本画笔的长轴从水平方向旋转的角度。键入度数，或在预览框中拖动水平轴。</p><figure><img src="/2020/05/20/ps-brush/pa_09.png" alt="Photoshop 角度"><figcaption aria-hidden="true">Photoshop 角度</figcaption></figure><p>带角度的画笔创建雕刻状描边</p><h4 id="圆度">圆度</h4><p>指定画笔短轴和长轴之间的比率。输入百分比值，或在预览框中拖动点。100% 表示圆形画笔，0% 表示线性画笔，介于两者之间的值表示椭圆画笔。</p><figure><img src="/2020/05/20/ps-brush/pa_10.png" alt="Photoshop 圆度"><figcaption aria-hidden="true">Photoshop 圆度</figcaption></figure><p>调整圆度以压缩画笔笔尖形状</p><h4 id="硬度">硬度</h4><p>控制画笔硬度中心的大小。键入数字，或者使用滑块输入画笔直径的百分比值。不能更改样本画笔的硬度。</p><figure><img src="/2020/05/20/ps-brush/pa_11.png" alt="Photoshop 硬度"><figcaption aria-hidden="true">Photoshop 硬度</figcaption></figure><p>具有不同硬度值的画笔描边</p><h4 id="间距">间距</h4><p>控制描边中两个画笔笔迹之间的距离。如果要更改间距，请键入数字，或使用滑块输入画笔直径的百分比值。当取消选择此选项时，光标的速度将确定间距。</p><figure><img src="/2020/05/20/ps-brush/pa_12.png" alt="Photoshop 间距"><figcaption aria-hidden="true">Photoshop 间距</figcaption></figure><p>增大间距可使画笔急速改变</p><h4 id="注意">注意:</h4><p>当使用预设画笔时，按 [ 键可减小画笔宽度；按 ] 键可增加宽度。对于硬边圆、柔边圆和书法画笔，按 Shift+ [ 键可减小画笔硬度；按 Shift+ ] 键可增加画笔硬度。</p><h2 id="常用辨析">常用辨析</h2><ul><li>常用的<strong>19号笔刷</strong>其实主要就是<code>大小抖动</code>+<code>流量抖动</code>+<code>建立</code>构成的。</li><li>常用的<strong>喷枪</strong>就是 <code>柔角笔尖形状</code>+ <code>流量抖动</code>+<code>建立</code>构成的。或者说，广义上的喷枪可以由任意一个笔刷加上<code>建立</code>选项构成。</li><li>能否<strong>混色</strong>不是由笔刷决定的，而是由<strong>笔的类型</strong>决定的。<strong>混合画笔工具</strong>可以让颜色混合，而普通 <strong>画笔工具</strong>则不可以。混合画笔工具的混色不仅仅是上下颜色的融合，而且会把下面的色调带到现有的笔刷之中，即使后面没有了下层其他颜色，画笔也会持续维持混色一段时间直至完全衰减。如下图：</li></ul><figure><img src="/2020/05/20/ps-brush/image-20200519171159376.png" alt="image-20200519171159376"><figcaption aria-hidden="true">image-20200519171159376</figcaption></figure><ul><li>要想实现颜色混合，比如<code>红+绿=黄</code>，任何有透明度的笔刷都可以实现。但如果想让这些颜色像是都粘在了刷子上一样后面会一起蹭出来，或是变化涂料的湿度，或是想要“新的涂料上去后下面的涂料被浸湿并聚集在新笔迹两侧”的效果，则需要使用 <strong>混合画笔工具</strong>。</li><li>水彩的核心是<code>低流量</code>+<code>建立</code>+<code>散布</code>+<code>双重画笔</code>+<code>流量抖动</code>+<code>大小抖动</code>。其中，在纸上氤氲散开的效果主要是由双重画笔中的辅助画笔完成的</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;选项研究&quot;&gt;选项研究&lt;/h2&gt;
&lt;p&gt;由于多次搜索和研究画笔设置和笔刷的制作方法，但均感觉网上所讲一知半解居多，这次特意找时间全部整理了一遍。难理解的、可能并不常用的放在前面着重讲，容易理解的、常用的则放在后面。&lt;/p&gt;
&lt;h3 id=&quot;杂色noise&quot;&gt;杂色(Noise)&lt;/h3&gt;
&lt;p&gt;为个别画笔笔尖增加额外的随机性。当应用于柔画笔笔尖（包含灰度值的画笔笔尖）时，此选项最有效。&lt;/p&gt;</summary>
    
    
    
    
    <category term="art" scheme="https://www.miracleyoo.com/tags/art/"/>
    
    <category term="painting" scheme="https://www.miracleyoo.com/tags/painting/"/>
    
    <category term="ps" scheme="https://www.miracleyoo.com/tags/ps/"/>
    
  </entry>
  
  <entry>
    <title>Mac、Windows、iPad三端完美论文管理、阅读与编辑系统配置</title>
    <link href="https://www.miracleyoo.com/2020/04/26/paper-zotero-dropbox/"/>
    <id>https://www.miracleyoo.com/2020/04/26/paper-zotero-dropbox/</id>
    <published>2020-04-27T01:09:00.000Z</published>
    <updated>2021-03-12T09:11:15.702Z</updated>
    
    <content type="html"><![CDATA[<p>管理好自己手中的论文，不让他们被吞噬与“Download”文件夹的茫茫文件海洋中已非易事，而能在需要时迅速定位，能在Windows的Desktop与便携的MacBook上无缝对接论文库则需要相对精细化的管理。然而，很多时候并不适合展开自己的笔记本电脑来“郑重”地阅读一篇论文，在一些相对零碎的时间里随手抽出包中的iPad，读读前面顺手存下的论文，则可大大增加自己的学术幸福感。</p><span id="more"></span><h2 id="需求分析">需求分析</h2><p>上面这几点也正是我对论文管理系统的要求。注意这里提到的是“系统”，而并是一个单一的软件。这里提炼一下日常对论文管理及阅读的需求，您可以看看和自己的需求是否吻合：</p><ol type="1"><li>以分层目录的形式将论文进行归档，并且有时需要让同一篇论文同时存放于多个目录中。</li><li>支持多及目录（多&gt;2）。</li><li>支持从网页上便捷地储存论文以及文档。</li><li>支持拖入PDF自动寻找论文信息。</li><li>需要对论文的PDF原件进行存储，最好能够自动下载缺失的PDF。</li><li>需要在Mac和Windows端都能访问论文目录，且对PDF文件的修改能够同步。</li><li>PDF的存储最好和数据库系统分离，以便搜索、单独更改或访问。</li><li>需要对论文进行可自定义格式的自动重命名。</li><li>界面不能太丑。</li><li>支持高度自定义或有充足的功能，最好可以使用第三方插件。</li><li>支持各种引文格式。</li><li>支持Latex和Word的便捷引用，Word最好有插件。</li><li>拥有较大的云存储空间，至少足以存储所有的PDF文档。</li><li>可以从iPad上访问并且可以同步、上传对论文的更改。</li><li>三个平台PDF阅读器配置最好统一，以免高亮、插入文本等格式不一。</li></ol><p>归纳出的这几点便是我对论文管理系统的所有需求了。之后便是对各种论文管理软件的试用和组合。这里直奔主题，给出我现在的全套系统配置及选择的原因。</p><h2 id="系统配置">系统配置</h2><ul><li>Mac与Windows端主论文管理软件：<a href="https://www.zotero.org/">Zotero</a></li><li>主要插件：<a href="http://zotfile.com/">zotfile</a>，<a href="https://github.com/retorquere/zotero-better-bibtex">zotero-better-bibtex</a></li><li>Mac、Windows与iPad统一PDF同步软件：Dropbox</li><li>Mac、Windows与iPad统一PDF阅读器：Adobe Acrobat</li></ul><h2 id="选用原因">选用原因</h2><h3 id="zotero">Zotero</h3><p>各大平台上以及Zotero官网对其讲解都很多也很充分了，如果你能够并且愿意花上一些时间来进行自定义配置，它将是一个可以满足几乎所有对论文管理需求的终极软件。我这里只指出一些我认为非常不错的特性。</p><ol type="1"><li>跨平台。Mac，Windows和Linux都有支持。</li><li>浏览器论文抓取插件很好用。既可直接抓取PDF文章之后解析出论文，也可在如Google Scholar等页面直接批量抓取添加论文索引，并再自动下载相应的PDF。甚至在一些作业性质的小论文中有时需要直接引用某个网页或某个文档，它也可以直接生成条目。</li><li>支持多级目录。这个多理论上似乎可以无限多下去，非常自由。</li><li>有很好用的Word插件，使用体验丝滑流畅。</li><li>拥有在线的庞大引用格式库，基本可以找到所有需要的引文格式。</li><li>可以加载很多第三方插件，如支持自定义格式重命名的zotfile，针对输出引用进行优化的better-bibtex等。</li><li>可以将条目的PDF文件储存目录单列出来，存到一个自定义的地方，比如Dropbox文件夹内部。</li></ol><h3 id="dropbox">Dropbox</h3><ol type="1"><li>因为现在在美国读书，Dropbox的服务相对来说是快速且稳定的。</li><li>Dropbox的同步功能做的非常好，这也是其在此领域深耕多年的结果。</li><li>最重要一点，Dropbox在iPad上和Adobe Acrobat有着原生的集成。在Dropbox中使用Acrobat打开并编辑文件，其修改是可以直接同步到Dropbox云端的。这一点对于移动端浏览和编辑论文起到了至关重要的作用。</li><li>免费版虽然只有2G初始空间，但是可以通过邀请好友注册达到最大的18G。虽然不是特别大，但是对于存储论文已经绰绰有余了。（另这个邀请注册送空间可以直接去淘宝搜索，可以画很少的钱得到很多注册服务，直达18G。）</li></ol><h3 id="adobe-acrobat">Adobe Acrobat</h3><ol type="1"><li>多平台支持。Mac，Windows，Linux，iOS，Android都有着很好的支持。</li><li>专业，支持各种编辑方式。常见的高亮、下划线、添加文本、画方框、做批注等自然都是支持的。</li><li>很多高校都和Adobe公司有着协议，使用学生邮箱可以直接免费使用全套功能。</li><li>同前面所述，其和Dropbox的关联性支持是整套系统成功的关键。</li></ol><h3 id="对比原因">对比原因</h3><ol type="1"><li>之前有在用Mendeley，但是对比Zotero，尤其是对比Zotero和其第三方插件的丰富功能后，前者明显力不从心。另外Windows版本其界面没有针对高分辨率进行适配，并且使用了默认的宋体，显示英文丑陋不堪，且无法更改。在论坛上看到有许多坛友几年前就提出了这些问题，然而显然开发者并没有做出相应。对比Zotero，其论坛环境、活跃程度以及问题解决速度都会更好。</li><li>Endnote是收费软件，我并没有深入使用，这里不多置评，但各位可以容易查到Endnote和Zotero的区别。</li><li>iPad上也考虑过使用PDF Reader -- Document Expert作为浏览和编辑软件，但是从Dropbox打开的文件编辑后是以副本的形式存在了本地，并没有被同步，这是无法接受的。</li></ol><h2 id="配置细节">配置细节</h2><h3 id="zotero设置部分">Zotero设置部分</h3><ol type="1"><li>在所有需要同步的电脑上登录Zotero的账号，如没有，请注册。</li><li>打开设置，更改<code>Files and Folders</code> 中的<code>Base Directory</code>选项为你的同步盘地址，如Dropbox，OneDrive，坚果云等。请不要动下面的<code>Data Directory Location</code>，这个是Zotero的总体数据库的地址，不建议放到云文件夹下，因为只要有两个端同时使用Zotero这个同步就崩掉了。</li></ol><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110225913571.png" alt="image-20191110225913571"><figcaption aria-hidden="true">image-20191110225913571</figcaption></figure><ol start="3" type="1"><li>安装<a href="http://zotfile.com/">zotfile</a>。更改有关文件的设置。</li></ol><p>从Tools栏进入ZotFile Preference</p><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230601630.png" alt="image-20191110230601630"><figcaption aria-hidden="true">image-20191110230601630</figcaption></figure><p>更改PDF文件存储位置。这里是把文件储存到云盘的关键步骤。上面那个更改文件存储地址的作用是指定未来加入的PDF文件的存储地址，而这里是把已经在库的文件移动到这个地址。两个地址相同。</p><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230944192.png" alt="image-20191110230944192"><figcaption aria-hidden="true">image-20191110230944192</figcaption></figure><p>更改重命名相关的设置。这里的%y就是论文发表年份，%j是期刊名，%t是论文标题。而中间的下划线则只是单纯的会在重命名后的文件名中的两个元素之间加一个下滑线罢了，这里可以替换做任意。</p><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110230654373.png" alt="image-20191110230654373"><figcaption aria-hidden="true">image-20191110230654373</figcaption></figure><p>在你的所有需要同步的电脑上做完上述步骤后，如果你之前没有Zotero或它是全新的没有条目，那你的设定已经结束。如果库中已有很多论文，想要直接移动到Dropbox相应目录下，那么请执行下一步：</p><ol start="4" type="1"><li>移动与重命名已有文件</li></ol><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110231928468.png" alt="image-20191110231928468"><figcaption aria-hidden="true">image-20191110231928468</figcaption></figure><p>点击 My Library，全选所有条目，右键选择<code>Manage Attachments</code>-&gt;<code>Rename Attachments</code>开始移动和重命名。</p><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110232045736.png" alt="image-20191110232045736"><figcaption aria-hidden="true">image-20191110232045736</figcaption></figure><figure><img src="/2020/04/26/paper-zotero-dropbox/image-20191110232150562.png" alt="image-20191110232150562"><figcaption aria-hidden="true">image-20191110232150562</figcaption></figure><h3 id="ipad设置部分">iPad设置部分</h3><ol type="1"><li>下载Dropbox和Acrobat。</li><li>打开Dropbox，进入你的论文同步文件夹，任选一篇点击打开</li><li>右下角找到一个光标键，点击，会提示用Adobe Acrobat Reader打开。</li><li>All set.</li></ol><figure><img src="/2020/04/26/paper-zotero-dropbox/IMG_2601.jpg" alt="IMG_2601"><figcaption aria-hidden="true">IMG_2601</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;管理好自己手中的论文，不让他们被吞噬与“Download”文件夹的茫茫文件海洋中已非易事，而能在需要时迅速定位，能在Windows的Desktop与便携的MacBook上无缝对接论文库则需要相对精细化的管理。然而，很多时候并不适合展开自己的笔记本电脑来“郑重”地阅读一篇论文，在一些相对零碎的时间里随手抽出包中的iPad，读读前面顺手存下的论文，则可大大增加自己的学术幸福感。&lt;/p&gt;</summary>
    
    
    
    
    <category term="paper" scheme="https://www.miracleyoo.com/tags/paper/"/>
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>避免脏活，完美使用Markdown在知乎编辑内容</title>
    <link href="https://www.miracleyoo.com/2020/04/26/markdown-4-zhihu/"/>
    <id>https://www.miracleyoo.com/2020/04/26/markdown-4-zhihu/</id>
    <published>2020-04-27T01:01:57.000Z</published>
    <updated>2021-03-12T09:09:03.150Z</updated>
    
    <content type="html"><![CDATA[<p>知乎上的本文链接：<a href="https://zhuanlan.zhihu.com/p/97455277">Link</a></p><p>首先吐槽一下知乎的编辑器。虽然个人博客上的不少内容都曾有想过搬到知乎一份，但是知乎的编辑器真的是令人绝望式的难用。尽管现在可以使用文件导入功能导入md文件和Word文档，且能支持一些简单的Markdown语法，但每种途径都有着无法避免的缺点，从结果上来说则是只能被迫接受或是不完美的格式亦或是大量手动且重复的图片上传。</p><p>口说无凭，这里放一下几种方法的对比图来详述一下问题所在：</p><span id="more"></span><h3 id="typora中原文件">Typora中原文件</h3><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174243537.png" alt="image-20191214174243537" style="zoom:50%;"></p><p>这份测试文件虽然短，但是基本包含了常见几种要素：标题、正文、图片、表格、代码、公式。下面让我们看看知乎支持的几种上传方式的效果：</p><h3 id="直接复制typora中的内容到知乎编辑器">1. 直接复制Typora中的内容到知乎编辑器</h3><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174118623.png" alt="image-20191214174118623" style="zoom:50%;"></p><p>可以看到，标题和正文区分开了，不过所有的标题都变成了一级标题。另外本地的图片无法导入，只剩下一个展占位符。表格全乱，公式直接消失了。但代码的高亮仍是C++，正确。</p><h3 id="直接导入markdown文件">2. 直接导入Markdown文件</h3><p>你可以在编辑器的这个位置导入文件：</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174529632.png" alt="image-20191214174529632" style="zoom:33%;"></p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174506716.png" alt="image-20191214174506716" style="zoom:33%;"></p><p>导入刚才我们看到的测试文件原档的效果是这样的：</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214174704023.png" alt="image-20191214174704023" style="zoom:50%;"></p><p>同前，标题和正文区分开了，不过所有的标题都变成了一级标题。另外本地的图片无法导入，只剩下一个占位符。表格全乱，公式没有消失，但也并没有被渲染。代码的高亮仍是C++，正确。</p><h3 id="先使用typora导出为word再用知乎编辑器导入word">3. 先使用Typora导出为Word，再用知乎编辑器导入Word</h3><p>上面的两种最直观的方法的一大问题就是图片导入不进去。而对于一些长篇的科技文章，图片既多又重要，手动一个个添加容易错而且浪费科研人员的时间和热情。当然我知道导入Markdown时并没有顺带把图片本身导入进去，但我仍觉得这是知乎团队应该做的工作，而且是相当基本的工作。好吧，既然现在不可行，那么导出成Word再导入该不会有这个问题了吧，我们来看看：</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214175626414.png" alt="image-20191214175626414" style="zoom:50%;"></p><p>好家伙，图片导入进去了，表格直接炸飞天了，而且更可气的是代码的高亮没了，格式也出现了问题。其他的嘛，不看不得了，一看发现公式似乎直接没了，中间还莫名其妙多了一堆空行。当然，标题等级的问题还是没解决。</p><p>那是Typora导出Word导出的不好吗？我打开了导出的Word文件：</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214180048117.png" alt="image-20191214180048117" style="zoom:50%;"></p><p>公式存在，高亮正确，标题等级正确，表格正确，没有奇怪的空行。虽然和Markdown渲染的结果相比也并不好看说实话，但至少它是对的，而知乎编辑器错的五花八门。</p><h2 id="那么如何拯救自己的双手和灵魂呢">那么，如何拯救自己的双手和灵魂呢？</h2><h3 id="首先调整好你的markdown编辑器">首先调整好你的Markdown编辑器</h3><p>为什么要首先调整好编辑器呢？这里我说的调整主要指的是对图片管理方式的调整。如果您使用Typora，建议在偏好设置页面将相关参数调整至和下图完全一致，以防后面出现问题。</p><p>这里做的工作主要是将所有来源的图片都自动保存至同名文件夹下，以相对路径储存。使用其他Markdown编辑器的小伙伴也可以对照调整。这么做的目的主要是为了方便后面对图片的批量上传与转换。</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214221536561.png" alt="image-20191214221536561" style="zoom:50%;"></p><p>相信很多同学看到这里就会发出疑问，为什么不适用iPic之类的图床软件直接上传至图床呢？既方便又舒适。我的答案是，因为我吃过亏。我的内容之前一直独发于我的个人博客，然而今年中旬，突然之间整个网站所有的图片都挂掉了，只显示一个占位符和无法访问的提示，之后我发现之前使用的新浪图床加入了防盗链，所以就GG了。当然后面我也用Python再一次解决了这个问题，对解决方法感兴趣的图形可以移步这里，然而这一次的教训让我理解了这些图床<strong>并不可控</strong>。它们随时可以剥夺掉你博客中的全部图片，而你是无力至极的。</p><p>在那之后，我就选择了本地储存+Github备份的模式，这样既可以永久有安心的本地档，也有方便使用的Github链接，可以说是既方便又安全。</p><h3 id="之后解决图片上传问题">之后解决图片上传问题</h3><p>最方便的解决办法即为利用好GitHub的资源了。建立一个Public的GitHub仓库，这里我命名作<strong><a href="https://github.com/miracleyoo/Markdown4Zhihu">Markdown4Zhihu</a></strong>，注意一定要为Public，否则知乎无法访问这些图片。</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214221339326.png" alt="image-20191214221339326" style="zoom:50%;"></p><p>当然，如果你觉得麻烦，也可以直接folk我建好的仓库，一会儿我们要提到的“一键Markdown知乎适配脚本”也会在这个仓库里。你只需要将你的文件和相应的图片文件夹放到这个<code>Data</code>子目录下，即可调用脚本一键转换，并将涉及到的图片自动推到你相应的GitHub仓库中。</p><p><img src="/2020/04/26/markdown-4-zhihu/image-20191214213407292.png" alt="image-20191214213407292" style="zoom:50%;"></p><p>这是我们使用脚本一键转换后的结果。它很好的解决的图片上传的问题，同时也保证了代码段的高亮，同时，所有的行内公式和多行公式都得到了转换。转换后的公式在知乎上传文件之后，是可交互的，即你可以在上传之后在知乎编辑器中修改你的公式，而不必重新再来一遍。</p><p>至于表格，这个真木得办法，因为知乎压根不支持表格你说这咋整嘛。但是也不是没有可替代方案。如果表格不是很多，你可以直接对其进行截图，删去原代码后粘贴截图。之后它就会按照图片模式被兼容上去。如果你不想截图也可，做了相应操作后会得到上图的结果，对于少量表格来看也是OK的。你可以在<a href="https://zhuanlan.zhihu.com/p/97432671">这里</a>看到上传到知乎后的效果。</p><h3 id="最后是具体使用流程">最后是具体使用流程</h3><p>这里我们假设您的文件名为<code>一个测试文档.md</code>，并将其和同名图片文件夹放到<code>Data</code>目录下（如果新建文件时就直接在Data里面建会更加方便），接着打开terminal(Linux/MacOS)或Git Bash(Windows)(或其他任何支持Git命令的终端)，<code>cd</code>进入该项目的根目录，即<code>Markdown4Zhihu</code>目录，输入以下命令：</p><p><code>python zhihu-publisher.py --input="./Data/一个测试文档.md"</code></p><p>OK，all set. 在<code>Data</code>目录下，你可以看到一个<code>一个测试文档_for_zhihu.md</code>的文件，将它上传至知乎编辑器即可。</p><p>PS: 脚本使用Python3，Python2可能会有潜在问题。</p><h2 id="最后的话">最后的话</h2><p>知乎的开发者的逻辑其实我真的比较迷，我们大学学生团队的自建论坛都可以原生完美支持Markdown和公式，然而知乎却一直说这个功能必要性不足强调开发难度。同样令人难受的是知乎的搜索，多少年过去了非热门话题还是一如既往的难用，搜索还是借助Google 的 “问题+知乎”。不知道这是什么原因，不过还是希望知乎团队先把这些非常基础的东西做好再大谈用户体验。</p><p>这次的解决方案需要对GitHub和命令行有基础的了解，不过考虑到会来读这篇文章的人应该程序员居多，问题应该不是很大。脚本还比较新，如果有bug欢迎提出。最后再放一下GitHub链接，如果它有帮到你，希望能随手留下一个star，谢谢！<strong><a href="https://github.com/miracleyoo/Markdown4Zhihu">Markdown4Zhihu</a></strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;知乎上的本文链接：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/97455277&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先吐槽一下知乎的编辑器。虽然个人博客上的不少内容都曾有想过搬到知乎一份，但是知乎的编辑器真的是令人绝望式的难用。尽管现在可以使用文件导入功能导入md文件和Word文档，且能支持一些简单的Markdown语法，但每种途径都有着无法避免的缺点，从结果上来说则是只能被迫接受或是不完美的格式亦或是大量手动且重复的图片上传。&lt;/p&gt;
&lt;p&gt;口说无凭，这里放一下几种方法的对比图来详述一下问题所在：&lt;/p&gt;</summary>
    
    
    
    
    <category term="tool" scheme="https://www.miracleyoo.com/tags/tool/"/>
    
    <category term="markdown" scheme="https://www.miracleyoo.com/tags/markdown/"/>
    
    <category term="math" scheme="https://www.miracleyoo.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Docker与Nivida-Docker的用法与注意事项</title>
    <link href="https://www.miracleyoo.com/2020/04/26/docker-et-nvidia/"/>
    <id>https://www.miracleyoo.com/2020/04/26/docker-et-nvidia/</id>
    <published>2020-04-27T00:56:52.000Z</published>
    <updated>2021-03-12T08:58:20.777Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker-images-and-containers">Docker Images and Containers</h2><ul><li><p>清除所有已经停止的container：<code>docker container prune -f</code> 。其中 <code>-f</code> 表示不弹出确认提示。也可使用<code>docker rm $(docker ps -a -q)</code>来清理。其中，<code>docker rm</code>代表删除container，而<code>docker rmi</code>则是删除image。</p></li><li><p>如果你需要实例化一个只用一次的container，那么使用<code>docker run --rm</code>参数，结束后会自动删除。</p><span id="more"></span></li><li><p><code>docker ps &lt;-a&gt;</code> 可以列出正在运行的/全部的container。其效果和<code>docker container ls &lt;-a&gt;</code>相同。而若想列出全部images，则要使用<code>docker images</code>。</p></li><li><p>docker images中的环境变量有四个来源：</p><ol type="1"><li>Dockerfile中通过<code>ENV</code>指令添加的环境变量，如<code>ENV PATH /opt/conda/bin:$PATH</code></li><li>Dockerfile中通过修改<code>/root/.bashrc</code>文件使用<code>export</code>命令添加到bash中的环境变量，如<code>export PATH=/OPT/conda/bin:$PATH</code>命令。</li><li>在通过image实例化container时添加<code>-e</code>或<code>--env</code>参数来添加到环境中的变量。这个方法有局限性，它不能完成对已有变量的“添加”操作，只能新建一个新的环境变量，如<code>--env NEW_VAR=/opt/conda/bin</code></li><li>在<code>docker run</code>末端的container内命令的前面添加一句<code>export</code>引导的命令，如：<code>docker run -it -v $(PWD):/app debian:jessie bash -c 'export PATH=$PATH:/opt/conda/bin; bash'</code>。它的缺点是较为复杂。</li></ol><p>其中，如果能找到源Dockerfile，最好的方法是通过修改Dockerfile然后重新build得到一个自己的版本。其次是方法三，实在不行使用方法四。如果先进入bash再运行命令可以正常运行，而直接使用<code>docker run</code>出现了环境变量相关的失败提示，很可能是由于Dockerfile写的时候使用的是在<code>/root/.bashrc</code>中添加环境变量的方法所致。</p></li><li><p>如果需要一个container长期在后台待机候命，那可以使用<code>-d</code>或<code>--detach</code>选项建立一个一直待机的docker进程。使用方法：</p><ol type="1"><li><code>docker run -itd --name NAME xxx/xxx:xx /bin/bash</code></li><li><code>docker exec -it NAME your-command</code></li></ol></li><li><p>启动时如果需要对本地文件夹和Docker内部文件夹做映射，则使用<code>docker run -v &lt;LOCAL_FOLDER&gt;:&lt;DOCKER_FOLDER&gt;</code> 。 该参数可以复数次出现，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run \</span><br><span class="line">    -v <span class="variable">$AUDIO_IN</span>:/input \</span><br><span class="line">    -v <span class="variable">$AUDIO_OUT</span>:/output \</span><br><span class="line">    -v <span class="variable">$MODEL_DIRECTORY</span>:/model \</span><br><span class="line">    -e MODEL_PATH=/model \</span><br><span class="line">    researchdeezer/spleeter \</span><br><span class="line">    separate -i /input/audio_1.mp3 /input/audio_2.mp3 -o /output</span><br></pre></td></tr></table></figure></li><li><p>docker run所有的参数都应该写在镜像名字<code>xxx/xxx:xx</code>前面，写在其后面的统统会被视作在docker container中运行的命令或命令参数。</p></li><li><p>如果你有了一个在后台持续运行的container，且你想弄一个交互性bash，此时你仍需要加上<code>-it</code>参数，同样是在<code>docker exec</code>后，container名字前加需要的参数，restart和start同理。</p></li><li><p>如果你对作者的Docker Image不满意，需要修改，此时有两种方法：</p><ol type="1"><li>找到Dockerfile并修改，<code>docker build</code>，<code>docker push</code></li><li>使用bash进入一个实例化的Image，在里面做一通操作，出来后使用<code>docker commit -m &lt;YOUR_MESSAGE&gt; -a &lt;AUTHOR_NAME&gt; &lt;CONTAINER_ID&gt; &lt;DOCKERHUB_USERNAME/NEW_IMAGE_NAME:TAG&gt;</code>提交更改使其保存为一个新的镜像，最后使用<code>docker push</code>推送新的镜像到docker hub。如果命名有误或忘记添加docker hub username作为前缀，那么可以使用<code>docker tag &lt;existing-image&gt; &lt;hub-user&gt;/&lt;repo-name&gt;[:&lt;tag&gt;]</code>改名。</li></ol><p>注意，使用<code>docker push</code>需要有docker hub账号，并在push前使用<code>docker login</code>操作登录。</p></li><li><p>一个辨析：<code>docker commit</code>针对的是一个正在运行的container，使其固化为一个image；而<code>docker push</code>推送的则是一个image到docker hub。前者是本地操作，后者是上传操作。</p></li><li><p>一个区别：<code>docker start</code>是启动一个已经停止的container，而<code>docker restart</code>则是先stop一个container再start。如果一个container已经停止了，那么二者等效。</p></li></ul><h2 id="dockerfile">Dockerfile</h2><ul><li>Docker Hub中并不直接提供Dockerfile，但可以通过查看image的“标签”页面看每个image的docker建立操作。但由于Docker build的时候使用git会很方便，所以很多作者会在其Github上发布这些Dockerfile，往往可以查看介绍页面找到链接。</li><li>Dockerfile中设置进入点命令：<code>ENTRYPOINT ["spleeter"]</code>。</li></ul><ol type="1"><li>这里”spleeter“是一个bin可执行文件。它的效果是：本来需要用户在<code>docker run</code>时输入<code>docker run xxx/xxx:xx spleeter separate</code>， 现在就只用输入<code>docker run xxx/xxx:xx separate</code>了，即run的时候帮你先打了一个命令标记但没给你按回车。</li><li>如果你发现自己在运行一个docker image时候提示了某个你没有输入的命令的相关问题，如<code>xxx don't have a parameter yyy, please input aaa, bbb, or ccc</code>，很有可能是Dockerfile中设定了进入点。</li><li>如果作者在Dockerfile中设定了进入点，但你需要进入docker进行调试或检查时，可以使用<code>docker run -it --entrypoint bash</code>来切换入点，进入一个bash命令行中调试。</li></ol><ul><li><code>docker build</code>针对的是一个url或是一个本地的文件夹。如果是本地的文件夹，文件夹内需要含有一个以<code>Dockerfile</code>为名的文件，如果需要导入某些文件到Docker Image中，则这些文件需要在正确的位置。<ol type="1"><li>如果dockerfile的名字不是<code>Dockerfile</code>，则使用<code>-f/--file &lt;DOCKERFILE_NAME&gt;</code>来指定名称。</li><li>如果需要指定输出image的名字和tag，则使用<code>-t/--tag</code>标签，以<code>name:tag</code>命名。</li><li>如果build的时候忘记了命名image，则输出的image没有名字和tag，只有一个随机序号。此时如果要重命名，可以用<code>docker tag &lt;SERIAL_NUMBER&gt; &lt;NAME:TAG&gt;</code> 命令。默认tag为latest。</li><li>Docker Build示例：<ul><li>本地文件夹：<code>docker build -f &lt;NAME:TAG&gt; &lt;TARGET DICTIONARY&gt;</code></li><li>本地文件：<code>docker build - &lt; &lt;Dockerfile_Path&gt;</code></li><li>URL：<code>docker build https://github.com/&lt;USERNAME&gt;/&lt;REPONAME&gt;.git#&lt;BRUNCH&gt;:&lt;SUBFOLDERNAME&gt;</code></li></ul></li></ol></li></ul><h2 id="nivdia-docker">Nivdia Docker</h2><ul><li><p>先放<a href="https://github.com/NVIDIA/nvidia-docker">链接</a>。这里是Nvidia Docker的Github仓库。</p></li><li><p>再说作用。若想在Docker中运行GPU程序，则普通的Docker是做不到的，程序无法默认在Docker中使用GPU计算资源；另一方面，如果本地已经安装了某个版本的CUDA，但目标程序需要依赖另一个版本，这也是非常麻烦的。而Nvidia Docker的出现则很好解决了这个问题。它相当于在Docker的下面塞进了一层CUDA层，介于Container和OS之间。</p><figure><img src="/2020/04/26/docker-et-nvidia/inner.png" alt="Nvidia Docker 原理图"><figcaption aria-hidden="true">Nvidia Docker 原理图</figcaption></figure></li><li><p>然后是安装。</p><ol type="1"><li><p>作为前置条件，需要本机上安装有Nvidia Driver，不强制要求CUDA。（不过既然都安到Driver了，不如把本机CUDA也装了）官方教程<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation">链接</a>。当然，请安装Docker。</p></li><li><p>执行以下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add the package repositories</span></span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> $ID<span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>代码可能会随着Nvidia Docker的的升级而发生变化，最好参阅本章第一条的链接。</p></li></ol></li><li><p>试运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Test nvidia-smi with the latest official CUDA image</span></span><br><span class="line">docker run --gpus all nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start a GPU enabled container on two GPUs</span></span><br><span class="line">docker run --gpus 2 nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting a GPU enabled container on specific GPUs</span></span><br><span class="line">docker run --gpus <span class="string">&#x27;&quot;device=1,2&quot;&#x27;</span> nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line">docker run --gpus <span class="string">&#x27;&quot;device=UUID-ABCDEF,1&quot;&#x27;</span> nvidia/cuda:10.0-base nvidia-smi</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specifying a capability (graphics, compute, ...) for my container</span></span><br><span class="line"><span class="comment"># Note this is rarely if ever used this way</span></span><br><span class="line">docker run --gpus all,capabilities=utility nvidia/cuda:10.0-base nvidia-smi</span><br></pre></td></tr></table></figure><p>其标志性特点就是一个参数<code>--gpus &lt;PARAMETERS&gt;</code> 一般使用<code>--gpus all</code>即可，其他部分和普通docker一模一样。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;docker-images-and-containers&quot;&gt;Docker Images and Containers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;清除所有已经停止的container：&lt;code&gt;docker container prune -f&lt;/code&gt; 。其中 &lt;code&gt;-f&lt;/code&gt; 表示不弹出确认提示。也可使用&lt;code&gt;docker rm $(docker ps -a -q)&lt;/code&gt;来清理。其中，&lt;code&gt;docker rm&lt;/code&gt;代表删除container，而&lt;code&gt;docker rmi&lt;/code&gt;则是删除image。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;如果你需要实例化一个只用一次的container，那么使用&lt;code&gt;docker run --rm&lt;/code&gt;参数，结束后会自动删除。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="docker" scheme="https://www.miracleyoo.com/tags/docker/"/>
    
    <category term="nvidia" scheme="https://www.miracleyoo.com/tags/nvidia/"/>
    
    <category term="gpu" scheme="https://www.miracleyoo.com/tags/gpu/"/>
    
  </entry>
  
  <entry>
    <title>Linux(Ubuntu)装机与配置笔记</title>
    <link href="https://www.miracleyoo.com/2020/04/11/linux-setup/"/>
    <id>https://www.miracleyoo.com/2020/04/11/linux-setup/</id>
    <published>2020-04-12T01:11:47.000Z</published>
    <updated>2021-03-12T09:08:49.885Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘相关">硬盘相关</h2><ol type="1"><li><p><strong>df命令</strong> <code>df</code>：检查linux服务器的文件系统的磁盘空间占用情况。<strong>它只会显示已经挂载的磁盘信息！</strong></p><p><code>df -h</code>, 即<code>--human-readble</code>：以1024的倍数的方式显示大小。(e.g., 1023M)</p><p><code>df -T</code>：查看所有磁盘的文件系统类型(type)</p><span id="more"></span></li><li><p><strong><code>fdisk</code>命令</strong></p><p><code>fdisk</code>：强大的磁盘监视和操作工具。</p><p><code>fdisk -l</code>会显示<strong>所有的</strong>磁盘和分区！不论有没有挂载，都会被列出来。</p></li><li><p><strong>mount命令</strong></p><p><code>mount</code>：挂载一个文件系统</p><p><code>mount -t ntfs &lt;source&gt; &lt;target&gt;</code>：以ntfs文件系统的形式从源目录挂载到目标目录。t表示types类型</p><p><code>mount -a</code>：挂载 fstab 中的所有文件系统。a表示all</p></li><li><p><strong>blkid命令</strong></p><p><code>sudo blkid</code>：获取各个分区的UUID和分区类型TYPE</p></li><li><p>物理磁盘与磁盘分区：一个物理磁盘在<code>fdisk -l</code>中的显示往往类似于<code>/dev/sda</code>，<code>/dev/sdb</code>，<code>/dev/nvme0n1</code>。一般情况下是不带数字的，sda sdb是最常见的命名。而分区命名则是如：<code>/dev/sda1</code>，<code>/dev/sdb2</code>之类在物理磁盘的后面带上数字表示分区编号。</p><p>但有些如双系统中，可能会出现最后一个例子中展示的命名，这种磁盘的分区则是以<code>p[x]</code>结尾，如<code>/dev/nvme0n1p1</code>，<code>/dev/nvme0n1p9</code>。</p></li><li><p>Linux开机后不会自动挂载Windows文件格式NTFS的磁盘。</p></li><li><p><code>sudo chmod -R 777 &lt;Folder_Name&gt;</code> 可以取消一个文件夹的全部访问权限。</p></li><li><p><code>chmod</code>命令对ext3/4文件系统，即Linux格式的文件系统才有效，对其他文件系统，如vfat(Fat32)，NTFS都是无效的。</p></li><li><p>/etc/fstab` 文件是掌管硬盘自动挂载配置的文件，包含自动挂载分区过程的必要信息。每一条记录格式如下：</p><p><code>[Device] [Mount Point] [File System Type] [Options] [Dump] [Pass]</code></p><p>如：</p><p><code>UUID=B45A01D55A019570 /data ntfs defaults 0 2</code></p><p>其中：</p><p><code>[Options]</code> ：<code>defaults</code>表示用默认的<code>rw, suid, dev, exec, auto, nouser, async</code>等选项（不同内核和文件系统不同）进行挂载，这些选项的含义：<code>rw</code> 可读写；<code>suid</code> 执行程序时遵守<code>uuid</code>；<code>dev</code> 解释字符或禁止特殊设备；<code>exec</code> 允许执行二进制文件；<code>auto</code> 可以<code>-a</code>方式加载；<code>nouser</code> 禁止普通用户挂载此文件系统；<code>async</code> 所有I/O异步完成。</p><p><code>[Dump]</code> ：是否开启分区备份，0表示关闭</p><p><code>[Pass]</code>：系统启动时检查分区错误的顺序，root为1，其他为2，0为不检查。</p></li><li><p>在<code>fstab</code>文件中添加记录前一定要先尝试用mount命令手动挂载。</p></li></ol><h3 id="参考">参考</h3><ol type="1"><li><a href="https://blog.csdn.net/qxqxqzzz/article/details/89790688">Ubuntu18.04 开机自动挂载其他硬盘</a></li><li><a href="https://blog.csdn.net/ybdesire/article/details/79145180">Linux查看与挂载新磁盘</a></li></ol><h2 id="cuda的安装">CUDA的安装</h2><ol type="1"><li><p>检查自己的GPU是否是CUDA-capable，在终端中输入<code>lspci | grep -I NVIDIA</code> ，会显示自己的NVIDIA GPU版本信息，去CUDA的官网查看自己的GPU版本是否在CUDA的支持列表中。</p></li><li><p>检查自己的Linux版本是否支持 CUDA（Ubuntu 稳定支持版没问题）。</p></li><li><p>检查其他问题。这里就不详述了，正常情况下一般OK，这里主要要检查是否安装了<code>gcc</code>，是否安装了<code>kernel header</code>和 <code>package development</code>。如果害怕出现问题可以参考<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">官网</a>执行这几步检测。</p></li><li><p>于<a href="https://developer.nvidia.com/cuda-downloads">CUDA官网</a>下载与系统对应的CUDA版本。最后一个选项选择<code>runfile</code>，因为其所需步骤最少，也因此最不容易出问题。所有选项完成后，你会看到如下两行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run</span><br><span class="line">sudo sh cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure><p>先不要执行第二条<code>sudo</code>开头的指令，只使用<code>wget</code>下载。</p></li><li><p>如果之前有安装过其他版本的CUDA并希望将其卸载，使用<code>sudo nvidia-uninstall</code>卸载。如果该命令不在系统路径中，则使用<code>sudo /usr/bin/nvidia-uninstall</code>（位置可能变化）卸载。如果还是没有，或是之前的驱动已经损坏，则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br><span class="line">sudo chmod +x NVIDIA-Linux-x86_64-410.93.run</span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-410.93.run --uninstall</span><br></pre></td></tr></table></figure></li><li><p>屏蔽<code>nouveau</code>驱动。</p></li></ol><h3 id="nouveau是什么">Nouveau是什么</h3><blockquote><h4 id="nouveau-accelerated-open-source-driver-for-nvidia-cards">Nouveau: Accelerated Open Source driver for nVidia cards</h4><p>The <strong>nouveau</strong> project aims to build high-quality, free/libre software drivers for <a href="https://nouveau.freedesktop.org/wiki/CodeNames/">nVidia cards</a>. “Nouveau” [<em>nuvo</em>] is the French word for “new”. Nouveau is composed of a Linux kernel KMS driver (nouveau), Gallium3D drivers in Mesa, and the Xorg DDX (xf86-video-nouveau). The kernel components have also been ported to <a href="https://nouveau.freedesktop.org/wiki/NetBSD/">NetBSD</a>.</p></blockquote><p>简单说，nouveau是Linux系统默认的给NVIDIA卡预装的一个图形加速驱动，而这个驱动会与CUDA产生部分冲突，所以在安装CUDA之前需要将其禁用，否则会出现卡在开机登录界面无法进入图形界面（仍然可以ssh访问），黑屏，鼠标键盘输入被禁用等问题中的一个或多个（亲身经历）。</p><p>继续安装教程：</p><ol start="6" type="1"><li><p>刚才说到要屏蔽<code>nouveau</code>，那么怎么知道你有没有装它呢？ 使用<code>lsmod | grep nouveau</code>命令，如果没有输出，就可以判定你没有运行<code>nouveau</code>，可以直接进入下一步，否则：</p><ol type="1"><li><p>Create a file at <code>/etc/modprobe.d/blacklist-nouveau.conf</code> with the following contents:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure></li><li><p>Regenerate the kernel initramfs:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure></li><li><p>Restart.</p></li><li><p>Run <code>lsmod | grep nouveau</code> again. If there is no output, then you succeed.</p></li></ol></li><li><p>此后建议进入一个非图形界面安装，这里可以在重启后使用<code>ssh</code>接入，也可以在重启后按<code>alt+ctrl+f1</code>，进入<strong>text mode</strong>，登录账户。</p></li><li><p>输入 <code>sudo service lightdm stop</code> 关闭图形化界面。</p></li><li><p>执行刚才官网中给出的第二条命令：<code>sudo sh cuda_10.2.89_440.33.01_linux.run</code>。注意这里的版本会不断有变化。注意这里有一个点，即你是否要同时安装OpenGL，如果你是双显，且主显是非NVIDIA的GPU需要选择no，否则yes。同理，如果准备选no，也可以一开始就加上参数<code>--no-opengl-files</code>。 另外，如果不能直接执行，使用<code>sudo chmod a+x cuda_xx.xx.xx_linux.run</code>为其赋权。</p></li><li><p>安装成功后，会提示你将cuda的几个路径添加到系统路径中，这里重复一下，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-10.2/bin:/usr/<span class="built_in">local</span>/cuda-10.2/NsightCompute-2019.1<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-10.2/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>使用<code>nvcc -V</code>检测是否安装成功。当然也可以同时测试<code>nvidia-smi</code>。这里可能会报错并提示需要apt安装一个包，按提示来。</p></li><li><p>完成。</p></li></ol><h3 id="参考-1">参考</h3><ol type="1"><li><a href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA下载官网</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">NVIDIA 官方安装指南（英文）</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pre-installation-actions">NVIDIA 官方安装指南中前置检查部分</a></li><li><a href="https://www.pugetsystems.com/labs/hpc/How-To-Install-CUDA-10-together-with-9-2-on-Ubuntu-18-04-with-support-for-NVIDIA-20XX-Turing-GPUs-1236/">How To Install CUDA 10 (together with 9.2) on Ubuntu 18.04 with support for NVIDIA 20XX Turing GPUs</a></li><li><a href="https://blog.csdn.net/lipi37/article/details/90407099">Ubuntu 安装 cuda 时卡在登录界面（login loop)的解决方案之一</a></li><li><a href="https://blog.csdn.net/wkk15903468980/article/details/56489704">ubuntu安装cuda循环登录</a></li><li><a href="https://blog.csdn.net/qq_33200967/article/details/80689543">Ubuntu安装和卸载CUDA和CUDNN</a></li><li><a href="https://blog.csdn.net/wf19930209/article/details/81879514">Linux安装CUDA的正确姿势</a></li></ol><h2 id="cuda-与-cudnn-的联系">CUDA 与 CUDNN 的联系</h2><ol type="1"><li>要先装CUDA再装CUDNN。</li><li>前者是平台，后者是基于平台的深度学习加速器。加速可以应用于几乎全部深度学习平台。还是要安的。</li><li>一般深度学习使用安装runtime版本即可。</li><li><a href="https://developer.nvidia.com/rdp/cudnn-download">CUDNN官方下载</a>，<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html">CUDNN官方安装步骤</a></li></ol><h2 id="修复ubuntu中检测到系统程序错误的问题">修复Ubuntu中“检测到系统程序错误”的问题</h2><h3 id="问题描述">问题描述</h3><p>每次开机时都会有“<strong>Ubuntu xx.xx 在启动时检测到系统程序错误</strong> ”弹窗出现。即使点击报告下次还会继续出现。</p><h3 id="问题来源">问题来源</h3><p>之前的某个时刻某个程序崩溃了，而Ubuntu想让你决定要不要把这个问题报告给开发者，这样他们就能够修复这个问题。</p><h3 id="解决办法">解决办法</h3><ol type="1"><li><code>sudo rm /var/crash/*</code> ：删除这些错误报告。但是如果又有一个程序崩溃了，你就会再次看到“检测到系统程序错误”的错误。你可以再次删除这些报告文件，或者选择禁用Apport来彻底地摆脱这个错误弹窗。如果你这样做，系统中任何程序崩溃时，系统都不会再通知你。但这未必一件坏事，除非你愿意填写错误报告。如果你不想填写错误报告，那么这些错误通知存不存在都不会有什么区别。</li><li><code>sudo vim /etc/default/apport</code> 永久屏蔽这些报错。</li></ol><h3 id="参考-2">参考</h3><ol type="1"><li><a href="https://blog.csdn.net/hywerr/article/details/72582082">如何修复ubuntu中检测到系统程序错误的问题</a></li><li><a href="https://itsfoss.com/how-to-fix-system-program-problem-detected-ubuntu/">How To Fix System Program Problem Detected In Ubuntu</a></li></ol><h2 id="安装python3.6版本的anaconda">安装Python3.6版本的Anaconda</h2><p>由于之前使用的一些开源库和软件对3.7的支持性尚还有问题，而Anaconda默认Python版本为3.6， 所以有必要把Anaconda降级为3.6版本。</p><p>安装方法：</p><ol type="1"><li><p>到Anaconda官网下载并安装最新3.7版本。</p></li><li><p>世界线开始分歧，你可以选择保留3.7版本的Anaconda，并创建一个虚拟环境，或是直接替换Python版本。</p><ol type="1"><li><p>对前者， 若只要一个python环境不要packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name ana36 python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure><p>反之，如果要安装一个新的Anaconda，包含默认的所有packages，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n ana36 anaconda python=3.6</span><br><span class="line"><span class="built_in">source</span> activate ana36</span><br></pre></td></tr></table></figure></li><li><p>对后者，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python=3.6</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="添加vim拷贝至系统剪贴板快捷键支持">添加Vim拷贝至系统剪贴板快捷键支持</h2><p>(from: <a href="http://vim.wikia.com/wiki/Mac_OS_X_clipboard_sharing">link</a>)</p><p>Having trouble copying selected text from Vim (not MacVim)? Since using <code>"+y</code> or '"*y' in Vim on a Mac doesn't actually copy the selected text to the system clipboard, you might find it beneficial to do the following:</p><ol type="1"><li>Open your <code>~/.vimrc</code> file</li><li>add <code>vmap '' :w !pbcopy</code></li><li>Save it and <code>source</code> the file</li></ol><p>现在，你就可以在 visual mode， 即在Esc命令模式后按下v键后的选择模式中，选好需要拷贝区域后，连击两次<code>'</code> ，即使用 <code>''</code>来拷贝所选区域。</p><h2 id="在maclinux上使用ssh挂载远程网络硬盘">在Mac/Linux上使用ssh挂载远程网络硬盘</h2><p>TL;DR：</p><ol type="1"><li>安装sshfs: <code>sudo apt-get install sshfs</code></li><li>直接在<code>~/.zshrc</code>中添加以下行：（当然，需要更改文件夹名称，以及挂载后的命名）</li></ol><h3 id="连接本地linux-server">连接本地Linux Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_misaka</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-home&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/misaka-home</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/home/miracle /Volumes/misaka-home/ -ovolname=mk-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/misaka-storage&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/misaka-storage</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 misaka:/data /Volumes/misaka-storage/ -ovolname=mk-2T</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="连接gypsum">连接Gypsum</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">connect_gypsum</span></span> () &#123;</span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/home/zhongyangzha /Volumes/gypsum/ -ovolname=gp-home</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-scratch/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum-scratch/</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/scratch1/zhongyangzha/ /Volumes/gypsum-scratch/ -ovolname=gp-scratch</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ ! -d <span class="string">&quot;/Volumes/gypsum-work/&quot;</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        mkdir /Volumes/gypsum-work</span><br><span class="line">        sshfs -o allow_other,default_permissions,IdentityFile=~/.ssh/id_rsa,reconnect,ServerAliveInterval=15,ServerAliveCountMax=3 gypsum:/mnt/nfs/work1/trahman/zhongyangzha /Volumes/gypsum-work/ -ovolname=gp-work</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数解释">参数解释</h3><ol type="1"><li><code>ovolname</code>：挂载上网络硬盘之后硬盘的命名</li><li><code>IdentityFile</code>：如果已经设置了免密登录，用这个参数指明ssh私钥位置即可，不需要输入密码。</li><li><code>&lt;source&gt; &lt;target&gt;</code>：网络硬盘源位置&lt;username@ip.address:/the/source/path&gt; 与本机目标挂载位置</li><li><code>reconnect,ServerAliveInterval=15,ServerAliveCountMax=3</code>：多次断线重连，可以再断开网络连接、服务器重启等问题发生后再次自动连接。</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;硬盘相关&quot;&gt;硬盘相关&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;df命令&lt;/strong&gt; &lt;code&gt;df&lt;/code&gt;：检查linux服务器的文件系统的磁盘空间占用情况。&lt;strong&gt;它只会显示已经挂载的磁盘信息！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df -h&lt;/code&gt;, 即&lt;code&gt;--human-readble&lt;/code&gt;：以1024的倍数的方式显示大小。(e.g., 1023M)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df -T&lt;/code&gt;：查看所有磁盘的文件系统类型(type)&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
    <category term="deep-learning" scheme="https://www.miracleyoo.com/tags/deep-learning/"/>
    
    <category term="linux" scheme="https://www.miracleyoo.com/tags/linux/"/>
    
    <category term="ssh" scheme="https://www.miracleyoo.com/tags/ssh/"/>
    
    <category term="net-disk" scheme="https://www.miracleyoo.com/tags/net-disk/"/>
    
    <category term="cuda" scheme="https://www.miracleyoo.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>Pandas Resample</title>
    <link href="https://www.miracleyoo.com/2020/03/24/pandas-resample/"/>
    <id>https://www.miracleyoo.com/2020/03/24/pandas-resample/</id>
    <published>2020-03-25T04:26:56.000Z</published>
    <updated>2021-03-12T09:11:02.817Z</updated>
    
    <content type="html"><![CDATA[<p>Pandas原生支持<code>resample</code>功能，前提是目标DataFrame需要有一个index的column。假设我们现在在对一个取样率为30Hz的DataFrame做操作，并想将它变resample为16Hz。</p><p>首先我们要建立一个<code>timestamp</code>的列，这个名字随意，然后它是以秒为单位的该帧的时间，如3.25，14.33。然后我们将其转换为datatime格式，单位为s。</p><span id="more"></span><p>之后便是直接resample，resample中的<code>rule</code>，即第一个参数，指明了resample后两帧之间的时间间隔，即周期。如果我们是16Hz，那这个周期为62.5ms。</p><p><code>resample</code>方法的格式是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.resample(rule, how=<span class="literal">None</span>, axis=<span class="number">0</span>, fill_method=<span class="literal">None</span>, closed=<span class="literal">None</span>, label=<span class="literal">None</span>, convention=<span class="string">&#x27;start&#x27;</span>,kind=<span class="literal">None</span>, loffset=<span class="literal">None</span>, limit=<span class="literal">None</span>, base=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="示例代码">示例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.index=pd.to_datetime(df[<span class="string">&#x27;timestamp&#x27;</span>],unit=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">df=df.resample(<span class="string">&#x27;62.5L&#x27;</span>).mean()</span><br><span class="line">df=df.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">del</span> df[<span class="string">&#x27;timestamp&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="pandas时间缩写">Pandas时间缩写</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">B         business day frequency</span><br><span class="line">C         custom business day frequency (experimental)</span><br><span class="line">D         calendar day frequency</span><br><span class="line">W         weekly frequency</span><br><span class="line">M         month end frequency</span><br><span class="line">SM        semi-month end frequency (15th and end of month)</span><br><span class="line">BM        business month end frequency</span><br><span class="line">CBM       custom business month end frequency</span><br><span class="line">MS        month start frequency</span><br><span class="line">SMS       semi-month start frequency (1st and 15th)</span><br><span class="line">BMS       business month start frequency</span><br><span class="line">CBMS      custom business month start frequency</span><br><span class="line">Q         quarter end frequency</span><br><span class="line">BQ        business quarter endfrequency</span><br><span class="line">QS        quarter start frequency</span><br><span class="line">BQS       business quarter start frequency</span><br><span class="line">A         year end frequency</span><br><span class="line">BA, BY    business year end frequency</span><br><span class="line">AS, YS    year start frequency</span><br><span class="line">BAS, BYS  business year start frequency</span><br><span class="line">BH        business hour frequency</span><br><span class="line">H         hourly frequency</span><br><span class="line">T, min    minutely frequency</span><br><span class="line">S         secondly frequency</span><br><span class="line">L, ms     milliseconds</span><br><span class="line">U, us     microseconds</span><br><span class="line">N         nanoseconds</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Pandas原生支持&lt;code&gt;resample&lt;/code&gt;功能，前提是目标DataFrame需要有一个index的column。假设我们现在在对一个取样率为30Hz的DataFrame做操作，并想将它变resample为16Hz。&lt;/p&gt;
&lt;p&gt;首先我们要建立一个&lt;code&gt;timestamp&lt;/code&gt;的列，这个名字随意，然后它是以秒为单位的该帧的时间，如3.25，14.33。然后我们将其转换为datatime格式，单位为s。&lt;/p&gt;</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://www.miracleyoo.com/tags/machine-learning/"/>
    
    <category term="python" scheme="https://www.miracleyoo.com/tags/python/"/>
    
  </entry>
  
</feed>
